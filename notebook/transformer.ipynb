{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d7f736-b9af-48e3-adb5-18ab3536abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkhokhlush/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n",
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    print(softmaxed_prod.shape)\n",
    "    # print(softmaxed_prod)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x)\n",
    "self_attention(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abcca093-9e66-4b46-8ec3-aded63ece6f8",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a9e6b-8461-4417-ab07-e5fbd79c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c5bf9-3800-4b22-b7d8-8886d4f15b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d626059-fd88-4a03-8b20-ffbf0115fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayerWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayerWithoutMask()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851eff3d-4816-4410-9184-f30c157e4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayerWithoutMask(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = EncoderWithoutMask()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With masks\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim)\n",
    "    if mask is not None:\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    # print(softmaxed_prod.shape)\n",
    "    print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eac5e23-ea43-4295-ba24-a7edbe1a1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4340, 0.5223, 0.4269, 0.2171],\n",
      "          [0.9768, 0.4637, 0.3135, 0.2232]],\n",
      "\n",
      "         [[0.5568, 0.9770, 0.7763, 0.2802],\n",
      "          [0.5467, 0.4784, 0.6872, 0.9199]],\n",
      "\n",
      "         [[0.3585, 0.4143, 0.5562, 0.6514],\n",
      "          [0.0876, 0.2890, 0.7103, 0.0556]]],\n",
      "\n",
      "\n",
      "        [[[0.4287, 0.1399, 0.1561, 0.8001],\n",
      "          [0.2555, 0.6752, 0.6786, 0.3966]],\n",
      "\n",
      "         [[0.1952, 0.0792, 0.6059, 0.4806],\n",
      "          [0.5169, 0.6115, 0.8734, 0.2706]],\n",
      "\n",
      "         [[0.1036, 0.7121, 0.9784, 0.0026],\n",
      "          [0.4910, 0.5604, 0.2121, 0.7938]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.3453, 0.5721,   -inf],\n",
      "          [0.5721, 0.9728,   -inf],\n",
      "          [0.3754, 0.6094,   -inf]],\n",
      "\n",
      "         [[0.6586,   -inf,   -inf],\n",
      "          [0.5883,   -inf,   -inf],\n",
      "          [0.2273,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4339, 0.2869,   -inf],\n",
      "          [0.2869, 0.3212,   -inf],\n",
      "          [0.1494, 0.3353,   -inf]],\n",
      "\n",
      "         [[0.5694,   -inf,   -inf],\n",
      "          [0.6224,   -inf,   -inf],\n",
      "          [0.4813,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4435, 0.5565, 0.0000],\n",
      "          [0.4011, 0.5989, 0.0000],\n",
      "          [0.4418, 0.5582, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5367, 0.4633, 0.0000],\n",
      "          [0.4914, 0.5086, 0.0000],\n",
      "          [0.4537, 0.5463, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "wrong a: \n",
      " tensor([[[[0.5023, 0.7753, 0.6213, 0.2522],\n",
      "          [0.5075, 0.7946, 0.6361, 0.2549],\n",
      "          [0.5025, 0.7761, 0.6219, 0.2523]],\n",
      "\n",
      "         [[0.9768, 0.4637, 0.3135, 0.2232],\n",
      "          [0.9768, 0.4637, 0.3135, 0.2232],\n",
      "          [0.9768, 0.4637, 0.3135, 0.2232]]],\n",
      "\n",
      "\n",
      "        [[[0.3205, 0.1118, 0.3645, 0.6521],\n",
      "          [0.3099, 0.1091, 0.3848, 0.6376],\n",
      "          [0.3011, 0.1068, 0.4018, 0.6255]],\n",
      "\n",
      "         [[0.2555, 0.6752, 0.6786, 0.3966],\n",
      "          [0.2555, 0.6752, 0.6786, 0.3966],\n",
      "          [0.2555, 0.6752, 0.6786, 0.3966]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.3453, 0.5721,   -inf],\n",
      "          [0.5721, 0.9728,   -inf],\n",
      "          [0.3754, 0.6094,   -inf]],\n",
      "\n",
      "         [[0.6586, 0.5883,   -inf],\n",
      "          [0.5883, 0.9231,   -inf],\n",
      "          [0.2273, 0.3627,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4339,   -inf,   -inf],\n",
      "          [0.2869,   -inf,   -inf],\n",
      "          [0.1494,   -inf,   -inf]],\n",
      "\n",
      "         [[0.5694,   -inf,   -inf],\n",
      "          [0.6224,   -inf,   -inf],\n",
      "          [0.4813,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4435, 0.5565, 0.0000],\n",
      "          [0.4011, 0.5989, 0.0000],\n",
      "          [0.4418, 0.5582, 0.0000]],\n",
      "\n",
      "         [[0.5176, 0.4824, 0.0000],\n",
      "          [0.4171, 0.5829, 0.0000],\n",
      "          [0.4662, 0.5338, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.5023, 0.7753, 0.6213, 0.2522],\n",
      "          [0.5075, 0.7946, 0.6361, 0.2549],\n",
      "          [0.5025, 0.7761, 0.6219, 0.2523]],\n",
      "\n",
      "         [[0.7693, 0.4708, 0.4938, 0.5593],\n",
      "          [0.7261, 0.4722, 0.5314, 0.6293],\n",
      "          [0.7472, 0.4715, 0.5130, 0.5951]]],\n",
      "\n",
      "\n",
      "        [[[0.4287, 0.1399, 0.1561, 0.8001],\n",
      "          [0.4287, 0.1399, 0.1561, 0.8001],\n",
      "          [0.4287, 0.1399, 0.1561, 0.8001]],\n",
      "\n",
      "         [[0.2555, 0.6752, 0.6786, 0.3966],\n",
      "          [0.2555, 0.6752, 0.6786, 0.3966],\n",
      "          [0.2555, 0.6752, 0.6786, 0.3966]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.4340, 0.5223, 0.4269, 0.2171],\n",
      "          [0.9768, 0.4637, 0.3135, 0.2232]],\n",
      "\n",
      "         [[0.5568, 0.9770, 0.7763, 0.2802],\n",
      "          [0.5467, 0.4784, 0.6872, 0.9199]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4287, 0.1399, 0.1561, 0.8001],\n",
      "          [0.2555, 0.6752, 0.6786, 0.3966]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.3453, 0.5721,   -inf],\n",
      "          [0.5721, 0.9728,   -inf],\n",
      "          [0.3754, 0.6094,   -inf]],\n",
      "\n",
      "         [[0.6586, 0.5883,   -inf],\n",
      "          [0.5883, 0.9231,   -inf],\n",
      "          [0.2273, 0.3627,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4339,   -inf,   -inf],\n",
      "          [0.2869,   -inf,   -inf],\n",
      "          [0.1494,   -inf,   -inf]],\n",
      "\n",
      "         [[0.5694,   -inf,   -inf],\n",
      "          [0.6224,   -inf,   -inf],\n",
      "          [0.4813,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4435, 0.5565, 0.0000],\n",
      "          [0.4011, 0.5989, 0.0000],\n",
      "          [0.4418, 0.5582, 0.0000]],\n",
      "\n",
      "         [[0.5176, 0.4824, 0.0000],\n",
      "          [0.4171, 0.5829, 0.0000],\n",
      "          [0.4662, 0.5338, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.5023, 0.7753, 0.6213, 0.2522],\n",
      "          [0.5075, 0.7946, 0.6361, 0.2549],\n",
      "          [0.5025, 0.7761, 0.6219, 0.2523]],\n",
      "\n",
      "         [[0.7693, 0.4708, 0.4938, 0.5593],\n",
      "          [0.7261, 0.4722, 0.5314, 0.6293],\n",
      "          [0.7472, 0.4715, 0.5130, 0.5951]]],\n",
      "\n",
      "\n",
      "        [[[0.4287, 0.1399, 0.1561, 0.8001],\n",
      "          [0.4287, 0.1399, 0.1561, 0.8001],\n",
      "          [0.4287, 0.1399, 0.1561, 0.8001]],\n",
      "\n",
      "         [[0.2555, 0.6752, 0.6786, 0.3966],\n",
      "          [0.2555, 0.6752, 0.6786, 0.3966],\n",
      "          [0.2555, 0.6752, 0.6786, 0.3966]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.6286, 0.5032, 0.9092, 0.1244],\n",
      "         [0.6114, 0.0027, 0.4636, 0.8190],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1192, 0.3343, 0.2947, 0.7903],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.6286, 0.5032],\n",
      "          [0.9092, 0.1244]],\n",
      "\n",
      "         [[0.6114, 0.0027],\n",
      "          [0.4636, 0.8190]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1192, 0.3343],\n",
      "          [0.2947, 0.7903]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.6286, 0.5032],\n",
      "          [0.6114, 0.0027],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.9092, 0.1244],\n",
      "          [0.4636, 0.8190],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1192, 0.3343],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.2947, 0.7903],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.9661, 0.8521, 0.6333, 0.7259],\n",
      "         [0.0489, 0.0971, 0.1809, 0.9006],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.4952, 0.7276, 0.3030, 0.0424],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.9661, 0.8521],\n",
      "          [0.6333, 0.7259]],\n",
      "\n",
      "         [[0.0489, 0.0971],\n",
      "          [0.1809, 0.9006]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4952, 0.7276],\n",
      "          [0.3030, 0.0424]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.9661, 0.8521],\n",
      "          [0.0489, 0.0971],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.6333, 0.7259],\n",
      "          [0.1809, 0.9006],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4952, 0.7276],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.3030, 0.0424],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[1.6595, 0.1300,   -inf],\n",
      "          [0.1300, 0.0118,   -inf],\n",
      "          [0.8401, 0.0779,   -inf]],\n",
      "\n",
      "         [[0.9280, 0.7683,   -inf],\n",
      "          [0.7683, 0.8438,   -inf],\n",
      "          [0.3828, 0.1558,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7747,   -inf,   -inf],\n",
      "          [0.7762,   -inf,   -inf],\n",
      "          [0.4791,   -inf,   -inf]],\n",
      "\n",
      "         [[0.0936,   -inf,   -inf],\n",
      "          [0.0968,   -inf,   -inf],\n",
      "          [0.3008,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.7419e-01, 5.5751e-01, 6.7694e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.8944e-01, 9.4574e-02, 7.7391e-01, 2.9337e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [3.7964e-01, 8.5827e-01, 7.9380e-01, 5.2391e-01, 9.6066e-01, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.1827e-01, 2.0493e-01, 4.5561e-01, 7.9130e-01, 5.9736e-01, 7.3213e-01]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5164e-01, 4.5999e-02, 4.3802e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.4553e-01, 4.3584e-01, 7.5343e-01, 8.7683e-01, 5.2853e-01, 1.0000e+02],\n",
      "        [4.2071e-03, 5.0097e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.7994e-01, 3.3792e-01, 9.0521e-01, 8.5616e-01, 5.2792e-01, 4.4129e-01]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(build_causal_mask(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-1.2840e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-4.5087e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-8.2892e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-8.7938e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-1.2900e-01,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 8.0442e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.0306e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.5971e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 9.7914e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.1338e-01,        -inf,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.1603e-01, -9.3408e-02,        -inf,        -inf,        -inf],\n",
      "          [-1.9925e-01, -1.2135e-01,        -inf,        -inf,        -inf],\n",
      "          [-9.3440e-03,  6.8273e-03,        -inf,        -inf,        -inf],\n",
      "          [-1.3091e-01, -7.5007e-02,        -inf,        -inf,        -inf],\n",
      "          [-3.0827e-03,  4.5655e-02,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-1.2775e-02, -1.3891e-01,        -inf,        -inf,        -inf],\n",
      "          [-1.0804e-02, -1.5692e-01,        -inf,        -inf,        -inf],\n",
      "          [-1.3295e-02, -1.8649e-01,        -inf,        -inf,        -inf],\n",
      "          [ 2.4989e-02, -1.6515e-01,        -inf,        -inf,        -inf],\n",
      "          [ 1.2383e-01, -9.1256e-02,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6620e-02, -2.3474e-03, -2.0851e-02,        -inf,        -inf],\n",
      "          [-9.1282e-02, -8.7290e-02, -6.7385e-02,        -inf,        -inf],\n",
      "          [-1.6480e-01, -2.1931e-01, -2.2645e-01,        -inf,        -inf],\n",
      "          [-9.5536e-02, -1.1478e-01, -1.0246e-01,        -inf,        -inf],\n",
      "          [-4.3221e-02, -1.9350e-02,  5.8498e-03,        -inf,        -inf]],\n",
      "\n",
      "         [[ 3.9887e-02,  1.4939e-01, -8.5790e-03,        -inf,        -inf],\n",
      "          [ 1.2022e-02,  5.3170e-02, -6.4600e-02,        -inf,        -inf],\n",
      "          [-7.8307e-05, -2.4086e-02, -1.1096e-01,        -inf,        -inf],\n",
      "          [ 5.0347e-02,  1.4205e-01,  2.5353e-02,        -inf,        -inf],\n",
      "          [ 2.5446e-02,  1.3693e-01, -1.0222e-02,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-9.1209e-02, -1.0227e-01, -7.3833e-02, -6.9008e-02,        -inf],\n",
      "          [-7.5515e-02, -8.1325e-02, -4.4961e-02, -4.9874e-02,        -inf],\n",
      "          [-1.4952e-01, -1.6611e-01, -1.2494e-01, -1.1525e-01,        -inf],\n",
      "          [-1.7799e-02, -5.9607e-02, -1.3521e-01, -6.5427e-02,        -inf],\n",
      "          [-2.2114e-02, -2.7969e-02, -6.0916e-02, -3.6941e-02,        -inf]],\n",
      "\n",
      "         [[-1.5274e-01, -3.2356e-02,  2.1766e-03, -8.5120e-02,        -inf],\n",
      "          [-1.5730e-01, -3.4759e-02, -3.4277e-03, -1.0031e-01,        -inf],\n",
      "          [-6.8887e-02,  9.1272e-02,  5.0549e-02, -9.3764e-02,        -inf],\n",
      "          [-4.1557e-02,  1.5095e-01,  7.1486e-02, -1.1720e-01,        -inf],\n",
      "          [-1.3576e-01, -5.0009e-02, -6.8830e-03, -6.1501e-02,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4943, 0.5057, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4805, 0.5195, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4960, 0.5040, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4860, 0.5140, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4878, 0.5122, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5315, 0.4685, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5365, 0.4635, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5432, 0.4568, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5474, 0.4526, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5536, 0.4464, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3419, 0.3321, 0.3260, 0.0000, 0.0000],\n",
      "          [0.3302, 0.3316, 0.3382, 0.0000, 0.0000],\n",
      "          [0.3464, 0.3280, 0.3257, 0.0000, 0.0000],\n",
      "          [0.3362, 0.3298, 0.3339, 0.0000, 0.0000],\n",
      "          [0.3253, 0.3331, 0.3416, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3259, 0.3636, 0.3105, 0.0000, 0.0000],\n",
      "          [0.3369, 0.3511, 0.3120, 0.0000, 0.0000],\n",
      "          [0.3483, 0.3400, 0.3117, 0.0000, 0.0000],\n",
      "          [0.3256, 0.3569, 0.3176, 0.0000, 0.0000],\n",
      "          [0.3244, 0.3626, 0.3130, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2482, 0.2455, 0.2526, 0.2538, 0.0000],\n",
      "          [0.2468, 0.2454, 0.2545, 0.2533, 0.0000],\n",
      "          [0.2473, 0.2433, 0.2535, 0.2559, 0.0000],\n",
      "          [0.2630, 0.2523, 0.2339, 0.2508, 0.0000],\n",
      "          [0.2537, 0.2522, 0.2441, 0.2500, 0.0000]],\n",
      "\n",
      "         [[0.2291, 0.2584, 0.2675, 0.2451, 0.0000],\n",
      "          [0.2296, 0.2595, 0.2678, 0.2431, 0.0000],\n",
      "          [0.2339, 0.2745, 0.2635, 0.2281, 0.0000],\n",
      "          [0.2348, 0.2846, 0.2629, 0.2177, 0.0000],\n",
      "          [0.2323, 0.2531, 0.2643, 0.2502, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[ 0.4302, -0.2647, -0.2833, -0.0925,  0.0861,  0.1228],\n",
      "         [ 0.4302, -0.2647, -0.2833, -0.0925,  0.0861,  0.1228],\n",
      "         [ 0.4302, -0.2647, -0.2833, -0.0925,  0.0861,  0.1228],\n",
      "         [ 0.4302, -0.2647, -0.2833, -0.0925,  0.0861,  0.1228],\n",
      "         [ 0.4302, -0.2647, -0.2833, -0.0925,  0.0861,  0.1228]],\n",
      "\n",
      "        [[ 0.3175, -0.3890, -0.5842,  0.0652,  0.0991,  0.3773],\n",
      "         [ 0.3173, -0.3872, -0.5830,  0.0643,  0.0993,  0.3760],\n",
      "         [ 0.3182, -0.3884, -0.5827,  0.0663,  0.0971,  0.3761],\n",
      "         [ 0.3181, -0.3871, -0.5817,  0.0657,  0.0971,  0.3751],\n",
      "         [ 0.3185, -0.3869, -0.5810,  0.0664,  0.0959,  0.3745]],\n",
      "\n",
      "        [[ 0.3129, -0.3928, -0.5306,  0.0685,  0.1174,  0.3678],\n",
      "         [ 0.3131, -0.3922, -0.5346,  0.0654,  0.1184,  0.3702],\n",
      "         [ 0.3121, -0.3937, -0.5319,  0.0679,  0.1200,  0.3693],\n",
      "         [ 0.3122, -0.3925, -0.5336,  0.0675,  0.1183,  0.3696],\n",
      "         [ 0.3134, -0.3915, -0.5349,  0.0652,  0.1172,  0.3699]],\n",
      "\n",
      "        [[ 0.3084, -0.3378, -0.4971, -0.0198,  0.1927,  0.3291],\n",
      "         [ 0.3087, -0.3378, -0.4968, -0.0201,  0.1921,  0.3288],\n",
      "         [ 0.3112, -0.3365, -0.4921, -0.0229,  0.1872,  0.3245],\n",
      "         [ 0.3127, -0.3363, -0.4879, -0.0231,  0.1848,  0.3203],\n",
      "         [ 0.3079, -0.3388, -0.4980, -0.0187,  0.1945,  0.3298]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-1.2840e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-4.5087e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-8.2892e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-8.7938e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-1.2900e-01,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 8.0442e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.0306e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.5971e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 9.7914e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.1338e-01,        -inf,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.1603e-01, -9.3408e-02,        -inf,        -inf,        -inf],\n",
      "          [-1.9925e-01, -1.2135e-01,        -inf,        -inf,        -inf],\n",
      "          [-9.3440e-03,  6.8273e-03,        -inf,        -inf,        -inf],\n",
      "          [-1.3091e-01, -7.5007e-02,        -inf,        -inf,        -inf],\n",
      "          [-3.0827e-03,  4.5655e-02,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-1.2775e-02, -1.3891e-01,        -inf,        -inf,        -inf],\n",
      "          [-1.0804e-02, -1.5692e-01,        -inf,        -inf,        -inf],\n",
      "          [-1.3295e-02, -1.8649e-01,        -inf,        -inf,        -inf],\n",
      "          [ 2.4989e-02, -1.6515e-01,        -inf,        -inf,        -inf],\n",
      "          [ 1.2383e-01, -9.1256e-02,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6620e-02, -2.3474e-03, -2.0851e-02,        -inf,        -inf],\n",
      "          [-9.1282e-02, -8.7290e-02, -6.7385e-02,        -inf,        -inf],\n",
      "          [-1.6480e-01, -2.1931e-01, -2.2645e-01,        -inf,        -inf],\n",
      "          [-9.5536e-02, -1.1478e-01, -1.0246e-01,        -inf,        -inf],\n",
      "          [-4.3221e-02, -1.9350e-02,  5.8498e-03,        -inf,        -inf]],\n",
      "\n",
      "         [[ 3.9887e-02,  1.4939e-01, -8.5790e-03,        -inf,        -inf],\n",
      "          [ 1.2022e-02,  5.3170e-02, -6.4600e-02,        -inf,        -inf],\n",
      "          [-7.8307e-05, -2.4086e-02, -1.1096e-01,        -inf,        -inf],\n",
      "          [ 5.0347e-02,  1.4205e-01,  2.5353e-02,        -inf,        -inf],\n",
      "          [ 2.5446e-02,  1.3693e-01, -1.0222e-02,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-9.1209e-02, -1.0227e-01, -7.3833e-02, -6.9008e-02,        -inf],\n",
      "          [-7.5515e-02, -8.1325e-02, -4.4961e-02, -4.9874e-02,        -inf],\n",
      "          [-1.4952e-01, -1.6611e-01, -1.2494e-01, -1.1525e-01,        -inf],\n",
      "          [-1.7799e-02, -5.9607e-02, -1.3521e-01, -6.5427e-02,        -inf],\n",
      "          [-2.2114e-02, -2.7969e-02, -6.0916e-02, -3.6941e-02,        -inf]],\n",
      "\n",
      "         [[-1.5274e-01, -3.2356e-02,  2.1766e-03, -8.5120e-02,        -inf],\n",
      "          [-1.5730e-01, -3.4759e-02, -3.4277e-03, -1.0031e-01,        -inf],\n",
      "          [-6.8887e-02,  9.1272e-02,  5.0549e-02, -9.3764e-02,        -inf],\n",
      "          [-4.1557e-02,  1.5095e-01,  7.1486e-02, -1.1720e-01,        -inf],\n",
      "          [-1.3576e-01, -5.0009e-02, -6.8830e-03, -6.1501e-02,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4943, 0.5057, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4805, 0.5195, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4960, 0.5040, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4860, 0.5140, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4878, 0.5122, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5315, 0.4685, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5365, 0.4635, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5432, 0.4568, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5474, 0.4526, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5536, 0.4464, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3419, 0.3321, 0.3260, 0.0000, 0.0000],\n",
      "          [0.3302, 0.3316, 0.3382, 0.0000, 0.0000],\n",
      "          [0.3464, 0.3280, 0.3257, 0.0000, 0.0000],\n",
      "          [0.3362, 0.3298, 0.3339, 0.0000, 0.0000],\n",
      "          [0.3253, 0.3331, 0.3416, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3259, 0.3636, 0.3105, 0.0000, 0.0000],\n",
      "          [0.3369, 0.3511, 0.3120, 0.0000, 0.0000],\n",
      "          [0.3483, 0.3400, 0.3117, 0.0000, 0.0000],\n",
      "          [0.3256, 0.3569, 0.3176, 0.0000, 0.0000],\n",
      "          [0.3244, 0.3626, 0.3130, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2482, 0.2455, 0.2526, 0.2538, 0.0000],\n",
      "          [0.2468, 0.2454, 0.2545, 0.2533, 0.0000],\n",
      "          [0.2473, 0.2433, 0.2535, 0.2559, 0.0000],\n",
      "          [0.2630, 0.2523, 0.2339, 0.2508, 0.0000],\n",
      "          [0.2537, 0.2522, 0.2441, 0.2500, 0.0000]],\n",
      "\n",
      "         [[0.2291, 0.2584, 0.2675, 0.2451, 0.0000],\n",
      "          [0.2296, 0.2595, 0.2678, 0.2431, 0.0000],\n",
      "          [0.2339, 0.2745, 0.2635, 0.2281, 0.0000],\n",
      "          [0.2348, 0.2846, 0.2629, 0.2177, 0.0000],\n",
      "          [0.2323, 0.2531, 0.2643, 0.2502, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729ae6f8-a3f8-4386-837d-f1e67f022b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0400,  0.1609,    -inf],\n",
      "          [ 0.0887,  0.1521,    -inf],\n",
      "          [ 0.0221,  0.0848,    -inf]],\n",
      "\n",
      "         [[-0.0770, -0.0267,    -inf],\n",
      "          [ 0.0371,  0.0551,    -inf],\n",
      "          [-0.0627,  0.0160,    -inf]],\n",
      "\n",
      "         [[ 0.1222,  0.1838,    -inf],\n",
      "          [-0.0441,  0.0523,    -inf],\n",
      "          [ 0.0183,  0.0960,    -inf]],\n",
      "\n",
      "         [[ 0.1087,  0.1351,    -inf],\n",
      "          [ 0.0745,  0.1493,    -inf],\n",
      "          [ 0.0137,  0.1409,    -inf]],\n",
      "\n",
      "         [[ 0.0836,  0.0215,    -inf],\n",
      "          [ 0.1627,  0.0865,    -inf],\n",
      "          [ 0.1105,  0.0523,    -inf]],\n",
      "\n",
      "         [[ 0.1443,  0.1185,    -inf],\n",
      "          [ 0.0323,  0.0797,    -inf],\n",
      "          [ 0.0802,  0.0625,    -inf]],\n",
      "\n",
      "         [[-0.1462, -0.1239,    -inf],\n",
      "          [-0.1055, -0.0627,    -inf],\n",
      "          [-0.1101, -0.0738,    -inf]],\n",
      "\n",
      "         [[-0.0942, -0.1523,    -inf],\n",
      "          [-0.1362, -0.2236,    -inf],\n",
      "          [-0.2240, -0.2347,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1188,    -inf,    -inf],\n",
      "          [ 0.1086,    -inf,    -inf],\n",
      "          [ 0.1756,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0387,    -inf,    -inf],\n",
      "          [-0.0695,    -inf,    -inf],\n",
      "          [-0.0760,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1208,    -inf,    -inf],\n",
      "          [ 0.1371,    -inf,    -inf],\n",
      "          [ 0.2742,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1089,    -inf,    -inf],\n",
      "          [ 0.0693,    -inf,    -inf],\n",
      "          [ 0.0260,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0129,    -inf,    -inf],\n",
      "          [ 0.0075,    -inf,    -inf],\n",
      "          [-0.1300,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0337,    -inf,    -inf],\n",
      "          [ 0.2012,    -inf,    -inf],\n",
      "          [ 0.0811,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1830,    -inf,    -inf],\n",
      "          [-0.1249,    -inf,    -inf],\n",
      "          [-0.2488,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1981,    -inf,    -inf],\n",
      "          [-0.1157,    -inf,    -inf],\n",
      "          [-0.1054,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4698, 0.5302, 0.0000],\n",
      "          [0.4841, 0.5159, 0.0000],\n",
      "          [0.4843, 0.5157, 0.0000]],\n",
      "\n",
      "         [[0.4874, 0.5126, 0.0000],\n",
      "          [0.4955, 0.5045, 0.0000],\n",
      "          [0.4803, 0.5197, 0.0000]],\n",
      "\n",
      "         [[0.4846, 0.5154, 0.0000],\n",
      "          [0.4759, 0.5241, 0.0000],\n",
      "          [0.4806, 0.5194, 0.0000]],\n",
      "\n",
      "         [[0.4934, 0.5066, 0.0000],\n",
      "          [0.4813, 0.5187, 0.0000],\n",
      "          [0.4682, 0.5318, 0.0000]],\n",
      "\n",
      "         [[0.5155, 0.4845, 0.0000],\n",
      "          [0.5190, 0.4810, 0.0000],\n",
      "          [0.5145, 0.4855, 0.0000]],\n",
      "\n",
      "         [[0.5064, 0.4936, 0.0000],\n",
      "          [0.4882, 0.5118, 0.0000],\n",
      "          [0.5044, 0.4956, 0.0000]],\n",
      "\n",
      "         [[0.4944, 0.5056, 0.0000],\n",
      "          [0.4893, 0.5107, 0.0000],\n",
      "          [0.4909, 0.5091, 0.0000]],\n",
      "\n",
      "         [[0.5145, 0.4855, 0.0000],\n",
      "          [0.5218, 0.4782, 0.0000],\n",
      "          [0.5027, 0.4973, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, self_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "x = torch.rand(2, 3, 512)\n",
    "\n",
    "encoder_layer(x, self_mask=self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.3789, -0.5068,    -inf],\n",
      "          [ 0.3039,  0.0900,    -inf],\n",
      "          [ 0.3416,  0.0758,    -inf]],\n",
      "\n",
      "         [[ 1.3474,  0.5011,    -inf],\n",
      "          [-0.1314,  0.0162,    -inf],\n",
      "          [ 0.3439, -0.4517,    -inf]],\n",
      "\n",
      "         [[-0.6137, -0.4129,    -inf],\n",
      "          [-0.4291, -0.5494,    -inf],\n",
      "          [-0.3670, -0.4084,    -inf]],\n",
      "\n",
      "         [[ 0.3829,  0.2659,    -inf],\n",
      "          [ 0.5651,  0.3559,    -inf],\n",
      "          [ 0.0485,  0.6513,    -inf]],\n",
      "\n",
      "         [[-0.1700, -0.4704,    -inf],\n",
      "          [ 0.2957,  0.4435,    -inf],\n",
      "          [ 0.3295,  0.2808,    -inf]],\n",
      "\n",
      "         [[ 0.3368,  0.1244,    -inf],\n",
      "          [ 0.2584,  0.4747,    -inf],\n",
      "          [-0.2681,  0.2208,    -inf]],\n",
      "\n",
      "         [[-0.2332,  0.3061,    -inf],\n",
      "          [-0.9967,  0.0122,    -inf],\n",
      "          [ 0.4552, -0.0845,    -inf]],\n",
      "\n",
      "         [[-0.1904, -0.6953,    -inf],\n",
      "          [-0.4211, -0.0056,    -inf],\n",
      "          [-0.1720, -0.3912,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2967,    -inf,    -inf],\n",
      "          [ 0.5577,    -inf,    -inf],\n",
      "          [ 0.5362,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3690,    -inf,    -inf],\n",
      "          [-0.3779,    -inf,    -inf],\n",
      "          [-0.8035,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0694,    -inf,    -inf],\n",
      "          [-0.3983,    -inf,    -inf],\n",
      "          [ 0.0057,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3822,    -inf,    -inf],\n",
      "          [ 0.1771,    -inf,    -inf],\n",
      "          [ 0.0562,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3937,    -inf,    -inf],\n",
      "          [-0.1562,    -inf,    -inf],\n",
      "          [-0.2007,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3339,    -inf,    -inf],\n",
      "          [ 0.4749,    -inf,    -inf],\n",
      "          [ 1.0238,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3016,    -inf,    -inf],\n",
      "          [-0.1357,    -inf,    -inf],\n",
      "          [-0.6870,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3720,    -inf,    -inf],\n",
      "          [-0.7937,    -inf,    -inf],\n",
      "          [-0.4054,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.7080, 0.2920, 0.0000],\n",
      "          [0.5533, 0.4467, 0.0000],\n",
      "          [0.5661, 0.4339, 0.0000]],\n",
      "\n",
      "         [[0.6998, 0.3002, 0.0000],\n",
      "          [0.4632, 0.5368, 0.0000],\n",
      "          [0.6890, 0.3110, 0.0000]],\n",
      "\n",
      "         [[0.4500, 0.5500, 0.0000],\n",
      "          [0.5300, 0.4700, 0.0000],\n",
      "          [0.5104, 0.4896, 0.0000]],\n",
      "\n",
      "         [[0.5292, 0.4708, 0.0000],\n",
      "          [0.5521, 0.4479, 0.0000],\n",
      "          [0.3537, 0.6463, 0.0000]],\n",
      "\n",
      "         [[0.5746, 0.4254, 0.0000],\n",
      "          [0.4631, 0.5369, 0.0000],\n",
      "          [0.5122, 0.4878, 0.0000]],\n",
      "\n",
      "         [[0.5529, 0.4471, 0.0000],\n",
      "          [0.4461, 0.5539, 0.0000],\n",
      "          [0.3802, 0.6198, 0.0000]],\n",
      "\n",
      "         [[0.3684, 0.6316, 0.0000],\n",
      "          [0.2672, 0.7328, 0.0000],\n",
      "          [0.6317, 0.3683, 0.0000]],\n",
      "\n",
      "         [[0.6236, 0.3764, 0.0000],\n",
      "          [0.3976, 0.6024, 0.0000],\n",
      "          [0.5546, 0.4454, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.3133,  0.2672,    -inf],\n",
      "          [-0.1754,  0.0393,    -inf],\n",
      "          [ 0.3627,  0.0249,    -inf]],\n",
      "\n",
      "         [[-0.0455, -0.1730,    -inf],\n",
      "          [-0.0761, -0.1108,    -inf],\n",
      "          [ 0.0168, -0.2264,    -inf]],\n",
      "\n",
      "         [[ 0.1078, -0.9787,    -inf],\n",
      "          [ 0.3338, -0.1248,    -inf],\n",
      "          [-0.1266, -0.0545,    -inf]],\n",
      "\n",
      "         [[ 1.1148,  0.6455,    -inf],\n",
      "          [ 0.3495,  0.2511,    -inf],\n",
      "          [ 0.5787,  0.7455,    -inf]],\n",
      "\n",
      "         [[ 0.4151, -0.2201,    -inf],\n",
      "          [-0.2537, -0.0605,    -inf],\n",
      "          [ 0.0745,  0.1082,    -inf]],\n",
      "\n",
      "         [[ 0.1311,  0.0960,    -inf],\n",
      "          [-0.6216,  0.1797,    -inf],\n",
      "          [ 0.0215,  0.6061,    -inf]],\n",
      "\n",
      "         [[ 0.1987, -0.6960,    -inf],\n",
      "          [-0.4395,  0.2142,    -inf],\n",
      "          [ 0.0823, -0.1394,    -inf]],\n",
      "\n",
      "         [[ 0.1158,  0.5392,    -inf],\n",
      "          [-0.1048,  0.3316,    -inf],\n",
      "          [-0.5654,  0.0440,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3081,    -inf,    -inf],\n",
      "          [ 0.0869,    -inf,    -inf],\n",
      "          [-0.1765,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1781,    -inf,    -inf],\n",
      "          [-0.3462,    -inf,    -inf],\n",
      "          [-0.1066,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3305,    -inf,    -inf],\n",
      "          [-0.1567,    -inf,    -inf],\n",
      "          [ 0.3147,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2041,    -inf,    -inf],\n",
      "          [-0.0391,    -inf,    -inf],\n",
      "          [-0.0744,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1756,    -inf,    -inf],\n",
      "          [ 0.0269,    -inf,    -inf],\n",
      "          [ 0.3417,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5342,    -inf,    -inf],\n",
      "          [ 0.6440,    -inf,    -inf],\n",
      "          [ 0.1107,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2159,    -inf,    -inf],\n",
      "          [ 0.7285,    -inf,    -inf],\n",
      "          [-0.1464,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3332,    -inf,    -inf],\n",
      "          [ 0.2917,    -inf,    -inf],\n",
      "          [ 0.3651,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3588, 0.6412, 0.0000],\n",
      "          [0.4465, 0.5535, 0.0000],\n",
      "          [0.5837, 0.4163, 0.0000]],\n",
      "\n",
      "         [[0.5318, 0.4682, 0.0000],\n",
      "          [0.5087, 0.4913, 0.0000],\n",
      "          [0.5605, 0.4395, 0.0000]],\n",
      "\n",
      "         [[0.7477, 0.2523, 0.0000],\n",
      "          [0.6127, 0.3873, 0.0000],\n",
      "          [0.4820, 0.5180, 0.0000]],\n",
      "\n",
      "         [[0.6152, 0.3848, 0.0000],\n",
      "          [0.5246, 0.4754, 0.0000],\n",
      "          [0.4584, 0.5416, 0.0000]],\n",
      "\n",
      "         [[0.6537, 0.3463, 0.0000],\n",
      "          [0.4519, 0.5481, 0.0000],\n",
      "          [0.4916, 0.5084, 0.0000]],\n",
      "\n",
      "         [[0.5088, 0.4912, 0.0000],\n",
      "          [0.3098, 0.6902, 0.0000],\n",
      "          [0.3579, 0.6421, 0.0000]],\n",
      "\n",
      "         [[0.7099, 0.2901, 0.0000],\n",
      "          [0.3421, 0.6579, 0.0000],\n",
      "          [0.5552, 0.4448, 0.0000]],\n",
      "\n",
      "         [[0.3957, 0.6043, 0.0000],\n",
      "          [0.3926, 0.6074, 0.0000],\n",
      "          [0.3522, 0.6478, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0855, -0.2633,    -inf],\n",
      "          [-0.2312, -0.0515,    -inf],\n",
      "          [-0.3090, -0.5780,    -inf]],\n",
      "\n",
      "         [[-0.2024, -0.0513,    -inf],\n",
      "          [-0.3786, -0.3209,    -inf],\n",
      "          [-0.0515, -0.0627,    -inf]],\n",
      "\n",
      "         [[ 0.1466, -0.0876,    -inf],\n",
      "          [-0.4263, -0.4054,    -inf],\n",
      "          [-0.6747, -0.1643,    -inf]],\n",
      "\n",
      "         [[-0.1060,  0.0066,    -inf],\n",
      "          [-0.3472, -0.2971,    -inf],\n",
      "          [-0.0132,  0.2930,    -inf]],\n",
      "\n",
      "         [[-0.1935, -0.1359,    -inf],\n",
      "          [-0.1084,  0.0919,    -inf],\n",
      "          [-0.5906,  0.3219,    -inf]],\n",
      "\n",
      "         [[ 0.6248,  0.3685,    -inf],\n",
      "          [-0.2762,  0.2592,    -inf],\n",
      "          [ 0.2057, -0.1001,    -inf]],\n",
      "\n",
      "         [[-0.2207, -0.0842,    -inf],\n",
      "          [ 0.1343,  0.4991,    -inf],\n",
      "          [ 0.2574, -0.0051,    -inf]],\n",
      "\n",
      "         [[ 0.4799, -0.3814,    -inf],\n",
      "          [ 0.2663,  0.2386,    -inf],\n",
      "          [ 0.3181,  0.2950,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.4704,    -inf,    -inf],\n",
      "          [-1.0460,    -inf,    -inf],\n",
      "          [-0.2591,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6564,    -inf,    -inf],\n",
      "          [-0.0953,    -inf,    -inf],\n",
      "          [-0.0487,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0093,    -inf,    -inf],\n",
      "          [-0.0935,    -inf,    -inf],\n",
      "          [-0.3632,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5641,    -inf,    -inf],\n",
      "          [ 0.3655,    -inf,    -inf],\n",
      "          [ 0.1376,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6106,    -inf,    -inf],\n",
      "          [ 0.5619,    -inf,    -inf],\n",
      "          [ 0.1538,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0741,    -inf,    -inf],\n",
      "          [ 0.2983,    -inf,    -inf],\n",
      "          [-0.3082,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6170,    -inf,    -inf],\n",
      "          [ 0.3207,    -inf,    -inf],\n",
      "          [ 0.2023,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0776,    -inf,    -inf],\n",
      "          [-0.1876,    -inf,    -inf],\n",
      "          [ 0.0292,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5443, 0.4557, 0.0000],\n",
      "          [0.4552, 0.5448, 0.0000],\n",
      "          [0.5668, 0.4332, 0.0000]],\n",
      "\n",
      "         [[0.4623, 0.5377, 0.0000],\n",
      "          [0.4856, 0.5144, 0.0000],\n",
      "          [0.5028, 0.4972, 0.0000]],\n",
      "\n",
      "         [[0.5583, 0.4417, 0.0000],\n",
      "          [0.4948, 0.5052, 0.0000],\n",
      "          [0.3751, 0.6249, 0.0000]],\n",
      "\n",
      "         [[0.4719, 0.5281, 0.0000],\n",
      "          [0.4875, 0.5125, 0.0000],\n",
      "          [0.4240, 0.5760, 0.0000]],\n",
      "\n",
      "         [[0.4856, 0.5144, 0.0000],\n",
      "          [0.4501, 0.5499, 0.0000],\n",
      "          [0.2865, 0.7135, 0.0000]],\n",
      "\n",
      "         [[0.5637, 0.4363, 0.0000],\n",
      "          [0.3693, 0.6307, 0.0000],\n",
      "          [0.5759, 0.4241, 0.0000]],\n",
      "\n",
      "         [[0.4659, 0.5341, 0.0000],\n",
      "          [0.4098, 0.5902, 0.0000],\n",
      "          [0.5652, 0.4348, 0.0000]],\n",
      "\n",
      "         [[0.7029, 0.2971, 0.0000],\n",
      "          [0.5069, 0.4931, 0.0000],\n",
      "          [0.5058, 0.4942, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1685,  0.1086,    -inf],\n",
      "          [-0.1805, -0.5224,    -inf],\n",
      "          [ 0.1270,  0.0648,    -inf]],\n",
      "\n",
      "         [[ 0.2432, -0.3111,    -inf],\n",
      "          [ 0.5143,  0.4699,    -inf],\n",
      "          [ 0.0254, -0.6068,    -inf]],\n",
      "\n",
      "         [[-0.5433, -0.1295,    -inf],\n",
      "          [-0.0963,  0.0930,    -inf],\n",
      "          [-0.2624, -0.6059,    -inf]],\n",
      "\n",
      "         [[ 0.4092,  0.1357,    -inf],\n",
      "          [ 0.0301, -0.0661,    -inf],\n",
      "          [ 0.5331,  0.4643,    -inf]],\n",
      "\n",
      "         [[ 0.3113,  0.2295,    -inf],\n",
      "          [ 0.5106,  0.3959,    -inf],\n",
      "          [-0.0545,  0.0908,    -inf]],\n",
      "\n",
      "         [[-0.7504, -0.4538,    -inf],\n",
      "          [-0.7539, -0.1484,    -inf],\n",
      "          [-0.1375,  0.1009,    -inf]],\n",
      "\n",
      "         [[-0.2888,  0.5399,    -inf],\n",
      "          [-0.4448,  0.1411,    -inf],\n",
      "          [ 0.0311,  0.4938,    -inf]],\n",
      "\n",
      "         [[ 0.0981, -0.1240,    -inf],\n",
      "          [-0.4635, -0.2410,    -inf],\n",
      "          [ 0.0587,  0.1268,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0858,    -inf,    -inf],\n",
      "          [-0.4684,    -inf,    -inf],\n",
      "          [-0.5439,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0862,    -inf,    -inf],\n",
      "          [-0.2878,    -inf,    -inf],\n",
      "          [ 0.2392,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3501,    -inf,    -inf],\n",
      "          [-0.5640,    -inf,    -inf],\n",
      "          [-0.6906,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1347,    -inf,    -inf],\n",
      "          [ 0.2869,    -inf,    -inf],\n",
      "          [ 0.0371,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5491,    -inf,    -inf],\n",
      "          [ 0.0321,    -inf,    -inf],\n",
      "          [ 0.1155,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2083,    -inf,    -inf],\n",
      "          [-0.2084,    -inf,    -inf],\n",
      "          [-0.2088,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2245,    -inf,    -inf],\n",
      "          [-0.5301,    -inf,    -inf],\n",
      "          [-0.0642,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0821,    -inf,    -inf],\n",
      "          [-0.3611,    -inf,    -inf],\n",
      "          [-0.0023,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5150, 0.4850, 0.0000],\n",
      "          [0.5846, 0.4154, 0.0000],\n",
      "          [0.5155, 0.4845, 0.0000]],\n",
      "\n",
      "         [[0.6351, 0.3649, 0.0000],\n",
      "          [0.5111, 0.4889, 0.0000],\n",
      "          [0.6530, 0.3470, 0.0000]],\n",
      "\n",
      "         [[0.3980, 0.6020, 0.0000],\n",
      "          [0.4528, 0.5472, 0.0000],\n",
      "          [0.5850, 0.4150, 0.0000]],\n",
      "\n",
      "         [[0.5680, 0.4320, 0.0000],\n",
      "          [0.5240, 0.4760, 0.0000],\n",
      "          [0.5172, 0.4828, 0.0000]],\n",
      "\n",
      "         [[0.5205, 0.4795, 0.0000],\n",
      "          [0.5286, 0.4714, 0.0000],\n",
      "          [0.4638, 0.5362, 0.0000]],\n",
      "\n",
      "         [[0.4264, 0.5736, 0.0000],\n",
      "          [0.3531, 0.6469, 0.0000],\n",
      "          [0.4407, 0.5593, 0.0000]],\n",
      "\n",
      "         [[0.3039, 0.6961, 0.0000],\n",
      "          [0.3576, 0.6424, 0.0000],\n",
      "          [0.3864, 0.6136, 0.0000]],\n",
      "\n",
      "         [[0.5553, 0.4447, 0.0000],\n",
      "          [0.4446, 0.5554, 0.0000],\n",
      "          [0.4830, 0.5170, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1716, -0.1001,    -inf],\n",
      "          [-0.0996,  0.0762,    -inf],\n",
      "          [-0.3687, -0.1120,    -inf]],\n",
      "\n",
      "         [[ 0.5162, -0.2971,    -inf],\n",
      "          [ 0.7544,  0.6513,    -inf],\n",
      "          [ 0.3292,  0.2090,    -inf]],\n",
      "\n",
      "         [[-0.2754, -0.2182,    -inf],\n",
      "          [-0.2302,  0.4774,    -inf],\n",
      "          [-0.3575, -0.0762,    -inf]],\n",
      "\n",
      "         [[-0.0036,  0.0344,    -inf],\n",
      "          [-0.0404,  0.2035,    -inf],\n",
      "          [-0.9728, -0.6388,    -inf]],\n",
      "\n",
      "         [[-0.2704,  0.2165,    -inf],\n",
      "          [-0.1933,  0.3440,    -inf],\n",
      "          [ 0.0719,  0.4599,    -inf]],\n",
      "\n",
      "         [[-0.4752,  0.0678,    -inf],\n",
      "          [ 0.3459, -0.0229,    -inf],\n",
      "          [ 0.5775,  0.1148,    -inf]],\n",
      "\n",
      "         [[-0.2321,  0.1339,    -inf],\n",
      "          [ 0.2659,  0.2435,    -inf],\n",
      "          [-0.0854, -0.1844,    -inf]],\n",
      "\n",
      "         [[ 0.0202, -0.3161,    -inf],\n",
      "          [ 0.3010,  0.0810,    -inf],\n",
      "          [ 0.3342, -0.2794,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.5993,    -inf,    -inf],\n",
      "          [-0.0739,    -inf,    -inf],\n",
      "          [-0.5283,    -inf,    -inf]],\n",
      "\n",
      "         [[ 1.0096,    -inf,    -inf],\n",
      "          [ 0.9120,    -inf,    -inf],\n",
      "          [ 0.6267,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3296,    -inf,    -inf],\n",
      "          [ 0.1935,    -inf,    -inf],\n",
      "          [ 0.3557,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1152,    -inf,    -inf],\n",
      "          [ 0.2568,    -inf,    -inf],\n",
      "          [-0.0322,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1605,    -inf,    -inf],\n",
      "          [-0.5434,    -inf,    -inf],\n",
      "          [-0.0716,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0113,    -inf,    -inf],\n",
      "          [ 0.3034,    -inf,    -inf],\n",
      "          [ 0.3755,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1578,    -inf,    -inf],\n",
      "          [-0.4838,    -inf,    -inf],\n",
      "          [-0.4245,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2390,    -inf,    -inf],\n",
      "          [-0.3135,    -inf,    -inf],\n",
      "          [ 0.1095,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4821, 0.5179, 0.0000],\n",
      "          [0.4562, 0.5438, 0.0000],\n",
      "          [0.4362, 0.5638, 0.0000]],\n",
      "\n",
      "         [[0.6928, 0.3072, 0.0000],\n",
      "          [0.5258, 0.4742, 0.0000],\n",
      "          [0.5300, 0.4700, 0.0000]],\n",
      "\n",
      "         [[0.4857, 0.5143, 0.0000],\n",
      "          [0.3301, 0.6699, 0.0000],\n",
      "          [0.4301, 0.5699, 0.0000]],\n",
      "\n",
      "         [[0.4905, 0.5095, 0.0000],\n",
      "          [0.4393, 0.5607, 0.0000],\n",
      "          [0.4173, 0.5827, 0.0000]],\n",
      "\n",
      "         [[0.3806, 0.6194, 0.0000],\n",
      "          [0.3688, 0.6312, 0.0000],\n",
      "          [0.4042, 0.5958, 0.0000]],\n",
      "\n",
      "         [[0.3675, 0.6325, 0.0000],\n",
      "          [0.5912, 0.4088, 0.0000],\n",
      "          [0.6137, 0.3863, 0.0000]],\n",
      "\n",
      "         [[0.4095, 0.5905, 0.0000],\n",
      "          [0.5056, 0.4944, 0.0000],\n",
      "          [0.5247, 0.4753, 0.0000]],\n",
      "\n",
      "         [[0.5833, 0.4167, 0.0000],\n",
      "          [0.5548, 0.4452, 0.0000],\n",
      "          [0.6488, 0.3512, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 1.5836e-01,  1.5838e-01,        -inf],\n",
      "          [ 2.2561e-01,  1.7343e-01,        -inf],\n",
      "          [ 1.0106e-01,  3.9696e-01,        -inf]],\n",
      "\n",
      "         [[-2.2095e-01,  4.1779e-01,        -inf],\n",
      "          [ 7.0731e-02,  3.1606e-01,        -inf],\n",
      "          [ 1.2412e-01,  5.2098e-01,        -inf]],\n",
      "\n",
      "         [[ 8.0194e-01,  1.2434e-01,        -inf],\n",
      "          [-1.7488e-01, -3.4077e-01,        -inf],\n",
      "          [ 4.7817e-01,  2.6708e-01,        -inf]],\n",
      "\n",
      "         [[-2.1060e-01, -2.5022e-01,        -inf],\n",
      "          [ 1.0621e-01, -6.0138e-01,        -inf],\n",
      "          [ 6.1078e-02, -2.2241e-01,        -inf]],\n",
      "\n",
      "         [[ 1.2728e-01,  5.4601e-04,        -inf],\n",
      "          [ 4.1314e-01,  1.9335e-01,        -inf],\n",
      "          [-1.4881e-02, -1.2227e-01,        -inf]],\n",
      "\n",
      "         [[-1.5882e-01, -4.1322e-02,        -inf],\n",
      "          [-2.0119e-02,  7.5021e-01,        -inf],\n",
      "          [-2.5774e-01,  1.0451e-02,        -inf]],\n",
      "\n",
      "         [[-1.3586e-02,  9.2966e-02,        -inf],\n",
      "          [ 2.8702e-01,  2.9571e-02,        -inf],\n",
      "          [ 3.5809e-01,  5.9997e-01,        -inf]],\n",
      "\n",
      "         [[-6.4374e-01,  4.0089e-02,        -inf],\n",
      "          [-1.0832e+00,  1.6016e-02,        -inf],\n",
      "          [-8.8489e-01,  4.4489e-02,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5745e-01,        -inf,        -inf],\n",
      "          [-1.9585e-01,        -inf,        -inf],\n",
      "          [ 2.7390e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[-2.4333e-01,        -inf,        -inf],\n",
      "          [-2.0909e-01,        -inf,        -inf],\n",
      "          [-3.8230e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 6.9874e-01,        -inf,        -inf],\n",
      "          [ 4.8207e-01,        -inf,        -inf],\n",
      "          [ 5.5650e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 5.5065e-02,        -inf,        -inf],\n",
      "          [-8.4766e-02,        -inf,        -inf],\n",
      "          [ 1.6888e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[-3.3634e-01,        -inf,        -inf],\n",
      "          [-1.3488e-01,        -inf,        -inf],\n",
      "          [ 1.1869e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 4.8653e-01,        -inf,        -inf],\n",
      "          [ 2.3960e-02,        -inf,        -inf],\n",
      "          [ 4.9505e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[-1.8651e-01,        -inf,        -inf],\n",
      "          [ 8.2111e-02,        -inf,        -inf],\n",
      "          [-3.5211e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[-5.0377e-02,        -inf,        -inf],\n",
      "          [-4.0616e-01,        -inf,        -inf],\n",
      "          [-6.2113e-01,        -inf,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5000, 0.5000, 0.0000],\n",
      "          [0.5130, 0.4870, 0.0000],\n",
      "          [0.4266, 0.5734, 0.0000]],\n",
      "\n",
      "         [[0.3455, 0.6545, 0.0000],\n",
      "          [0.4390, 0.5610, 0.0000],\n",
      "          [0.4021, 0.5979, 0.0000]],\n",
      "\n",
      "         [[0.6632, 0.3368, 0.0000],\n",
      "          [0.5414, 0.4586, 0.0000],\n",
      "          [0.5526, 0.4474, 0.0000]],\n",
      "\n",
      "         [[0.5099, 0.4901, 0.0000],\n",
      "          [0.6699, 0.3301, 0.0000],\n",
      "          [0.5704, 0.4296, 0.0000]],\n",
      "\n",
      "         [[0.5316, 0.4684, 0.0000],\n",
      "          [0.5547, 0.4453, 0.0000],\n",
      "          [0.5268, 0.4732, 0.0000]],\n",
      "\n",
      "         [[0.4707, 0.5293, 0.0000],\n",
      "          [0.3164, 0.6836, 0.0000],\n",
      "          [0.4334, 0.5666, 0.0000]],\n",
      "\n",
      "         [[0.4734, 0.5266, 0.0000],\n",
      "          [0.5640, 0.4360, 0.0000],\n",
      "          [0.4398, 0.5602, 0.0000]],\n",
      "\n",
      "         [[0.3354, 0.6646, 0.0000],\n",
      "          [0.2499, 0.7501, 0.0000],\n",
      "          [0.2831, 0.7169, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x, self_mask = None):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask=self_mask)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "encoder(x, self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.3372,    -inf,    -inf],\n",
      "          [-0.2703,    -inf,    -inf],\n",
      "          [-0.2615,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0415,    -inf,    -inf],\n",
      "          [-0.1248,    -inf,    -inf],\n",
      "          [-0.1250,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2189,    -inf,    -inf],\n",
      "          [-0.2664,    -inf,    -inf],\n",
      "          [-0.3544,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0070,    -inf,    -inf],\n",
      "          [-0.0170,    -inf,    -inf],\n",
      "          [ 0.0187,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3368, -0.3447,    -inf],\n",
      "          [-0.3586, -0.1568,    -inf],\n",
      "          [-0.2514, -0.2337,    -inf]],\n",
      "\n",
      "         [[ 0.0364,  0.0679,    -inf],\n",
      "          [ 0.0441, -0.0261,    -inf],\n",
      "          [-0.0408, -0.1311,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5020, 0.4980, 0.0000],\n",
      "          [0.4497, 0.5503, 0.0000],\n",
      "          [0.4956, 0.5044, 0.0000]],\n",
      "\n",
      "         [[0.4921, 0.5079, 0.0000],\n",
      "          [0.5175, 0.4825, 0.0000],\n",
      "          [0.5226, 0.4774, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0598, -0.2029, -0.0657],\n",
      "          [-0.1006, -0.1429, -0.0339],\n",
      "          [-0.3775, -0.4889, -0.3555]],\n",
      "\n",
      "         [[-0.2248, -0.2132, -0.0569],\n",
      "          [-0.0746, -0.3279, -0.4983],\n",
      "          [-0.2788, -0.6229, -0.9149]]],\n",
      "\n",
      "\n",
      "        [[[-0.1209,    -inf,    -inf],\n",
      "          [-0.3603,    -inf,    -inf],\n",
      "          [-0.2184,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3268,    -inf,    -inf],\n",
      "          [-0.7799,    -inf,    -inf],\n",
      "          [-0.2128,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1673, -0.1973,    -inf],\n",
      "          [-0.0985, -0.0868,    -inf],\n",
      "          [ 0.2885,  0.4675,    -inf]],\n",
      "\n",
      "         [[-0.5256, -0.5742,    -inf],\n",
      "          [-0.3863, -0.3942,    -inf],\n",
      "          [-0.3189, -0.3150,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3495, 0.3029, 0.3475],\n",
      "          [0.3303, 0.3166, 0.3531],\n",
      "          [0.3429, 0.3067, 0.3505]],\n",
      "\n",
      "         [[0.3130, 0.3167, 0.3703],\n",
      "          [0.4114, 0.3193, 0.2693],\n",
      "          [0.4468, 0.3167, 0.2365]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5075, 0.4925, 0.0000],\n",
      "          [0.4971, 0.5029, 0.0000],\n",
      "          [0.4554, 0.5446, 0.0000]],\n",
      "\n",
      "         [[0.5122, 0.4878, 0.0000],\n",
      "          [0.5020, 0.4980, 0.0000],\n",
      "          [0.4990, 0.5010, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = dec_x.size()\n",
    "        x = dec_x + self.attn_dropout(self.mhsa(dec_x, dec_x, dec_x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, enc_x, enc_x, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1814,    -inf,    -inf],\n",
      "          [ 0.1285,    -inf,    -inf],\n",
      "          [-0.1112,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2331,    -inf,    -inf],\n",
      "          [-0.6291,    -inf,    -inf],\n",
      "          [-0.0124,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0882,    -inf,    -inf],\n",
      "          [ 0.0636,    -inf,    -inf],\n",
      "          [ 0.1732,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.8352,    -inf,    -inf],\n",
      "          [-0.0262,    -inf,    -inf],\n",
      "          [ 0.2353,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1567,  0.0190,    -inf],\n",
      "          [ 0.1670,  0.3132,    -inf],\n",
      "          [ 0.0628,  0.1927,    -inf]],\n",
      "\n",
      "         [[-0.4262, -0.2413,    -inf],\n",
      "          [-0.0261,  0.0867,    -inf],\n",
      "          [ 0.0382,  0.2407,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5344, 0.4656, 0.0000],\n",
      "          [0.4635, 0.5365, 0.0000],\n",
      "          [0.4676, 0.5324, 0.0000]],\n",
      "\n",
      "         [[0.4539, 0.5461, 0.0000],\n",
      "          [0.4718, 0.5282, 0.0000],\n",
      "          [0.4495, 0.5505, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.3431,  0.3874,  0.2324],\n",
      "          [-0.1428, -0.1282, -0.2558],\n",
      "          [ 0.5211,  0.4056,  0.3042]],\n",
      "\n",
      "         [[-0.0692,  0.1168,  0.0775],\n",
      "          [ 0.0904,  0.1753, -0.0220],\n",
      "          [ 0.0818,  0.0905,  0.0177]]],\n",
      "\n",
      "\n",
      "        [[[-0.1553,    -inf,    -inf],\n",
      "          [ 0.4961,    -inf,    -inf],\n",
      "          [ 0.5270,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2120,    -inf,    -inf],\n",
      "          [ 0.0952,    -inf,    -inf],\n",
      "          [ 0.0352,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1717, -0.0781,    -inf],\n",
      "          [ 0.2582,  0.1648,    -inf],\n",
      "          [ 0.3421,  0.1890,    -inf]],\n",
      "\n",
      "         [[ 0.0143,  0.0634,    -inf],\n",
      "          [ 0.0441,  0.0026,    -inf],\n",
      "          [ 0.0351, -0.0255,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3401, 0.3555, 0.3044],\n",
      "          [0.3439, 0.3489, 0.3071],\n",
      "          [0.3709, 0.3305, 0.2986]],\n",
      "\n",
      "         [[0.2974, 0.3582, 0.3444],\n",
      "          [0.3353, 0.3650, 0.2997],\n",
      "          [0.3394, 0.3423, 0.3183]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5621, 0.4379, 0.0000],\n",
      "          [0.5233, 0.4767, 0.0000],\n",
      "          [0.5382, 0.4618, 0.0000]],\n",
      "\n",
      "         [[0.4877, 0.5123, 0.0000],\n",
      "          [0.5104, 0.4896, 0.0000],\n",
      "          [0.5151, 0.4849, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0449,    -inf,    -inf],\n",
      "          [-0.3818,    -inf,    -inf],\n",
      "          [-0.2065,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1148,    -inf,    -inf],\n",
      "          [ 0.7371,    -inf,    -inf],\n",
      "          [ 0.6518,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0564,    -inf,    -inf],\n",
      "          [-0.0759,    -inf,    -inf],\n",
      "          [-0.0058,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3177,    -inf,    -inf],\n",
      "          [ 0.4324,    -inf,    -inf],\n",
      "          [ 0.6676,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1801,  0.2125,    -inf],\n",
      "          [ 0.3628,  0.4406,    -inf],\n",
      "          [ 0.2468,  0.1868,    -inf]],\n",
      "\n",
      "         [[ 0.1479,  0.2209,    -inf],\n",
      "          [ 0.2917,  0.4128,    -inf],\n",
      "          [ 0.7478,  0.7335,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4919, 0.5081, 0.0000],\n",
      "          [0.4806, 0.5194, 0.0000],\n",
      "          [0.5150, 0.4850, 0.0000]],\n",
      "\n",
      "         [[0.4818, 0.5182, 0.0000],\n",
      "          [0.4697, 0.5303, 0.0000],\n",
      "          [0.5036, 0.4964, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0086,  0.0729,  0.1302],\n",
      "          [-0.1952, -0.0013,  0.0437],\n",
      "          [-0.0304,  0.0376,  0.1311]],\n",
      "\n",
      "         [[-0.0335, -0.0912, -0.0520],\n",
      "          [ 0.1240,  0.1343,  0.0541],\n",
      "          [ 0.0796,  0.0786,  0.1157]]],\n",
      "\n",
      "\n",
      "        [[[-0.0752,    -inf,    -inf],\n",
      "          [ 0.0061,    -inf,    -inf],\n",
      "          [ 0.0038,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1885,    -inf,    -inf],\n",
      "          [ 0.1422,    -inf,    -inf],\n",
      "          [ 0.1385,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0238, -0.1921,    -inf],\n",
      "          [ 0.0011,  0.0318,    -inf],\n",
      "          [ 0.1248, -0.0138,    -inf]],\n",
      "\n",
      "         [[-0.0331, -0.0165,    -inf],\n",
      "          [ 0.3249,  0.3804,    -inf],\n",
      "          [-0.0209,  0.0273,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3129, 0.3337, 0.3534],\n",
      "          [0.2870, 0.3485, 0.3645],\n",
      "          [0.3081, 0.3298, 0.3621]],\n",
      "\n",
      "         [[0.3418, 0.3227, 0.3355],\n",
      "          [0.3398, 0.3433, 0.3169],\n",
      "          [0.3294, 0.3291, 0.3415]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5538, 0.4462, 0.0000],\n",
      "          [0.4923, 0.5077, 0.0000],\n",
      "          [0.5346, 0.4654, 0.0000]],\n",
      "\n",
      "         [[0.4958, 0.5042, 0.0000],\n",
      "          [0.4861, 0.5139, 0.0000],\n",
      "          [0.4880, 0.5120, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [DecoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=self_mask, cross_mask=cross_mask):\n",
    "        b, t = dec_x.size()\n",
    "        x = self.embed(dec_x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_x, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "    def get_embed_weights(self):\n",
    "        return self.embed.weight\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b5aba42-4209-4fbb-8cc9-a9e8a0236e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Output(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, d: int = 512, ff_weight = None):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(d, vocab_size)\n",
    "        # weight tying with the decoder embedding\n",
    "        if ff_weight is not None:\n",
    "            self.ff.weight = ff_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2477aec-7287-498b-bf31-cbb0bdfe154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: \n",
      " tensor([[1, 1, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 1]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.4327,  0.1332,  0.8796],\n",
      "          [ 0.3176, -0.0848,  0.6358],\n",
      "          [ 0.6411,  0.4102,  0.7332]],\n",
      "\n",
      "         [[ 0.1669,  0.0483,  0.2508],\n",
      "          [ 0.8063,  0.1612,  0.5078],\n",
      "          [ 0.0907, -0.2746,  0.0381]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0689,  0.3154,    -inf],\n",
      "          [ 0.2397,  0.2922,    -inf],\n",
      "          [ 0.0916,  0.2613,    -inf]],\n",
      "\n",
      "         [[ 0.3067,  0.5630,    -inf],\n",
      "          [ 0.2814,  0.6763,    -inf],\n",
      "          [ 0.0569,  0.1825,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0593,    -inf,    -inf],\n",
      "          [ 0.3711,    -inf,    -inf],\n",
      "          [ 0.2092,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1513,    -inf,    -inf],\n",
      "          [ 0.5066,    -inf,    -inf],\n",
      "          [ 0.3477,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3026, 0.2243, 0.4731],\n",
      "          [0.3286, 0.2197, 0.4517],\n",
      "          [0.3460, 0.2746, 0.3794]],\n",
      "\n",
      "         [[0.3361, 0.2985, 0.3655],\n",
      "          [0.4412, 0.2314, 0.3273],\n",
      "          [0.3784, 0.2626, 0.3590]]],\n",
      "\n",
      "\n",
      "        [[[0.4387, 0.5613, 0.0000],\n",
      "          [0.4869, 0.5131, 0.0000],\n",
      "          [0.4577, 0.5423, 0.0000]],\n",
      "\n",
      "         [[0.4363, 0.5637, 0.0000],\n",
      "          [0.4025, 0.5975, 0.0000],\n",
      "          [0.4686, 0.5314, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.2807, -0.1075, -0.0046],\n",
      "          [ 0.3483, -0.0903, -0.0204],\n",
      "          [ 0.2782, -0.5084,  0.1910]],\n",
      "\n",
      "         [[-0.2778, -0.3308, -0.2751],\n",
      "          [-0.1993, -0.2607, -0.0548],\n",
      "          [ 0.0920, -0.2636, -0.1194]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6037, -0.4163,    -inf],\n",
      "          [ 0.2268, -0.3299,    -inf],\n",
      "          [ 0.4588,  0.3733,    -inf]],\n",
      "\n",
      "         [[-0.6463, -0.4191,    -inf],\n",
      "          [-1.0803, -0.5827,    -inf],\n",
      "          [-0.2513, -0.2861,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2594,    -inf,    -inf],\n",
      "          [-0.3348,    -inf,    -inf],\n",
      "          [-0.2841,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1084,    -inf,    -inf],\n",
      "          [ 0.0733,    -inf,    -inf],\n",
      "          [-0.2878,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4115, 0.2791, 0.3094],\n",
      "          [0.4280, 0.2760, 0.2960],\n",
      "          [0.4216, 0.1920, 0.3864]],\n",
      "\n",
      "         [[0.3389, 0.3214, 0.3398],\n",
      "          [0.3230, 0.3038, 0.3732],\n",
      "          [0.3984, 0.2792, 0.3225]]],\n",
      "\n",
      "\n",
      "        [[[0.7350, 0.2650, 0.0000],\n",
      "          [0.6357, 0.3643, 0.0000],\n",
      "          [0.5214, 0.4786, 0.0000]],\n",
      "\n",
      "         [[0.4435, 0.5565, 0.0000],\n",
      "          [0.3781, 0.6219, 0.0000],\n",
      "          [0.5087, 0.4913, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1005,    -inf,    -inf],\n",
      "          [ 0.2094,    -inf,    -inf],\n",
      "          [-0.8217,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3664,    -inf,    -inf],\n",
      "          [ 0.5289,    -inf,    -inf],\n",
      "          [-0.1446,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.6430,    -inf,    -inf],\n",
      "          [ 0.3623,    -inf,    -inf],\n",
      "          [ 0.4703,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2652,    -inf,    -inf],\n",
      "          [ 0.1011,    -inf,    -inf],\n",
      "          [-0.0208,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6883,  0.9261, -0.7565],\n",
      "          [-0.3591,  1.1313, -1.1321],\n",
      "          [ 0.7213,  0.9784,  0.5257]],\n",
      "\n",
      "         [[-0.1952, -0.5481,  0.5262],\n",
      "          [ 1.7371,  1.0009,  0.7103],\n",
      "          [-0.4115, -1.0477,  0.2346]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3993, 0.5065, 0.0942],\n",
      "          [0.1695, 0.7523, 0.0782],\n",
      "          [0.3210, 0.4151, 0.2640]],\n",
      "\n",
      "         [[0.2660, 0.1869, 0.5472],\n",
      "          [0.5443, 0.2607, 0.1950],\n",
      "          [0.2909, 0.1540, 0.5551]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0989,  0.0737,  0.2672],\n",
      "          [ 0.2243,  0.3713,  0.4450],\n",
      "          [ 0.0617,  0.1580,  0.4767]],\n",
      "\n",
      "         [[ 0.1257,  0.1490,  0.2890],\n",
      "          [-0.0633, -0.3375, -0.6600],\n",
      "          [ 0.2463,  0.0022,  0.3222]]],\n",
      "\n",
      "\n",
      "        [[[-0.1212,  0.3198,    -inf],\n",
      "          [-0.1531,  0.3742,    -inf],\n",
      "          [-0.0253,  0.1639,    -inf]],\n",
      "\n",
      "         [[-0.1211, -0.0189,    -inf],\n",
      "          [ 0.1212, -0.2286,    -inf],\n",
      "          [-0.0278, -0.1786,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0162,    -inf,    -inf],\n",
      "          [-0.2768,    -inf,    -inf],\n",
      "          [ 0.2060,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5485,    -inf,    -inf],\n",
      "          [ 1.2397,    -inf,    -inf],\n",
      "          [-0.0850,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3166, 0.3087, 0.3746],\n",
      "          [0.2937, 0.3402, 0.3662],\n",
      "          [0.2766, 0.3045, 0.4189]],\n",
      "\n",
      "         [[0.3124, 0.3198, 0.3678],\n",
      "          [0.4327, 0.3290, 0.2383],\n",
      "          [0.3494, 0.2737, 0.3769]]],\n",
      "\n",
      "\n",
      "        [[[0.3915, 0.6085, 0.0000],\n",
      "          [0.3711, 0.6289, 0.0000],\n",
      "          [0.4528, 0.5472, 0.0000]],\n",
      "\n",
      "         [[0.4745, 0.5255, 0.0000],\n",
      "          [0.5866, 0.4134, 0.0000],\n",
      "          [0.5376, 0.4624, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 2.9219e-01,        -inf,        -inf],\n",
      "          [-5.7408e-03,        -inf,        -inf],\n",
      "          [ 2.6511e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 3.0061e-01,        -inf,        -inf],\n",
      "          [ 8.2247e-01,        -inf,        -inf],\n",
      "          [ 4.6627e-01,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.5929e-01,        -inf,        -inf],\n",
      "          [-1.4688e-01,        -inf,        -inf],\n",
      "          [ 1.0517e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[-6.7495e-01,        -inf,        -inf],\n",
      "          [-2.1203e-01,        -inf,        -inf],\n",
      "          [-2.3922e-01,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-3.8002e-01, -4.8199e-02, -4.0988e-02],\n",
      "          [ 1.3471e-01, -2.4494e-01,  2.9151e-02],\n",
      "          [ 2.2095e-01, -2.5369e-01,  4.5539e-01]],\n",
      "\n",
      "         [[ 3.2280e-01, -5.1429e-01,  5.6541e-02],\n",
      "          [-9.4202e-02, -3.1672e-04,  1.6839e-01],\n",
      "          [-4.1013e-01, -1.9580e-01, -2.6339e-01]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2634, 0.3670, 0.3696],\n",
      "          [0.3870, 0.2648, 0.3482],\n",
      "          [0.3465, 0.2155, 0.4380]],\n",
      "\n",
      "         [[0.4547, 0.1969, 0.3484],\n",
      "          [0.2942, 0.3232, 0.3826],\n",
      "          [0.2944, 0.3647, 0.3409]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1145, -0.0623, -0.0738],\n",
      "          [-0.5513, -0.6622, -0.3227],\n",
      "          [ 0.2968,  0.0820,  0.2235]],\n",
      "\n",
      "         [[-0.3727, -0.2928, -0.4652],\n",
      "          [ 0.5150,  0.0897,  0.1494],\n",
      "          [-0.4143, -0.3682, -0.4694]]],\n",
      "\n",
      "\n",
      "        [[[-0.2036, -0.9430,    -inf],\n",
      "          [ 0.0403, -0.0511,    -inf],\n",
      "          [ 0.1107,  0.0335,    -inf]],\n",
      "\n",
      "         [[ 0.6582,  0.4239,    -inf],\n",
      "          [ 0.2333,  0.2489,    -inf],\n",
      "          [-0.1825, -0.0664,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1183,    -inf,    -inf],\n",
      "          [-0.2597,    -inf,    -inf],\n",
      "          [ 0.1246,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.7679,    -inf,    -inf],\n",
      "          [ 0.6170,    -inf,    -inf],\n",
      "          [ 0.2986,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3750, 0.3143, 0.3107],\n",
      "          [0.3173, 0.2840, 0.3988],\n",
      "          [0.3655, 0.2948, 0.3397]],\n",
      "\n",
      "         [[0.3339, 0.3617, 0.3044],\n",
      "          [0.4260, 0.2784, 0.2956],\n",
      "          [0.3341, 0.3498, 0.3161]]],\n",
      "\n",
      "\n",
      "        [[[0.6769, 0.3231, 0.0000],\n",
      "          [0.5228, 0.4772, 0.0000],\n",
      "          [0.5193, 0.4807, 0.0000]],\n",
      "\n",
      "         [[0.5583, 0.4417, 0.0000],\n",
      "          [0.4961, 0.5039, 0.0000],\n",
      "          [0.4710, 0.5290, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 3, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8, embed_tying=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        if embed_tying:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d, ff_weight = self.decoder.get_embed_weights())\n",
    "        else:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d)\n",
    "\n",
    "    def forward(self, enc_x, dec_x, enc_mask, dec_mask):\n",
    "        encoded = self.encoder(enc_x, enc_mask)\n",
    "        decoded = self.decoder(dec_x=dec_x, enc_x=encoded, self_mask=dec_mask, cross_mask=enc_mask)\n",
    "        return self.output(decoded)\n",
    "\n",
    "transformer = Transformer(vocab_size=32, n=2, d=16, h=2)\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "enc_x = torch.tensor([[15, 7, 3], [10, 10, 0], [1, 0, 0]])\n",
    "dec_x = torch.tensor([[21, 8, 0], [25, 0, 0], [8, 1, 2]])\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=0)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=0)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "print(transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f69bab-e9a5-44a7-af1d-f294f62d3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c43eb-c348-4905-9384-1b28378435b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082af878-9425-4445-b4b4-592cef79a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba170-623a-486b-8b67-58360bde104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d1a8-69aa-4c60-b3b4-ebb2e31f1c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
