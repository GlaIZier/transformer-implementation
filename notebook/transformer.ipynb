{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d7f736-b9af-48e3-adb5-18ab3536abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkhokhlush/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n",
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    print(softmaxed_prod.shape)\n",
    "    # print(softmaxed_prod)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x)\n",
    "self_attention(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abcca093-9e66-4b46-8ec3-aded63ece6f8",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a9e6b-8461-4417-ab07-e5fbd79c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c5bf9-3800-4b22-b7d8-8886d4f15b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d626059-fd88-4a03-8b20-ffbf0115fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayerWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayerWithoutMask()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851eff3d-4816-4410-9184-f30c157e4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayerWithoutMask(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = EncoderWithoutMask()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With masks\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim)\n",
    "    if mask is not None:\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    # print(softmaxed_prod.shape)\n",
    "    print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eac5e23-ea43-4295-ba24-a7edbe1a1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7577, 0.9060, 0.7230, 0.9500],\n",
      "          [0.6063, 0.0449, 0.7067, 0.5544]],\n",
      "\n",
      "         [[0.6978, 0.1751, 0.0026, 0.9557],\n",
      "          [0.9310, 0.8582, 0.4305, 0.2082]],\n",
      "\n",
      "         [[0.2590, 0.9259, 0.9320, 0.9326],\n",
      "          [0.4594, 0.2069, 0.3897, 0.9627]]],\n",
      "\n",
      "\n",
      "        [[[0.1810, 0.9472, 0.1688, 0.4276],\n",
      "          [0.3366, 0.1658, 0.4613, 0.7097]],\n",
      "\n",
      "         [[0.1066, 0.2280, 0.1131, 0.0694],\n",
      "          [0.8893, 0.6127, 0.1256, 0.5475]],\n",
      "\n",
      "         [[0.8238, 0.5106, 0.8772, 0.0728],\n",
      "          [0.4804, 0.8187, 0.2267, 0.7607]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[1.4100, 0.7986,   -inf],\n",
      "          [0.7986, 0.7155,   -inf],\n",
      "          [1.2974, 0.6183,   -inf]],\n",
      "\n",
      "         [[0.5882,   -inf,   -inf],\n",
      "          [0.5113,   -inf,   -inf],\n",
      "          [0.5485,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.5707, 0.1420,   -inf],\n",
      "          [0.1420, 0.0405,   -inf],\n",
      "          [0.4060, 0.1543,   -inf]],\n",
      "\n",
      "         [[0.4286,   -inf,   -inf],\n",
      "          [0.4237,   -inf,   -inf],\n",
      "          [0.4709,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6483, 0.3517, 0.0000],\n",
      "          [0.5208, 0.4792, 0.0000],\n",
      "          [0.6636, 0.3364, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6056, 0.3944, 0.0000],\n",
      "          [0.5254, 0.4746, 0.0000],\n",
      "          [0.5626, 0.4374, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "wrong a: \n",
      " tensor([[[[0.7366, 0.6489, 0.4696, 0.9520],\n",
      "          [0.7290, 0.5557, 0.3777, 0.9527],\n",
      "          [0.7375, 0.6601, 0.4806, 0.9519]],\n",
      "\n",
      "         [[0.6063, 0.0449, 0.7067, 0.5544],\n",
      "          [0.6063, 0.0449, 0.7067, 0.5544],\n",
      "          [0.6063, 0.0449, 0.7067, 0.5544]]],\n",
      "\n",
      "\n",
      "        [[[0.1516, 0.6635, 0.1469, 0.2863],\n",
      "          [0.1457, 0.6058, 0.1424, 0.2576],\n",
      "          [0.1484, 0.6326, 0.1445, 0.2709]],\n",
      "\n",
      "         [[0.3366, 0.1658, 0.4613, 0.7097],\n",
      "          [0.3366, 0.1658, 0.4613, 0.7097],\n",
      "          [0.3366, 0.1658, 0.4613, 0.7097]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[1.4100, 0.7986,   -inf],\n",
      "          [0.7986, 0.7155,   -inf],\n",
      "          [1.2974, 0.6183,   -inf]],\n",
      "\n",
      "         [[0.5882, 0.5113,   -inf],\n",
      "          [0.5113, 0.9160,   -inf],\n",
      "          [0.5485, 0.4867,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.5707,   -inf,   -inf],\n",
      "          [0.1420,   -inf,   -inf],\n",
      "          [0.4060,   -inf,   -inf]],\n",
      "\n",
      "         [[0.4286,   -inf,   -inf],\n",
      "          [0.4237,   -inf,   -inf],\n",
      "          [0.4709,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6483, 0.3517, 0.0000],\n",
      "          [0.5208, 0.4792, 0.0000],\n",
      "          [0.6636, 0.3364, 0.0000]],\n",
      "\n",
      "         [[0.5192, 0.4808, 0.0000],\n",
      "          [0.4002, 0.5998, 0.0000],\n",
      "          [0.5154, 0.4846, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.7366, 0.6489, 0.4696, 0.9520],\n",
      "          [0.7290, 0.5557, 0.3777, 0.9527],\n",
      "          [0.7375, 0.6601, 0.4806, 0.9519]],\n",
      "\n",
      "         [[0.7624, 0.4359, 0.5739, 0.3880],\n",
      "          [0.8010, 0.5327, 0.5410, 0.3468],\n",
      "          [0.7636, 0.4390, 0.5729, 0.3867]]],\n",
      "\n",
      "\n",
      "        [[[0.1810, 0.9472, 0.1688, 0.4276],\n",
      "          [0.1810, 0.9472, 0.1688, 0.4276],\n",
      "          [0.1810, 0.9472, 0.1688, 0.4276]],\n",
      "\n",
      "         [[0.3366, 0.1658, 0.4613, 0.7097],\n",
      "          [0.3366, 0.1658, 0.4613, 0.7097],\n",
      "          [0.3366, 0.1658, 0.4613, 0.7097]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.7577, 0.9060, 0.7230, 0.9500],\n",
      "          [0.6063, 0.0449, 0.7067, 0.5544]],\n",
      "\n",
      "         [[0.6978, 0.1751, 0.0026, 0.9557],\n",
      "          [0.9310, 0.8582, 0.4305, 0.2082]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.1810, 0.9472, 0.1688, 0.4276],\n",
      "          [0.3366, 0.1658, 0.4613, 0.7097]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[1.4100, 0.7986,   -inf],\n",
      "          [0.7986, 0.7155,   -inf],\n",
      "          [1.2974, 0.6183,   -inf]],\n",
      "\n",
      "         [[0.5882, 0.5113,   -inf],\n",
      "          [0.5113, 0.9160,   -inf],\n",
      "          [0.5485, 0.4867,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.5707,   -inf,   -inf],\n",
      "          [0.1420,   -inf,   -inf],\n",
      "          [0.4060,   -inf,   -inf]],\n",
      "\n",
      "         [[0.4286,   -inf,   -inf],\n",
      "          [0.4237,   -inf,   -inf],\n",
      "          [0.4709,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6483, 0.3517, 0.0000],\n",
      "          [0.5208, 0.4792, 0.0000],\n",
      "          [0.6636, 0.3364, 0.0000]],\n",
      "\n",
      "         [[0.5192, 0.4808, 0.0000],\n",
      "          [0.4002, 0.5998, 0.0000],\n",
      "          [0.5154, 0.4846, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.7366, 0.6489, 0.4696, 0.9520],\n",
      "          [0.7290, 0.5557, 0.3777, 0.9527],\n",
      "          [0.7375, 0.6601, 0.4806, 0.9519]],\n",
      "\n",
      "         [[0.7624, 0.4359, 0.5739, 0.3880],\n",
      "          [0.8010, 0.5327, 0.5410, 0.3468],\n",
      "          [0.7636, 0.4390, 0.5729, 0.3867]]],\n",
      "\n",
      "\n",
      "        [[[0.1810, 0.9472, 0.1688, 0.4276],\n",
      "          [0.1810, 0.9472, 0.1688, 0.4276],\n",
      "          [0.1810, 0.9472, 0.1688, 0.4276]],\n",
      "\n",
      "         [[0.3366, 0.1658, 0.4613, 0.7097],\n",
      "          [0.3366, 0.1658, 0.4613, 0.7097],\n",
      "          [0.3366, 0.1658, 0.4613, 0.7097]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.3404, 0.8537, 0.3342, 0.1061],\n",
      "         [0.6718, 0.2126, 0.5408, 0.0635],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3736, 0.8908, 0.9749, 0.1567],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.3404, 0.8537],\n",
      "          [0.3342, 0.1061]],\n",
      "\n",
      "         [[0.6718, 0.2126],\n",
      "          [0.5408, 0.0635]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3736, 0.8908],\n",
      "          [0.9749, 0.1567]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.3404, 0.8537],\n",
      "          [0.6718, 0.2126],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3342, 0.1061],\n",
      "          [0.5408, 0.0635],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3736, 0.8908],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.9749, 0.1567],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.8135, 0.9469, 0.4522, 0.1258],\n",
      "         [0.5559, 0.7649, 0.5074, 0.3948],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.0809, 0.5235, 0.5967, 0.9535],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.8135, 0.9469],\n",
      "          [0.4522, 0.1258]],\n",
      "\n",
      "         [[0.5559, 0.7649],\n",
      "          [0.5074, 0.3948]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.0809, 0.5235],\n",
      "          [0.5967, 0.9535]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.8135, 0.9469],\n",
      "          [0.5559, 0.7649],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.4522, 0.1258],\n",
      "          [0.5074, 0.3948],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.0809, 0.5235],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.5967, 0.9535],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[1.5584, 1.1765,   -inf],\n",
      "          [1.1765, 0.8940,   -inf],\n",
      "          [1.0751, 0.8080,   -inf]],\n",
      "\n",
      "         [[0.2203, 0.2791,   -inf],\n",
      "          [0.2791, 0.4133,   -inf],\n",
      "          [0.2848, 0.4836,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.2806,   -inf,   -inf],\n",
      "          [0.1640,   -inf,   -inf],\n",
      "          [0.0554,   -inf,   -inf]],\n",
      "\n",
      "         [[1.2653,   -inf,   -inf],\n",
      "          [0.6622,   -inf,   -inf],\n",
      "          [1.4057,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.6423e-02, 2.8367e-01, 2.9734e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [2.7809e-01, 3.4594e-01, 7.0030e-01, 5.5790e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [2.8416e-01, 5.6292e-01, 2.7772e-01, 2.1171e-01, 4.1438e-01, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [5.8055e-02, 6.6687e-01, 8.2529e-01, 7.1399e-01, 2.7075e-01, 4.5091e-01]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.8224,   0.1996,   0.5486, 100.0000, 100.0000, 100.0000],\n",
      "        [  0.7127,   0.1006,   0.3603,   0.2332,   0.1146, 100.0000],\n",
      "        [  0.3179,   0.7870, 100.0000, 100.0000, 100.0000, 100.0000],\n",
      "        [100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000],\n",
      "        [  0.6792,   0.4480,   0.2321,   0.4819,   0.8868,   0.4640]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(build_causal_mask(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0447,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0756,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1176,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0998,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0753,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1483,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0733,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1115,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1080,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0453,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0149, -0.0304,    -inf,    -inf,    -inf],\n",
      "          [-0.0232, -0.0135,    -inf,    -inf,    -inf],\n",
      "          [-0.0262, -0.0398,    -inf,    -inf,    -inf],\n",
      "          [-0.0208, -0.0575,    -inf,    -inf,    -inf],\n",
      "          [ 0.0038, -0.0325,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0552,  0.0408,    -inf,    -inf,    -inf],\n",
      "          [ 0.0198,  0.0363,    -inf,    -inf,    -inf],\n",
      "          [-0.0773, -0.0050,    -inf,    -inf,    -inf],\n",
      "          [-0.0105,  0.0312,    -inf,    -inf,    -inf],\n",
      "          [ 0.0964,  0.0738,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0108, -0.1023,  0.0072,    -inf,    -inf],\n",
      "          [-0.0412, -0.0029,  0.0016,    -inf,    -inf],\n",
      "          [-0.0256, -0.0676, -0.0026,    -inf,    -inf],\n",
      "          [-0.0488, -0.0791, -0.0271,    -inf,    -inf],\n",
      "          [ 0.0011, -0.0331,  0.0148,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1094, -0.0955, -0.1389,    -inf,    -inf],\n",
      "          [-0.0366,  0.0393, -0.0302,    -inf,    -inf],\n",
      "          [-0.1219, -0.0803, -0.1333,    -inf,    -inf],\n",
      "          [-0.1151, -0.0882, -0.1302,    -inf,    -inf],\n",
      "          [-0.1091, -0.1005, -0.1344,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0498, -0.0273, -0.0758, -0.0408,    -inf],\n",
      "          [ 0.0352,  0.1271, -0.0594,  0.0054,    -inf],\n",
      "          [-0.0524, -0.0051, -0.0747, -0.0444,    -inf],\n",
      "          [-0.0401,  0.0224, -0.0857, -0.0398,    -inf],\n",
      "          [-0.0234,  0.0357, -0.0437, -0.0244,    -inf]],\n",
      "\n",
      "         [[-0.1574, -0.1167, -0.1352, -0.1415,    -inf],\n",
      "          [-0.0792, -0.0335, -0.0552, -0.0650,    -inf],\n",
      "          [-0.0986, -0.0109, -0.0537, -0.0742,    -inf],\n",
      "          [-0.0988, -0.0257, -0.0614, -0.0780,    -inf],\n",
      "          [-0.0516,  0.0077, -0.0240, -0.0387,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5113, 0.4887, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4976, 0.5024, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5034, 0.4966, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5092, 0.4908, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5091, 0.4909, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5036, 0.4964, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4959, 0.5041, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4819, 0.5181, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4896, 0.5104, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5056, 0.4944, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3461, 0.3091, 0.3448, 0.0000, 0.0000],\n",
      "          [0.3244, 0.3371, 0.3386, 0.0000, 0.0000],\n",
      "          [0.3353, 0.3215, 0.3431, 0.0000, 0.0000],\n",
      "          [0.3342, 0.3242, 0.3416, 0.0000, 0.0000],\n",
      "          [0.3356, 0.3243, 0.3402, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3350, 0.3397, 0.3253, 0.0000, 0.0000],\n",
      "          [0.3241, 0.3497, 0.3262, 0.0000, 0.0000],\n",
      "          [0.3299, 0.3439, 0.3262, 0.0000, 0.0000],\n",
      "          [0.3320, 0.3410, 0.3270, 0.0000, 0.0000],\n",
      "          [0.3351, 0.3381, 0.3268, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2496, 0.2553, 0.2432, 0.2519, 0.0000],\n",
      "          [0.2515, 0.2757, 0.2288, 0.2441, 0.0000],\n",
      "          [0.2479, 0.2599, 0.2424, 0.2499, 0.0000],\n",
      "          [0.2487, 0.2648, 0.2377, 0.2488, 0.0000],\n",
      "          [0.2475, 0.2626, 0.2426, 0.2473, 0.0000]],\n",
      "\n",
      "         [[0.2451, 0.2553, 0.2506, 0.2490, 0.0000],\n",
      "          [0.2448, 0.2562, 0.2507, 0.2483, 0.0000],\n",
      "          [0.2403, 0.2623, 0.2513, 0.2462, 0.0000],\n",
      "          [0.2418, 0.2602, 0.2511, 0.2469, 0.0000],\n",
      "          [0.2438, 0.2587, 0.2506, 0.2470, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[ 0.1060, -0.0206, -0.4838, -0.0821,  0.4818, -0.2968],\n",
      "         [ 0.1060, -0.0206, -0.4838, -0.0821,  0.4818, -0.2968],\n",
      "         [ 0.1060, -0.0206, -0.4838, -0.0821,  0.4818, -0.2968],\n",
      "         [ 0.1060, -0.0206, -0.4838, -0.0821,  0.4818, -0.2968],\n",
      "         [ 0.1060, -0.0206, -0.4838, -0.0821,  0.4818, -0.2968]],\n",
      "\n",
      "        [[ 0.0192, -0.1475, -0.2536,  0.0090,  0.6098, -0.0451],\n",
      "         [ 0.0194, -0.1465, -0.2548,  0.0089,  0.6089, -0.0459],\n",
      "         [ 0.0185, -0.1457, -0.2549,  0.0095,  0.6081, -0.0467],\n",
      "         [ 0.0187, -0.1464, -0.2543,  0.0094,  0.6088, -0.0461],\n",
      "         [ 0.0194, -0.1475, -0.2537,  0.0089,  0.6099, -0.0451]],\n",
      "\n",
      "        [[ 0.1563, -0.1758, -0.3809, -0.1409,  0.6467, -0.1481],\n",
      "         [ 0.1578, -0.1775, -0.3800, -0.1420,  0.6483, -0.1475],\n",
      "         [ 0.1569, -0.1766, -0.3804, -0.1414,  0.6475, -0.1478],\n",
      "         [ 0.1571, -0.1765, -0.3808, -0.1417,  0.6475, -0.1482],\n",
      "         [ 0.1572, -0.1762, -0.3813, -0.1420,  0.6474, -0.1486]],\n",
      "\n",
      "        [[ 0.0938, -0.1317, -0.3486, -0.0716,  0.5961, -0.1468],\n",
      "         [ 0.0908, -0.1308, -0.3465, -0.0687,  0.5950, -0.1459],\n",
      "         [ 0.0946, -0.1329, -0.3481, -0.0728,  0.5978, -0.1459],\n",
      "         [ 0.0933, -0.1322, -0.3476, -0.0714,  0.5968, -0.1459],\n",
      "         [ 0.0938, -0.1320, -0.3484, -0.0716,  0.5965, -0.1462]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0447,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0756,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1176,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0998,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0753,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1483,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0733,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1115,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1080,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0453,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0149, -0.0304,    -inf,    -inf,    -inf],\n",
      "          [-0.0232, -0.0135,    -inf,    -inf,    -inf],\n",
      "          [-0.0262, -0.0398,    -inf,    -inf,    -inf],\n",
      "          [-0.0208, -0.0575,    -inf,    -inf,    -inf],\n",
      "          [ 0.0038, -0.0325,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0552,  0.0408,    -inf,    -inf,    -inf],\n",
      "          [ 0.0198,  0.0363,    -inf,    -inf,    -inf],\n",
      "          [-0.0773, -0.0050,    -inf,    -inf,    -inf],\n",
      "          [-0.0105,  0.0312,    -inf,    -inf,    -inf],\n",
      "          [ 0.0964,  0.0738,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0108, -0.1023,  0.0072,    -inf,    -inf],\n",
      "          [-0.0412, -0.0029,  0.0016,    -inf,    -inf],\n",
      "          [-0.0256, -0.0676, -0.0026,    -inf,    -inf],\n",
      "          [-0.0488, -0.0791, -0.0271,    -inf,    -inf],\n",
      "          [ 0.0011, -0.0331,  0.0148,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1094, -0.0955, -0.1389,    -inf,    -inf],\n",
      "          [-0.0366,  0.0393, -0.0302,    -inf,    -inf],\n",
      "          [-0.1219, -0.0803, -0.1333,    -inf,    -inf],\n",
      "          [-0.1151, -0.0882, -0.1302,    -inf,    -inf],\n",
      "          [-0.1091, -0.1005, -0.1344,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0498, -0.0273, -0.0758, -0.0408,    -inf],\n",
      "          [ 0.0352,  0.1271, -0.0594,  0.0054,    -inf],\n",
      "          [-0.0524, -0.0051, -0.0747, -0.0444,    -inf],\n",
      "          [-0.0401,  0.0224, -0.0857, -0.0398,    -inf],\n",
      "          [-0.0234,  0.0357, -0.0437, -0.0244,    -inf]],\n",
      "\n",
      "         [[-0.1574, -0.1167, -0.1352, -0.1415,    -inf],\n",
      "          [-0.0792, -0.0335, -0.0552, -0.0650,    -inf],\n",
      "          [-0.0986, -0.0109, -0.0537, -0.0742,    -inf],\n",
      "          [-0.0988, -0.0257, -0.0614, -0.0780,    -inf],\n",
      "          [-0.0516,  0.0077, -0.0240, -0.0387,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5113, 0.4887, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4976, 0.5024, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5034, 0.4966, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5092, 0.4908, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5091, 0.4909, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5036, 0.4964, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4959, 0.5041, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4819, 0.5181, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4896, 0.5104, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5056, 0.4944, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3461, 0.3091, 0.3448, 0.0000, 0.0000],\n",
      "          [0.3244, 0.3371, 0.3386, 0.0000, 0.0000],\n",
      "          [0.3353, 0.3215, 0.3431, 0.0000, 0.0000],\n",
      "          [0.3342, 0.3242, 0.3416, 0.0000, 0.0000],\n",
      "          [0.3356, 0.3243, 0.3402, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3350, 0.3397, 0.3253, 0.0000, 0.0000],\n",
      "          [0.3241, 0.3497, 0.3262, 0.0000, 0.0000],\n",
      "          [0.3299, 0.3439, 0.3262, 0.0000, 0.0000],\n",
      "          [0.3320, 0.3410, 0.3270, 0.0000, 0.0000],\n",
      "          [0.3351, 0.3381, 0.3268, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2496, 0.2553, 0.2432, 0.2519, 0.0000],\n",
      "          [0.2515, 0.2757, 0.2288, 0.2441, 0.0000],\n",
      "          [0.2479, 0.2599, 0.2424, 0.2499, 0.0000],\n",
      "          [0.2487, 0.2648, 0.2377, 0.2488, 0.0000],\n",
      "          [0.2475, 0.2626, 0.2426, 0.2473, 0.0000]],\n",
      "\n",
      "         [[0.2451, 0.2553, 0.2506, 0.2490, 0.0000],\n",
      "          [0.2448, 0.2562, 0.2507, 0.2483, 0.0000],\n",
      "          [0.2403, 0.2623, 0.2513, 0.2462, 0.0000],\n",
      "          [0.2418, 0.2602, 0.2511, 0.2469, 0.0000],\n",
      "          [0.2438, 0.2587, 0.2506, 0.2470, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729ae6f8-a3f8-4386-837d-f1e67f022b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0138, -0.1131,    -inf],\n",
      "          [-0.0717, -0.1310,    -inf],\n",
      "          [-0.1991, -0.2871,    -inf]],\n",
      "\n",
      "         [[ 0.0512, -0.0554,    -inf],\n",
      "          [ 0.0345, -0.0757,    -inf],\n",
      "          [ 0.0630, -0.0499,    -inf]],\n",
      "\n",
      "         [[-0.0876, -0.0904,    -inf],\n",
      "          [-0.0636, -0.0532,    -inf],\n",
      "          [-0.0091, -0.0932,    -inf]],\n",
      "\n",
      "         [[ 0.1765,  0.1944,    -inf],\n",
      "          [ 0.0701,  0.1181,    -inf],\n",
      "          [ 0.1089,  0.1160,    -inf]],\n",
      "\n",
      "         [[-0.0338,  0.0017,    -inf],\n",
      "          [ 0.0580,  0.0787,    -inf],\n",
      "          [ 0.0451,  0.1084,    -inf]],\n",
      "\n",
      "         [[ 0.0717,  0.0508,    -inf],\n",
      "          [ 0.0883,  0.0564,    -inf],\n",
      "          [ 0.0369,  0.0327,    -inf]],\n",
      "\n",
      "         [[ 0.2495,  0.1865,    -inf],\n",
      "          [ 0.0967,  0.1040,    -inf],\n",
      "          [ 0.0930,  0.1004,    -inf]],\n",
      "\n",
      "         [[ 0.2367,  0.2364,    -inf],\n",
      "          [ 0.1689,  0.2307,    -inf],\n",
      "          [ 0.0794,  0.1427,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0569,    -inf,    -inf],\n",
      "          [-0.0152,    -inf,    -inf],\n",
      "          [-0.1022,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0322,    -inf,    -inf],\n",
      "          [-0.0303,    -inf,    -inf],\n",
      "          [-0.0958,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1575,    -inf,    -inf],\n",
      "          [-0.0432,    -inf,    -inf],\n",
      "          [-0.1793,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0739,    -inf,    -inf],\n",
      "          [ 0.0943,    -inf,    -inf],\n",
      "          [ 0.1116,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0031,    -inf,    -inf],\n",
      "          [ 0.0004,    -inf,    -inf],\n",
      "          [-0.0939,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1834,    -inf,    -inf],\n",
      "          [ 0.1104,    -inf,    -inf],\n",
      "          [ 0.0505,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1345,    -inf,    -inf],\n",
      "          [ 0.1818,    -inf,    -inf],\n",
      "          [ 0.1823,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0906,    -inf,    -inf],\n",
      "          [ 0.1321,    -inf,    -inf],\n",
      "          [ 0.1952,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5248, 0.4752, 0.0000],\n",
      "          [0.5148, 0.4852, 0.0000],\n",
      "          [0.5220, 0.4780, 0.0000]],\n",
      "\n",
      "         [[0.5266, 0.4734, 0.0000],\n",
      "          [0.5275, 0.4725, 0.0000],\n",
      "          [0.5282, 0.4718, 0.0000]],\n",
      "\n",
      "         [[0.5007, 0.4993, 0.0000],\n",
      "          [0.4974, 0.5026, 0.0000],\n",
      "          [0.5210, 0.4790, 0.0000]],\n",
      "\n",
      "         [[0.4955, 0.5045, 0.0000],\n",
      "          [0.4880, 0.5120, 0.0000],\n",
      "          [0.4982, 0.5018, 0.0000]],\n",
      "\n",
      "         [[0.4911, 0.5089, 0.0000],\n",
      "          [0.4948, 0.5052, 0.0000],\n",
      "          [0.4842, 0.5158, 0.0000]],\n",
      "\n",
      "         [[0.5052, 0.4948, 0.0000],\n",
      "          [0.5080, 0.4920, 0.0000],\n",
      "          [0.5011, 0.4989, 0.0000]],\n",
      "\n",
      "         [[0.5157, 0.4843, 0.0000],\n",
      "          [0.4982, 0.5018, 0.0000],\n",
      "          [0.4982, 0.5018, 0.0000]],\n",
      "\n",
      "         [[0.5001, 0.4999, 0.0000],\n",
      "          [0.4846, 0.5154, 0.0000],\n",
      "          [0.4842, 0.5158, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, self_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "x = torch.rand(2, 3, 512)\n",
    "\n",
    "encoder_layer(x, self_mask=self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-3.4819e-01, -9.6599e-01,        -inf],\n",
      "          [-1.0084e+00, -1.0910e+00,        -inf],\n",
      "          [ 5.0857e-02, -3.7348e-01,        -inf]],\n",
      "\n",
      "         [[-1.1522e-01, -3.9379e-01,        -inf],\n",
      "          [-1.7539e-01, -1.0004e+00,        -inf],\n",
      "          [ 1.8132e-01, -9.6912e-02,        -inf]],\n",
      "\n",
      "         [[ 1.4321e+00, -3.4864e-01,        -inf],\n",
      "          [ 1.2077e-01,  3.2830e-01,        -inf],\n",
      "          [ 5.4350e-01,  4.3532e-01,        -inf]],\n",
      "\n",
      "         [[-1.0962e+00, -3.3000e-01,        -inf],\n",
      "          [-6.7391e-01,  5.8167e-02,        -inf],\n",
      "          [-5.0507e-01,  3.7956e-01,        -inf]],\n",
      "\n",
      "         [[ 7.8002e-01, -4.6078e-01,        -inf],\n",
      "          [-8.8403e-01,  3.4761e-01,        -inf],\n",
      "          [-3.6121e-01, -4.7940e-01,        -inf]],\n",
      "\n",
      "         [[ 3.8456e-01,  6.8160e-01,        -inf],\n",
      "          [ 4.2822e-01,  1.1415e+00,        -inf],\n",
      "          [ 2.4242e-01,  7.8037e-01,        -inf]],\n",
      "\n",
      "         [[-1.0444e+00, -7.0451e-01,        -inf],\n",
      "          [ 5.3525e-04,  5.3964e-01,        -inf],\n",
      "          [-4.2875e-01,  6.3320e-01,        -inf]],\n",
      "\n",
      "         [[-9.1320e-01, -5.2153e-01,        -inf],\n",
      "          [-9.1661e-01, -7.9991e-01,        -inf],\n",
      "          [-6.7086e-01,  4.2033e-01,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5628e-01,        -inf,        -inf],\n",
      "          [-2.3359e-01,        -inf,        -inf],\n",
      "          [-2.1556e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.4915e-01,        -inf,        -inf],\n",
      "          [-2.0829e-01,        -inf,        -inf],\n",
      "          [ 2.3626e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[-7.7355e-01,        -inf,        -inf],\n",
      "          [-2.1376e-01,        -inf,        -inf],\n",
      "          [ 6.7703e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[-3.7642e-01,        -inf,        -inf],\n",
      "          [ 7.4653e-01,        -inf,        -inf],\n",
      "          [-4.1510e-02,        -inf,        -inf]],\n",
      "\n",
      "         [[-9.7006e-01,        -inf,        -inf],\n",
      "          [ 2.3334e-02,        -inf,        -inf],\n",
      "          [-9.3315e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.0616e+00,        -inf,        -inf],\n",
      "          [ 3.4757e-01,        -inf,        -inf],\n",
      "          [ 9.2566e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 3.0539e-01,        -inf,        -inf],\n",
      "          [ 3.1384e-01,        -inf,        -inf],\n",
      "          [ 2.0229e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 6.1658e-01,        -inf,        -inf],\n",
      "          [-2.5398e-01,        -inf,        -inf],\n",
      "          [ 2.0067e-01,        -inf,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6497, 0.3503, 0.0000],\n",
      "          [0.5207, 0.4793, 0.0000],\n",
      "          [0.6045, 0.3955, 0.0000]],\n",
      "\n",
      "         [[0.5692, 0.4308, 0.0000],\n",
      "          [0.6953, 0.3047, 0.0000],\n",
      "          [0.5691, 0.4309, 0.0000]],\n",
      "\n",
      "         [[0.8558, 0.1442, 0.0000],\n",
      "          [0.4483, 0.5517, 0.0000],\n",
      "          [0.5270, 0.4730, 0.0000]],\n",
      "\n",
      "         [[0.3173, 0.6827, 0.0000],\n",
      "          [0.3247, 0.6753, 0.0000],\n",
      "          [0.2922, 0.7078, 0.0000]],\n",
      "\n",
      "         [[0.7757, 0.2243, 0.0000],\n",
      "          [0.2259, 0.7741, 0.0000],\n",
      "          [0.5295, 0.4705, 0.0000]],\n",
      "\n",
      "         [[0.4263, 0.5737, 0.0000],\n",
      "          [0.3289, 0.6711, 0.0000],\n",
      "          [0.3687, 0.6313, 0.0000]],\n",
      "\n",
      "         [[0.4158, 0.5842, 0.0000],\n",
      "          [0.3684, 0.6316, 0.0000],\n",
      "          [0.2569, 0.7431, 0.0000]],\n",
      "\n",
      "         [[0.4033, 0.5967, 0.0000],\n",
      "          [0.4709, 0.5291, 0.0000],\n",
      "          [0.2514, 0.7486, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1364, -0.8741,    -inf],\n",
      "          [ 0.1662, -0.2565,    -inf],\n",
      "          [ 0.5233,  0.0969,    -inf]],\n",
      "\n",
      "         [[ 0.1308,  0.1169,    -inf],\n",
      "          [ 0.1469, -0.1468,    -inf],\n",
      "          [ 0.3126,  0.0711,    -inf]],\n",
      "\n",
      "         [[-0.3299, -0.0410,    -inf],\n",
      "          [-0.4064,  0.1835,    -inf],\n",
      "          [-0.5932,  0.2574,    -inf]],\n",
      "\n",
      "         [[-0.2972, -0.2572,    -inf],\n",
      "          [ 0.1604,  0.3311,    -inf],\n",
      "          [-0.0618,  0.2174,    -inf]],\n",
      "\n",
      "         [[-0.3799, -0.1237,    -inf],\n",
      "          [-0.2568,  0.6173,    -inf],\n",
      "          [-0.2282, -0.4574,    -inf]],\n",
      "\n",
      "         [[ 0.0069,  0.4369,    -inf],\n",
      "          [ 0.5566,  0.3317,    -inf],\n",
      "          [ 0.5221,  0.0281,    -inf]],\n",
      "\n",
      "         [[ 0.0710,  0.0081,    -inf],\n",
      "          [-0.4671, -0.2628,    -inf],\n",
      "          [-0.2030,  0.2510,    -inf]],\n",
      "\n",
      "         [[-0.0033,  0.1787,    -inf],\n",
      "          [ 0.3487,  0.0121,    -inf],\n",
      "          [ 0.7773,  0.7423,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2109,    -inf,    -inf],\n",
      "          [ 0.0416,    -inf,    -inf],\n",
      "          [-0.7047,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0782,    -inf,    -inf],\n",
      "          [-0.2628,    -inf,    -inf],\n",
      "          [-0.1485,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6458,    -inf,    -inf],\n",
      "          [-0.3785,    -inf,    -inf],\n",
      "          [-0.4469,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1363,    -inf,    -inf],\n",
      "          [-0.1269,    -inf,    -inf],\n",
      "          [ 0.2682,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.7432,    -inf,    -inf],\n",
      "          [-0.2966,    -inf,    -inf],\n",
      "          [-0.3885,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0331,    -inf,    -inf],\n",
      "          [ 0.1348,    -inf,    -inf],\n",
      "          [ 0.3182,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4013,    -inf,    -inf],\n",
      "          [ 0.1183,    -inf,    -inf],\n",
      "          [ 0.1497,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1890,    -inf,    -inf],\n",
      "          [ 0.2080,    -inf,    -inf],\n",
      "          [ 0.5610,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6765, 0.3235, 0.0000],\n",
      "          [0.6041, 0.3959, 0.0000],\n",
      "          [0.6050, 0.3950, 0.0000]],\n",
      "\n",
      "         [[0.5035, 0.4965, 0.0000],\n",
      "          [0.5729, 0.4271, 0.0000],\n",
      "          [0.5601, 0.4399, 0.0000]],\n",
      "\n",
      "         [[0.4283, 0.5717, 0.0000],\n",
      "          [0.3567, 0.6433, 0.0000],\n",
      "          [0.2993, 0.7007, 0.0000]],\n",
      "\n",
      "         [[0.4900, 0.5100, 0.0000],\n",
      "          [0.4574, 0.5426, 0.0000],\n",
      "          [0.4306, 0.5694, 0.0000]],\n",
      "\n",
      "         [[0.4363, 0.5637, 0.0000],\n",
      "          [0.2944, 0.7056, 0.0000],\n",
      "          [0.5571, 0.4429, 0.0000]],\n",
      "\n",
      "         [[0.3941, 0.6059, 0.0000],\n",
      "          [0.5560, 0.4440, 0.0000],\n",
      "          [0.6210, 0.3790, 0.0000]],\n",
      "\n",
      "         [[0.5157, 0.4843, 0.0000],\n",
      "          [0.4491, 0.5509, 0.0000],\n",
      "          [0.3884, 0.6116, 0.0000]],\n",
      "\n",
      "         [[0.4546, 0.5454, 0.0000],\n",
      "          [0.5834, 0.4166, 0.0000],\n",
      "          [0.5087, 0.4913, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1311, -0.2263,    -inf],\n",
      "          [ 0.0942, -0.0358,    -inf],\n",
      "          [ 0.1706, -0.3722,    -inf]],\n",
      "\n",
      "         [[-0.4435, -0.6203,    -inf],\n",
      "          [-0.1574,  0.1319,    -inf],\n",
      "          [-0.4042, -0.3154,    -inf]],\n",
      "\n",
      "         [[ 0.2137, -0.5999,    -inf],\n",
      "          [ 0.2662,  0.9913,    -inf],\n",
      "          [-0.0768, -0.1814,    -inf]],\n",
      "\n",
      "         [[ 0.1452, -0.0280,    -inf],\n",
      "          [ 0.1021,  0.1994,    -inf],\n",
      "          [ 0.0850,  0.6648,    -inf]],\n",
      "\n",
      "         [[-0.8500,  0.2082,    -inf],\n",
      "          [ 0.1253, -0.0820,    -inf],\n",
      "          [-0.1258,  0.2846,    -inf]],\n",
      "\n",
      "         [[-0.3348, -0.0716,    -inf],\n",
      "          [-0.1771, -0.2376,    -inf],\n",
      "          [-0.1361, -0.0902,    -inf]],\n",
      "\n",
      "         [[-0.5168, -0.4172,    -inf],\n",
      "          [-0.4951, -0.5268,    -inf],\n",
      "          [-0.0716, -0.1684,    -inf]],\n",
      "\n",
      "         [[ 0.2197, -0.0122,    -inf],\n",
      "          [ 0.3325,  0.6155,    -inf],\n",
      "          [ 0.3470,  0.2206,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3790,    -inf,    -inf],\n",
      "          [ 0.2303,    -inf,    -inf],\n",
      "          [ 0.4677,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.6113,    -inf,    -inf],\n",
      "          [-0.1282,    -inf,    -inf],\n",
      "          [ 0.0410,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.8765,    -inf,    -inf],\n",
      "          [-0.0360,    -inf,    -inf],\n",
      "          [ 0.1297,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1045,    -inf,    -inf],\n",
      "          [ 0.0532,    -inf,    -inf],\n",
      "          [ 0.5788,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0147,    -inf,    -inf],\n",
      "          [ 0.1818,    -inf,    -inf],\n",
      "          [ 0.0386,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3445,    -inf,    -inf],\n",
      "          [-0.0914,    -inf,    -inf],\n",
      "          [-0.4295,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3203,    -inf,    -inf],\n",
      "          [-0.4889,    -inf,    -inf],\n",
      "          [-0.7248,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1099,    -inf,    -inf],\n",
      "          [-0.0457,    -inf,    -inf],\n",
      "          [ 0.2113,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5884, 0.4116, 0.0000],\n",
      "          [0.5324, 0.4676, 0.0000],\n",
      "          [0.6325, 0.3675, 0.0000]],\n",
      "\n",
      "         [[0.5441, 0.4559, 0.0000],\n",
      "          [0.4282, 0.5718, 0.0000],\n",
      "          [0.4778, 0.5222, 0.0000]],\n",
      "\n",
      "         [[0.6929, 0.3071, 0.0000],\n",
      "          [0.3263, 0.6737, 0.0000],\n",
      "          [0.5261, 0.4739, 0.0000]],\n",
      "\n",
      "         [[0.5432, 0.4568, 0.0000],\n",
      "          [0.4757, 0.5243, 0.0000],\n",
      "          [0.3590, 0.6410, 0.0000]],\n",
      "\n",
      "         [[0.2577, 0.7423, 0.0000],\n",
      "          [0.5516, 0.4484, 0.0000],\n",
      "          [0.3988, 0.6012, 0.0000]],\n",
      "\n",
      "         [[0.4346, 0.5654, 0.0000],\n",
      "          [0.5151, 0.4849, 0.0000],\n",
      "          [0.4885, 0.5115, 0.0000]],\n",
      "\n",
      "         [[0.4751, 0.5249, 0.0000],\n",
      "          [0.5079, 0.4921, 0.0000],\n",
      "          [0.5242, 0.4758, 0.0000]],\n",
      "\n",
      "         [[0.5577, 0.4423, 0.0000],\n",
      "          [0.4297, 0.5703, 0.0000],\n",
      "          [0.5316, 0.4684, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.5601, -0.5725,    -inf],\n",
      "          [-0.0017,  0.1242,    -inf],\n",
      "          [-0.1650,  0.2854,    -inf]],\n",
      "\n",
      "         [[-0.4693, -0.4090,    -inf],\n",
      "          [-0.2925, -0.3181,    -inf],\n",
      "          [-0.1386, -0.4151,    -inf]],\n",
      "\n",
      "         [[-0.0374,  0.1202,    -inf],\n",
      "          [-0.0901, -0.0868,    -inf],\n",
      "          [-0.6561, -0.5567,    -inf]],\n",
      "\n",
      "         [[-0.0817,  0.3789,    -inf],\n",
      "          [-0.0799,  0.2313,    -inf],\n",
      "          [-0.0389,  0.0605,    -inf]],\n",
      "\n",
      "         [[ 0.1574, -0.2241,    -inf],\n",
      "          [-0.0032, -0.4152,    -inf],\n",
      "          [-0.1079, -0.1480,    -inf]],\n",
      "\n",
      "         [[ 0.3230, -0.3094,    -inf],\n",
      "          [-0.6060,  0.0601,    -inf],\n",
      "          [ 0.3557, -0.3616,    -inf]],\n",
      "\n",
      "         [[-0.1493, -0.0969,    -inf],\n",
      "          [-0.4107, -0.2540,    -inf],\n",
      "          [-0.2508,  0.0413,    -inf]],\n",
      "\n",
      "         [[ 0.1491,  0.0016,    -inf],\n",
      "          [ 0.0378,  0.1683,    -inf],\n",
      "          [ 0.7130,  0.5782,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1428,    -inf,    -inf],\n",
      "          [-0.1574,    -inf,    -inf],\n",
      "          [-0.1616,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0933,    -inf,    -inf],\n",
      "          [ 0.5288,    -inf,    -inf],\n",
      "          [-0.2409,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1129,    -inf,    -inf],\n",
      "          [-0.1410,    -inf,    -inf],\n",
      "          [ 0.1083,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1248,    -inf,    -inf],\n",
      "          [ 0.0128,    -inf,    -inf],\n",
      "          [-0.2685,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3904,    -inf,    -inf],\n",
      "          [ 0.4401,    -inf,    -inf],\n",
      "          [-0.0154,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2485,    -inf,    -inf],\n",
      "          [ 0.5696,    -inf,    -inf],\n",
      "          [ 0.0480,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4579,    -inf,    -inf],\n",
      "          [-0.1825,    -inf,    -inf],\n",
      "          [-0.2693,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2929,    -inf,    -inf],\n",
      "          [ 0.3158,    -inf,    -inf],\n",
      "          [-0.3552,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5031, 0.4969, 0.0000],\n",
      "          [0.4686, 0.5314, 0.0000],\n",
      "          [0.3893, 0.6107, 0.0000]],\n",
      "\n",
      "         [[0.4849, 0.5151, 0.0000],\n",
      "          [0.5064, 0.4936, 0.0000],\n",
      "          [0.5687, 0.4313, 0.0000]],\n",
      "\n",
      "         [[0.4607, 0.5393, 0.0000],\n",
      "          [0.4992, 0.5008, 0.0000],\n",
      "          [0.4752, 0.5248, 0.0000]],\n",
      "\n",
      "         [[0.3868, 0.6132, 0.0000],\n",
      "          [0.4228, 0.5772, 0.0000],\n",
      "          [0.4752, 0.5248, 0.0000]],\n",
      "\n",
      "         [[0.5942, 0.4058, 0.0000],\n",
      "          [0.6016, 0.3984, 0.0000],\n",
      "          [0.5100, 0.4900, 0.0000]],\n",
      "\n",
      "         [[0.6530, 0.3470, 0.0000],\n",
      "          [0.3394, 0.6606, 0.0000],\n",
      "          [0.6720, 0.3280, 0.0000]],\n",
      "\n",
      "         [[0.4869, 0.5131, 0.0000],\n",
      "          [0.4609, 0.5391, 0.0000],\n",
      "          [0.4275, 0.5725, 0.0000]],\n",
      "\n",
      "         [[0.5368, 0.4632, 0.0000],\n",
      "          [0.4674, 0.5326, 0.0000],\n",
      "          [0.5336, 0.4664, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-8.9159e-02,  4.0806e-01,        -inf],\n",
      "          [ 4.3278e-01,  4.1378e-01,        -inf],\n",
      "          [ 1.6109e-02,  6.2863e-01,        -inf]],\n",
      "\n",
      "         [[-2.3256e-01,  5.2379e-01,        -inf],\n",
      "          [ 5.0238e-01,  1.0578e-01,        -inf],\n",
      "          [ 5.7711e-02,  1.4046e-01,        -inf]],\n",
      "\n",
      "         [[-3.0171e-01, -4.0062e-03,        -inf],\n",
      "          [ 1.2667e-01,  1.9249e-01,        -inf],\n",
      "          [-5.9999e-02, -3.1842e-01,        -inf]],\n",
      "\n",
      "         [[ 3.6921e-01,  2.8455e-01,        -inf],\n",
      "          [ 7.2897e-02, -2.3314e-01,        -inf],\n",
      "          [-1.1967e-01, -1.5761e-01,        -inf]],\n",
      "\n",
      "         [[ 6.7048e-01,  1.9500e-01,        -inf],\n",
      "          [ 2.1893e-01, -9.0200e-02,        -inf],\n",
      "          [ 2.1359e-01,  4.6520e-04,        -inf]],\n",
      "\n",
      "         [[-3.3407e-01, -2.5214e-01,        -inf],\n",
      "          [ 4.2911e-01, -2.0843e-01,        -inf],\n",
      "          [ 9.3179e-02, -4.4344e-01,        -inf]],\n",
      "\n",
      "         [[ 5.8815e-01,  3.6500e-01,        -inf],\n",
      "          [ 7.4663e-01,  2.9348e-01,        -inf],\n",
      "          [ 7.8279e-01,  2.6781e-01,        -inf]],\n",
      "\n",
      "         [[ 5.0917e-01, -1.0764e-01,        -inf],\n",
      "          [ 1.1700e-02, -1.8387e-02,        -inf],\n",
      "          [ 1.0107e-01,  2.8785e-01,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3321e-01,        -inf,        -inf],\n",
      "          [ 3.3088e-01,        -inf,        -inf],\n",
      "          [-6.2410e-02,        -inf,        -inf]],\n",
      "\n",
      "         [[-5.2079e-01,        -inf,        -inf],\n",
      "          [-2.2478e-01,        -inf,        -inf],\n",
      "          [-3.4277e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.2146e-01,        -inf,        -inf],\n",
      "          [ 1.1708e-02,        -inf,        -inf],\n",
      "          [ 4.4005e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[-4.6188e-01,        -inf,        -inf],\n",
      "          [-4.8016e-01,        -inf,        -inf],\n",
      "          [-4.7277e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.0459e-01,        -inf,        -inf],\n",
      "          [ 3.2253e-01,        -inf,        -inf],\n",
      "          [-2.1470e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[-2.8846e-01,        -inf,        -inf],\n",
      "          [-1.5874e-01,        -inf,        -inf],\n",
      "          [-1.7987e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 4.5434e-01,        -inf,        -inf],\n",
      "          [ 2.5119e-01,        -inf,        -inf],\n",
      "          [ 2.7983e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 2.5881e-01,        -inf,        -inf],\n",
      "          [-2.3731e-01,        -inf,        -inf],\n",
      "          [-8.7794e-02,        -inf,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3782, 0.6218, 0.0000],\n",
      "          [0.5047, 0.4953, 0.0000],\n",
      "          [0.3515, 0.6485, 0.0000]],\n",
      "\n",
      "         [[0.3194, 0.6806, 0.0000],\n",
      "          [0.5979, 0.4021, 0.0000],\n",
      "          [0.4793, 0.5207, 0.0000]],\n",
      "\n",
      "         [[0.4261, 0.5739, 0.0000],\n",
      "          [0.4836, 0.5164, 0.0000],\n",
      "          [0.5642, 0.4358, 0.0000]],\n",
      "\n",
      "         [[0.5212, 0.4788, 0.0000],\n",
      "          [0.5759, 0.4241, 0.0000],\n",
      "          [0.5095, 0.4905, 0.0000]],\n",
      "\n",
      "         [[0.6167, 0.3833, 0.0000],\n",
      "          [0.5767, 0.4233, 0.0000],\n",
      "          [0.5531, 0.4469, 0.0000]],\n",
      "\n",
      "         [[0.4795, 0.5205, 0.0000],\n",
      "          [0.6542, 0.3458, 0.0000],\n",
      "          [0.6310, 0.3690, 0.0000]],\n",
      "\n",
      "         [[0.5556, 0.4444, 0.0000],\n",
      "          [0.6114, 0.3886, 0.0000],\n",
      "          [0.6260, 0.3740, 0.0000]],\n",
      "\n",
      "         [[0.6495, 0.3505, 0.0000],\n",
      "          [0.5075, 0.4925, 0.0000],\n",
      "          [0.4534, 0.5466, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.4223, -0.0157,    -inf],\n",
      "          [ 0.1553, -0.0593,    -inf],\n",
      "          [-0.0240,  0.1912,    -inf]],\n",
      "\n",
      "         [[ 0.2023, -0.3917,    -inf],\n",
      "          [ 0.3858,  0.2718,    -inf],\n",
      "          [ 0.5877, -0.1253,    -inf]],\n",
      "\n",
      "         [[-1.1470,  0.1663,    -inf],\n",
      "          [-0.8609,  0.0958,    -inf],\n",
      "          [-0.3313,  0.1399,    -inf]],\n",
      "\n",
      "         [[-0.0165, -0.2044,    -inf],\n",
      "          [ 0.3071,  0.2537,    -inf],\n",
      "          [-0.1245, -0.2716,    -inf]],\n",
      "\n",
      "         [[-0.3730, -0.2452,    -inf],\n",
      "          [-0.1941, -0.5434,    -inf],\n",
      "          [-0.2615, -0.1458,    -inf]],\n",
      "\n",
      "         [[ 0.3385,  0.4678,    -inf],\n",
      "          [-0.1625,  0.1412,    -inf],\n",
      "          [-0.0431,  0.1519,    -inf]],\n",
      "\n",
      "         [[-0.4029, -0.2814,    -inf],\n",
      "          [-0.1011,  0.3658,    -inf],\n",
      "          [-0.1766,  0.1320,    -inf]],\n",
      "\n",
      "         [[-0.2396, -0.9422,    -inf],\n",
      "          [ 0.1187, -0.4926,    -inf],\n",
      "          [ 0.0101, -0.2880,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4333,    -inf,    -inf],\n",
      "          [ 0.5015,    -inf,    -inf],\n",
      "          [ 0.0930,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1807,    -inf,    -inf],\n",
      "          [ 0.0289,    -inf,    -inf],\n",
      "          [-0.0173,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1312,    -inf,    -inf],\n",
      "          [ 0.0325,    -inf,    -inf],\n",
      "          [-0.1111,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2906,    -inf,    -inf],\n",
      "          [ 0.3642,    -inf,    -inf],\n",
      "          [-0.2423,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1161,    -inf,    -inf],\n",
      "          [ 0.3900,    -inf,    -inf],\n",
      "          [-0.0728,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.9951,    -inf,    -inf],\n",
      "          [-0.7061,    -inf,    -inf],\n",
      "          [-0.4344,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3109,    -inf,    -inf],\n",
      "          [ 0.1428,    -inf,    -inf],\n",
      "          [ 0.1770,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0034,    -inf,    -inf],\n",
      "          [-0.0693,    -inf,    -inf],\n",
      "          [ 0.0832,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6078, 0.3922, 0.0000],\n",
      "          [0.5534, 0.4466, 0.0000],\n",
      "          [0.4464, 0.5536, 0.0000]],\n",
      "\n",
      "         [[0.6443, 0.3557, 0.0000],\n",
      "          [0.5285, 0.4715, 0.0000],\n",
      "          [0.6711, 0.3289, 0.0000]],\n",
      "\n",
      "         [[0.2119, 0.7881, 0.0000],\n",
      "          [0.2775, 0.7225, 0.0000],\n",
      "          [0.3843, 0.6157, 0.0000]],\n",
      "\n",
      "         [[0.5468, 0.4532, 0.0000],\n",
      "          [0.5133, 0.4867, 0.0000],\n",
      "          [0.5367, 0.4633, 0.0000]],\n",
      "\n",
      "         [[0.4681, 0.5319, 0.0000],\n",
      "          [0.5865, 0.4135, 0.0000],\n",
      "          [0.4711, 0.5289, 0.0000]],\n",
      "\n",
      "         [[0.4677, 0.5323, 0.0000],\n",
      "          [0.4247, 0.5753, 0.0000],\n",
      "          [0.4514, 0.5486, 0.0000]],\n",
      "\n",
      "         [[0.4697, 0.5303, 0.0000],\n",
      "          [0.3853, 0.6147, 0.0000],\n",
      "          [0.4235, 0.5765, 0.0000]],\n",
      "\n",
      "         [[0.6688, 0.3312, 0.0000],\n",
      "          [0.6482, 0.3518, 0.0000],\n",
      "          [0.5740, 0.4260, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x, self_mask = None):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask=self_mask)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "encoder(x, self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1762,    -inf,    -inf],\n",
      "          [ 0.0689,    -inf,    -inf],\n",
      "          [ 0.1165,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0080,    -inf,    -inf],\n",
      "          [ 0.0103,    -inf,    -inf],\n",
      "          [ 0.0217,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0650,    -inf,    -inf],\n",
      "          [ 0.2666,    -inf,    -inf],\n",
      "          [-0.0464,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0322,    -inf,    -inf],\n",
      "          [-0.1587,    -inf,    -inf],\n",
      "          [-0.0562,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1190,  0.0704,    -inf],\n",
      "          [ 0.0547,  0.0566,    -inf],\n",
      "          [ 0.0734,  0.0607,    -inf]],\n",
      "\n",
      "         [[-0.0423, -0.0187,    -inf],\n",
      "          [-0.0498, -0.0490,    -inf],\n",
      "          [-0.0530, -0.0532,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5121, 0.4879, 0.0000],\n",
      "          [0.4995, 0.5005, 0.0000],\n",
      "          [0.5032, 0.4968, 0.0000]],\n",
      "\n",
      "         [[0.4941, 0.5059, 0.0000],\n",
      "          [0.4998, 0.5002, 0.0000],\n",
      "          [0.5001, 0.4999, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0561, -0.0164,  0.1969],\n",
      "          [ 0.0632,  0.0993,  0.2345],\n",
      "          [ 0.1733,  0.1210,  0.3302]],\n",
      "\n",
      "         [[-0.3017, -0.2881, -0.3514],\n",
      "          [-0.2473, -0.3144, -0.4042],\n",
      "          [-0.3500, -0.4208, -0.5493]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0945,    -inf,    -inf],\n",
      "          [-0.0405,    -inf,    -inf],\n",
      "          [-0.0050,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2494,    -inf,    -inf],\n",
      "          [-0.2538,    -inf,    -inf],\n",
      "          [-0.2296,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0205,  0.0303,    -inf],\n",
      "          [-0.0150,  0.0634,    -inf],\n",
      "          [ 0.0032,  0.2129,    -inf]],\n",
      "\n",
      "         [[-0.3862, -0.3377,    -inf],\n",
      "          [-0.2070, -0.0399,    -inf],\n",
      "          [-0.2755, -0.1472,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3246, 0.3018, 0.3736],\n",
      "          [0.3102, 0.3216, 0.3682],\n",
      "          [0.3206, 0.3043, 0.3751]],\n",
      "\n",
      "         [[0.3372, 0.3419, 0.3209],\n",
      "          [0.3584, 0.3352, 0.3064],\n",
      "          [0.3635, 0.3387, 0.2978]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4873, 0.5127, 0.0000],\n",
      "          [0.4804, 0.5196, 0.0000],\n",
      "          [0.4478, 0.5522, 0.0000]],\n",
      "\n",
      "         [[0.4879, 0.5121, 0.0000],\n",
      "          [0.4583, 0.5417, 0.0000],\n",
      "          [0.4680, 0.5320, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = dec_x.size()\n",
    "        x = dec_x + self.attn_dropout(self.mhsa(dec_x, dec_x, dec_x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, enc_x, enc_x, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.5455,    -inf,    -inf],\n",
      "          [-0.7486,    -inf,    -inf],\n",
      "          [ 0.4464,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2762,    -inf,    -inf],\n",
      "          [-0.4434,    -inf,    -inf],\n",
      "          [ 0.6578,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.0599,    -inf,    -inf],\n",
      "          [-1.4474,    -inf,    -inf],\n",
      "          [-1.6861,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2073,    -inf,    -inf],\n",
      "          [ 0.5623,    -inf,    -inf],\n",
      "          [ 0.4858,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2773,  0.2538,    -inf],\n",
      "          [-0.3311,  0.3636,    -inf],\n",
      "          [ 0.2938,  0.4350,    -inf]],\n",
      "\n",
      "         [[ 0.0601,  0.0843,    -inf],\n",
      "          [ 0.7677,  0.7050,    -inf],\n",
      "          [ 0.6450, -0.0937,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3702, 0.6298, 0.0000],\n",
      "          [0.3330, 0.6670, 0.0000],\n",
      "          [0.4648, 0.5352, 0.0000]],\n",
      "\n",
      "         [[0.4940, 0.5060, 0.0000],\n",
      "          [0.5157, 0.4843, 0.0000],\n",
      "          [0.6767, 0.3233, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0496, -0.0122,  0.0701],\n",
      "          [-0.0477, -0.0487, -0.1080],\n",
      "          [-0.0339, -0.0807,  0.0571]],\n",
      "\n",
      "         [[ 0.1367, -0.0605,  0.0032],\n",
      "          [ 0.3087,  0.1280,  0.2627],\n",
      "          [ 0.1426,  0.1810,  0.0420]]],\n",
      "\n",
      "\n",
      "        [[[-0.0782,    -inf,    -inf],\n",
      "          [ 0.0321,    -inf,    -inf],\n",
      "          [ 0.0406,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1072,    -inf,    -inf],\n",
      "          [ 0.2074,    -inf,    -inf],\n",
      "          [ 0.1168,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2140,  0.0177,    -inf],\n",
      "          [ 0.0004,  0.0371,    -inf],\n",
      "          [-0.1237, -0.0675,    -inf]],\n",
      "\n",
      "         [[ 0.2205,  0.3375,    -inf],\n",
      "          [ 0.0927, -0.0442,    -inf],\n",
      "          [ 0.1724,  0.1291,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3378, 0.3175, 0.3447],\n",
      "          [0.3401, 0.3398, 0.3202],\n",
      "          [0.3279, 0.3129, 0.3592]],\n",
      "\n",
      "         [[0.3709, 0.3045, 0.3245],\n",
      "          [0.3585, 0.2992, 0.3423],\n",
      "          [0.3397, 0.3530, 0.3072]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4423, 0.5577, 0.0000],\n",
      "          [0.4908, 0.5092, 0.0000],\n",
      "          [0.4859, 0.5141, 0.0000]],\n",
      "\n",
      "         [[0.4708, 0.5292, 0.0000],\n",
      "          [0.5342, 0.4658, 0.0000],\n",
      "          [0.5108, 0.4892, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.5637,    -inf,    -inf],\n",
      "          [ 0.1955,    -inf,    -inf],\n",
      "          [-0.3623,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0024,    -inf,    -inf],\n",
      "          [ 0.0562,    -inf,    -inf],\n",
      "          [-0.2163,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0430,    -inf,    -inf],\n",
      "          [ 0.1331,    -inf,    -inf],\n",
      "          [ 0.0909,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4215,    -inf,    -inf],\n",
      "          [ 0.6404,    -inf,    -inf],\n",
      "          [ 0.2836,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3939,  0.3637,    -inf],\n",
      "          [-0.8440,  0.1175,    -inf],\n",
      "          [-0.2585,  0.5298,    -inf]],\n",
      "\n",
      "         [[ 0.1840, -0.4540,    -inf],\n",
      "          [-0.3239,  0.2280,    -inf],\n",
      "          [ 0.2108,  0.1385,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5075, 0.4925, 0.0000],\n",
      "          [0.2766, 0.7234, 0.0000],\n",
      "          [0.3125, 0.6875, 0.0000]],\n",
      "\n",
      "         [[0.6543, 0.3457, 0.0000],\n",
      "          [0.3654, 0.6346, 0.0000],\n",
      "          [0.5181, 0.4819, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1382, -0.0497, -0.0606],\n",
      "          [-0.1741,  0.1585,  0.0273],\n",
      "          [-0.3757, -0.4314, -0.2853]],\n",
      "\n",
      "         [[-0.0031,  0.0087, -0.0309],\n",
      "          [ 0.0720,  0.1919,  0.1560],\n",
      "          [ 0.2395,  0.1937,  0.1730]]],\n",
      "\n",
      "\n",
      "        [[[-0.0909,    -inf,    -inf],\n",
      "          [-0.2462,    -inf,    -inf],\n",
      "          [-0.2757,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0929,    -inf,    -inf],\n",
      "          [ 0.0506,    -inf,    -inf],\n",
      "          [ 0.0812,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0202, -0.0340,    -inf],\n",
      "          [ 0.2124,  0.2535,    -inf],\n",
      "          [-0.2087, -0.3445,    -inf]],\n",
      "\n",
      "         [[-0.0903, -0.0255,    -inf],\n",
      "          [ 0.1069,  0.0875,    -inf],\n",
      "          [ 0.0724,  0.0940,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3151, 0.3443, 0.3406],\n",
      "          [0.2764, 0.3855, 0.3381],\n",
      "          [0.3289, 0.3111, 0.3600]],\n",
      "\n",
      "         [[0.3351, 0.3390, 0.3259],\n",
      "          [0.3110, 0.3507, 0.3383],\n",
      "          [0.3459, 0.3304, 0.3237]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5035, 0.4965, 0.0000],\n",
      "          [0.4897, 0.5103, 0.0000],\n",
      "          [0.5339, 0.4661, 0.0000]],\n",
      "\n",
      "         [[0.4838, 0.5162, 0.0000],\n",
      "          [0.5048, 0.4952, 0.0000],\n",
      "          [0.4946, 0.5054, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [DecoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=self_mask, cross_mask=cross_mask):\n",
    "        b, t = dec_x.size()\n",
    "        x = self.embed(dec_x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_x, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "    def get_embed_weights(self):\n",
    "        return self.embed.weight\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b5aba42-4209-4fbb-8cc9-a9e8a0236e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Output(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, d: int = 512, ff_weight = None):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(d, vocab_size)\n",
    "        # weight tying with the decoder embedding\n",
    "        if ff_weight is not None:\n",
    "            self.ff.weight = ff_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2477aec-7287-498b-bf31-cbb0bdfe154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: \n",
      " tensor([[1, 1, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 1]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.3163, -0.4204,  0.2447],\n",
      "          [-0.2666, -1.2326, -0.5926],\n",
      "          [-0.1814,  0.0949, -0.4774]],\n",
      "\n",
      "         [[ 0.0432,  0.1339,  0.0038],\n",
      "          [ 0.2903, -0.2122,  0.1938],\n",
      "          [ 0.2361,  0.1003,  0.2385]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2112,  0.8207,    -inf],\n",
      "          [ 0.9672,  0.4356,    -inf],\n",
      "          [-0.0881, -0.5046,    -inf]],\n",
      "\n",
      "         [[ 0.1077, -0.1028,    -inf],\n",
      "          [-0.0283, -0.2030,    -inf],\n",
      "          [ 0.5097,  1.0081,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.1653,    -inf,    -inf],\n",
      "          [ 0.6043,    -inf,    -inf],\n",
      "          [ 0.6063,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5251,    -inf,    -inf],\n",
      "          [-0.7017,    -inf,    -inf],\n",
      "          [-0.4575,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4150, 0.1987, 0.3863],\n",
      "          [0.4756, 0.1810, 0.3433],\n",
      "          [0.3266, 0.4305, 0.2429]],\n",
      "\n",
      "         [[0.3272, 0.3583, 0.3146],\n",
      "          [0.3979, 0.2407, 0.3613],\n",
      "          [0.3478, 0.3036, 0.3486]]],\n",
      "\n",
      "\n",
      "        [[[0.5964, 0.4036, 0.0000],\n",
      "          [0.6299, 0.3701, 0.0000],\n",
      "          [0.6026, 0.3974, 0.0000]],\n",
      "\n",
      "         [[0.5524, 0.4476, 0.0000],\n",
      "          [0.5436, 0.4564, 0.0000],\n",
      "          [0.3779, 0.6221, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.7348, -0.3693, -1.1584],\n",
      "          [-0.1683, -0.0788, -0.1876],\n",
      "          [-0.0519, -0.6736, -0.6060]],\n",
      "\n",
      "         [[ 0.0721, -0.1840, -0.0622],\n",
      "          [ 0.3183,  0.4310, -0.0517],\n",
      "          [ 0.1610,  0.4655, -0.0969]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0376,  0.2821,    -inf],\n",
      "          [-0.0699,  0.0249,    -inf],\n",
      "          [-0.0362,  0.0845,    -inf]],\n",
      "\n",
      "         [[ 0.2251,  0.1376,    -inf],\n",
      "          [ 0.3426,  0.1741,    -inf],\n",
      "          [-0.4394, -0.1682,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.4682,    -inf,    -inf],\n",
      "          [-0.5027,    -inf,    -inf],\n",
      "          [-0.3033,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1352,    -inf,    -inf],\n",
      "          [-0.1882,    -inf,    -inf],\n",
      "          [ 0.1810,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3230, 0.4655, 0.2115],\n",
      "          [0.3253, 0.3557, 0.3190],\n",
      "          [0.4736, 0.2543, 0.2721]],\n",
      "\n",
      "         [[0.3776, 0.2923, 0.3301],\n",
      "          [0.3559, 0.3983, 0.2458],\n",
      "          [0.3196, 0.4334, 0.2470]]],\n",
      "\n",
      "\n",
      "        [[[0.4392, 0.5608, 0.0000],\n",
      "          [0.4763, 0.5237, 0.0000],\n",
      "          [0.4699, 0.5301, 0.0000]],\n",
      "\n",
      "         [[0.5219, 0.4781, 0.0000],\n",
      "          [0.5420, 0.4580, 0.0000],\n",
      "          [0.4326, 0.5674, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.6164,    -inf,    -inf],\n",
      "          [-0.1406,    -inf,    -inf],\n",
      "          [-0.6422,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3040,    -inf,    -inf],\n",
      "          [-0.0171,    -inf,    -inf],\n",
      "          [-0.8555,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.7212,    -inf,    -inf],\n",
      "          [-0.5550,    -inf,    -inf],\n",
      "          [-0.6815,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.7638,    -inf,    -inf],\n",
      "          [-1.1385,    -inf,    -inf],\n",
      "          [-1.1663,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0364,  0.4076, -0.7419],\n",
      "          [-0.1932,  0.5181, -0.8179],\n",
      "          [-0.6280, -1.4564,  0.3820]],\n",
      "\n",
      "         [[ 0.3206,  0.1306,  0.1095],\n",
      "          [-0.7238, -0.6697,  0.2589],\n",
      "          [ 0.2811, -0.4353,  0.0167]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3438, 0.4983, 0.1579],\n",
      "          [0.2800, 0.5701, 0.1499],\n",
      "          [0.2391, 0.1044, 0.6565]],\n",
      "\n",
      "         [[0.3793, 0.3136, 0.3071],\n",
      "          [0.2115, 0.2233, 0.5652],\n",
      "          [0.4432, 0.2165, 0.3402]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-7.1994e-02, -4.8399e-04, -2.9622e-01],\n",
      "          [-1.7108e-01, -2.7795e-01, -1.4071e-01],\n",
      "          [ 5.4418e-03, -9.6418e-02,  5.4518e-02]],\n",
      "\n",
      "         [[ 5.4656e-01, -3.2614e-01,  5.2804e-01],\n",
      "          [-5.3359e-01, -4.5635e-01, -1.2999e-02],\n",
      "          [ 3.7534e-01,  3.3671e-01,  1.5028e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6164e-01, -4.1012e-02,        -inf],\n",
      "          [ 1.8720e-02,  9.7200e-02,        -inf],\n",
      "          [-1.0355e-01,  3.2018e-01,        -inf]],\n",
      "\n",
      "         [[-5.7419e-01, -1.8664e-01,        -inf],\n",
      "          [-2.6360e-01, -9.6467e-02,        -inf],\n",
      "          [-1.2071e-01, -1.2982e-02,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 6.9647e-01,        -inf,        -inf],\n",
      "          [ 5.5865e-02,        -inf,        -inf],\n",
      "          [ 8.4481e-02,        -inf,        -inf]],\n",
      "\n",
      "         [[ 8.2685e-02,        -inf,        -inf],\n",
      "          [ 2.6153e-01,        -inf,        -inf],\n",
      "          [ 4.0558e-02,        -inf,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3480, 0.3738, 0.2781],\n",
      "          [0.3414, 0.3068, 0.3519],\n",
      "          [0.3386, 0.3058, 0.3556]],\n",
      "\n",
      "         [[0.4168, 0.1741, 0.4091],\n",
      "          [0.2657, 0.2871, 0.4472],\n",
      "          [0.3622, 0.3485, 0.2892]]],\n",
      "\n",
      "\n",
      "        [[[0.4699, 0.5301, 0.0000],\n",
      "          [0.4804, 0.5196, 0.0000],\n",
      "          [0.3956, 0.6044, 0.0000]],\n",
      "\n",
      "         [[0.4043, 0.5957, 0.0000],\n",
      "          [0.4583, 0.5417, 0.0000],\n",
      "          [0.4731, 0.5269, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2078,    -inf,    -inf],\n",
      "          [-0.1855,    -inf,    -inf],\n",
      "          [-0.0764,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.6994,    -inf,    -inf],\n",
      "          [-0.1099,    -inf,    -inf],\n",
      "          [-0.5136,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1905,    -inf,    -inf],\n",
      "          [-0.1501,    -inf,    -inf],\n",
      "          [ 0.0381,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2656,    -inf,    -inf],\n",
      "          [-0.2364,    -inf,    -inf],\n",
      "          [-0.2748,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1291, -0.2544,  0.0104],\n",
      "          [ 0.6005,  0.3160,  0.3536],\n",
      "          [-0.1585, -0.1525, -0.4235]],\n",
      "\n",
      "         [[-0.5534, -0.1026, -0.1501],\n",
      "          [-0.0261,  0.0902, -0.0663],\n",
      "          [-0.3514,  0.3087, -0.5891]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3298, 0.2910, 0.3792],\n",
      "          [0.3947, 0.2970, 0.3083],\n",
      "          [0.3606, 0.3628, 0.2767]],\n",
      "\n",
      "         [[0.2459, 0.3860, 0.3681],\n",
      "          [0.3243, 0.3643, 0.3115],\n",
      "          [0.2686, 0.5197, 0.2118]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.2467,  0.0825, -0.0320],\n",
      "          [ 0.0818,  0.3570,  0.2022],\n",
      "          [-0.0692, -0.0622, -0.5037]],\n",
      "\n",
      "         [[ 0.0978, -0.3928, -0.6692],\n",
      "          [ 0.0174, -0.9287, -0.1488],\n",
      "          [ 0.5493,  0.0309, -0.6969]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0185, -0.3496,    -inf],\n",
      "          [ 0.0656, -0.0966,    -inf],\n",
      "          [ 0.0198, -0.0787,    -inf]],\n",
      "\n",
      "         [[-0.2848, -0.1017,    -inf],\n",
      "          [-0.3193, -0.4442,    -inf],\n",
      "          [-0.5534, -0.7718,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1423,    -inf,    -inf],\n",
      "          [-0.1658,    -inf,    -inf],\n",
      "          [ 0.1192,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1948,    -inf,    -inf],\n",
      "          [-0.0294,    -inf,    -inf],\n",
      "          [ 0.3740,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3838, 0.3257, 0.2905],\n",
      "          [0.2903, 0.3823, 0.3274],\n",
      "          [0.3767, 0.3794, 0.2439]],\n",
      "\n",
      "         [[0.4815, 0.2948, 0.2236],\n",
      "          [0.4474, 0.1737, 0.3789],\n",
      "          [0.5311, 0.3162, 0.1527]]],\n",
      "\n",
      "\n",
      "        [[[0.5910, 0.4090, 0.0000],\n",
      "          [0.5405, 0.4595, 0.0000],\n",
      "          [0.5246, 0.4754, 0.0000]],\n",
      "\n",
      "         [[0.4543, 0.5457, 0.0000],\n",
      "          [0.5312, 0.4688, 0.0000],\n",
      "          [0.5544, 0.4456, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 3, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8, embed_tying=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        if embed_tying:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d, ff_weight = self.decoder.get_embed_weights())\n",
    "        else:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d)\n",
    "\n",
    "    def forward(self, enc_x, dec_x, enc_mask, dec_mask):\n",
    "        encoded = self.encoder(enc_x, enc_mask)\n",
    "        decoded = self.decoder(dec_x=dec_x, enc_x=encoded, self_mask=dec_mask, cross_mask=enc_mask)\n",
    "        return self.output(decoded)\n",
    "\n",
    "transformer = Transformer(vocab_size=32, n=2, d=16, h=2)\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "enc_x = torch.tensor([[15, 7, 3], [10, 10, 0], [1, 0, 0]])\n",
    "dec_x = torch.tensor([[21, 8, 0], [25, 0, 0], [8, 1, 2]])\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=0)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=0)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "print(transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f69bab-e9a5-44a7-af1d-f294f62d3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a06c43eb-c348-4905-9384-1b28378435b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 1609, 5963, 374, 2294, 0]\n",
      "tensor([[  9906,   4435, 100257, 100257, 100257],\n",
      "        [  2028,    374,    264,   4382,  11914],\n",
      "        [  7979, 100257, 100257, 100257, 100257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(encoding.encode(\"tiktoken is great!\"))\n",
    "\n",
    "sents = [\"Hello World\", \"This is a simple sentence\", \"Me\"]\n",
    "encoded_sents = [encoding.encode(s) for s in sents]\n",
    "inputs = pad_sequence([torch.tensor(es) for es in encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082af878-9425-4445-b4b4-592cef79a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba170-623a-486b-8b67-58360bde104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d1a8-69aa-4c60-b3b4-ebb2e31f1c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
