{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d7f736-b9af-48e3-adb5-18ab3536abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v, verbose=False):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    if verbose:\n",
    "        print(softmaxed_prod.shape)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x, verbose=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abcca093-9e66-4b46-8ec3-aded63ece6f8",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a9e6b-8461-4417-ab07-e5fbd79c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c5bf9-3800-4b22-b7d8-8886d4f15b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d626059-fd88-4a03-8b20-ffbf0115fe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayerWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayerWithoutMask()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851eff3d-4816-4410-9184-f30c157e4eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayerWithoutMask(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = EncoderWithoutMask()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(torch.tensor([[1, 0, 0], [1, 1, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With masks\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None, verbose=False):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    if verbose:\n",
    "        print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim), e.g. # b t -> b 1 1 t\n",
    "    if mask is not None:\n",
    "        mask = mask if scaled_prod.dim() == mask.dim() else reshape_mask(mask)\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    if verbose:\n",
    "        print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    if verbose:\n",
    "        print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eac5e23-ea43-4295-ba24-a7edbe1a1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9161, 0.5284, 0.3874, 0.0296],\n",
      "          [0.2724, 0.6993, 0.6833, 0.8454]],\n",
      "\n",
      "         [[0.9751, 0.4556, 0.5572, 0.5613],\n",
      "          [0.5171, 0.4448, 0.5494, 0.1597]],\n",
      "\n",
      "         [[0.1415, 0.6000, 0.2259, 0.7091],\n",
      "          [0.8315, 0.2869, 0.9799, 0.6451]]],\n",
      "\n",
      "\n",
      "        [[[0.8202, 0.5965, 0.0961, 0.8646],\n",
      "          [0.5152, 0.0899, 0.2010, 0.3476]],\n",
      "\n",
      "         [[0.3940, 0.4795, 0.6472, 0.3459],\n",
      "          [0.3430, 0.9845, 0.6830, 0.2335]],\n",
      "\n",
      "         [[0.4451, 0.0086, 0.3697, 0.7003],\n",
      "          [0.1723, 0.1358, 0.8003, 0.7740]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[[0.6347, 0.6833,   -inf],\n",
      "           [0.6833, 0.8920,   -inf],\n",
      "           [0.2776, 0.4676,   -inf]],\n",
      "\n",
      "          [[0.8724, 0.4811,   -inf],\n",
      "           [0.4811, 0.3963,   -inf],\n",
      "           [0.8210, 0.5995,   -inf]]],\n",
      "\n",
      "\n",
      "         [[[0.8927, 0.4852,   -inf],\n",
      "           [0.4852, 0.4618,   -inf],\n",
      "           [0.5056, 0.3305,   -inf]],\n",
      "\n",
      "          [[0.2174, 0.2418,   -inf],\n",
      "           [0.2418, 0.8039,   -inf],\n",
      "           [0.2654, 0.4600,   -inf]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.6347,   -inf,   -inf],\n",
      "           [0.6833,   -inf,   -inf],\n",
      "           [0.2776,   -inf,   -inf]],\n",
      "\n",
      "          [[0.8724,   -inf,   -inf],\n",
      "           [0.4811,   -inf,   -inf],\n",
      "           [0.8210,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "         [[[0.8927,   -inf,   -inf],\n",
      "           [0.4852,   -inf,   -inf],\n",
      "           [0.5056,   -inf,   -inf]],\n",
      "\n",
      "          [[0.2174,   -inf,   -inf],\n",
      "           [0.2418,   -inf,   -inf],\n",
      "           [0.2654,   -inf,   -inf]]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[[0.4879, 0.5121, 0.0000],\n",
      "           [0.4480, 0.5520, 0.0000],\n",
      "           [0.4526, 0.5474, 0.0000]],\n",
      "\n",
      "          [[0.5966, 0.4034, 0.0000],\n",
      "           [0.5212, 0.4788, 0.0000],\n",
      "           [0.5552, 0.4448, 0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.6005, 0.3995, 0.0000],\n",
      "           [0.5058, 0.4942, 0.0000],\n",
      "           [0.5437, 0.4563, 0.0000]],\n",
      "\n",
      "          [[0.4939, 0.5061, 0.0000],\n",
      "           [0.3631, 0.6369, 0.0000],\n",
      "           [0.4515, 0.5485, 0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "          [[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "         [[[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "          [[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]]]]])\n",
      "wrong a: \n",
      " tensor([[[[[0.9463, 0.4911, 0.4744, 0.3019],\n",
      "           [0.9487, 0.4882, 0.4811, 0.3231],\n",
      "           [0.9484, 0.4886, 0.4804, 0.3207]],\n",
      "\n",
      "          [[0.3711, 0.5966, 0.6293, 0.5688],\n",
      "           [0.3896, 0.5774, 0.6192, 0.5171],\n",
      "           [0.3813, 0.5861, 0.6238, 0.5404]]],\n",
      "\n",
      "\n",
      "         [[[0.6499, 0.5498, 0.3163, 0.6574],\n",
      "           [0.6095, 0.5387, 0.3684, 0.6083],\n",
      "           [0.6257, 0.5431, 0.3476, 0.6279]],\n",
      "\n",
      "          [[0.4280, 0.5427, 0.4449, 0.2898],\n",
      "           [0.4055, 0.6597, 0.5080, 0.2749],\n",
      "           [0.4207, 0.5806, 0.4653, 0.2850]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.9161, 0.5284, 0.3874, 0.0296],\n",
      "           [0.9161, 0.5284, 0.3874, 0.0296],\n",
      "           [0.9161, 0.5284, 0.3874, 0.0296]],\n",
      "\n",
      "          [[0.2724, 0.6993, 0.6833, 0.8454],\n",
      "           [0.2724, 0.6993, 0.6833, 0.8454],\n",
      "           [0.2724, 0.6993, 0.6833, 0.8454]]],\n",
      "\n",
      "\n",
      "         [[[0.8202, 0.5965, 0.0961, 0.8646],\n",
      "           [0.8202, 0.5965, 0.0961, 0.8646],\n",
      "           [0.8202, 0.5965, 0.0961, 0.8646]],\n",
      "\n",
      "          [[0.5152, 0.0899, 0.2010, 0.3476],\n",
      "           [0.5152, 0.0899, 0.2010, 0.3476],\n",
      "           [0.5152, 0.0899, 0.2010, 0.3476]]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6347, 0.6833,   -inf],\n",
      "          [0.6833, 0.8920,   -inf],\n",
      "          [0.2776, 0.4676,   -inf]],\n",
      "\n",
      "         [[0.8724, 0.4811,   -inf],\n",
      "          [0.4811, 0.3963,   -inf],\n",
      "          [0.8210, 0.5995,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8927,   -inf,   -inf],\n",
      "          [0.4852,   -inf,   -inf],\n",
      "          [0.5056,   -inf,   -inf]],\n",
      "\n",
      "         [[0.2174,   -inf,   -inf],\n",
      "          [0.2418,   -inf,   -inf],\n",
      "          [0.2654,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4879, 0.5121, 0.0000],\n",
      "          [0.4480, 0.5520, 0.0000],\n",
      "          [0.4526, 0.5474, 0.0000]],\n",
      "\n",
      "         [[0.5966, 0.4034, 0.0000],\n",
      "          [0.5212, 0.4788, 0.0000],\n",
      "          [0.5552, 0.4448, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.9463, 0.4911, 0.4744, 0.3019],\n",
      "          [0.9487, 0.4882, 0.4811, 0.3231],\n",
      "          [0.9484, 0.4886, 0.4804, 0.3207]],\n",
      "\n",
      "         [[0.3711, 0.5966, 0.6293, 0.5688],\n",
      "          [0.3896, 0.5774, 0.6192, 0.5171],\n",
      "          [0.3813, 0.5861, 0.6238, 0.5404]]],\n",
      "\n",
      "\n",
      "        [[[0.8202, 0.5965, 0.0961, 0.8646],\n",
      "          [0.8202, 0.5965, 0.0961, 0.8646],\n",
      "          [0.8202, 0.5965, 0.0961, 0.8646]],\n",
      "\n",
      "         [[0.5152, 0.0899, 0.2010, 0.3476],\n",
      "          [0.5152, 0.0899, 0.2010, 0.3476],\n",
      "          [0.5152, 0.0899, 0.2010, 0.3476]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask, verbose=True)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask, verbose=True)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.9161, 0.5284, 0.3874, 0.0296],\n",
      "          [0.2724, 0.6993, 0.6833, 0.8454]],\n",
      "\n",
      "         [[0.9751, 0.4556, 0.5572, 0.5613],\n",
      "          [0.5171, 0.4448, 0.5494, 0.1597]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8202, 0.5965, 0.0961, 0.8646],\n",
      "          [0.5152, 0.0899, 0.2010, 0.3476]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6347, 0.6833,   -inf],\n",
      "          [0.6833, 0.8920,   -inf],\n",
      "          [0.2776, 0.4676,   -inf]],\n",
      "\n",
      "         [[0.8724, 0.4811,   -inf],\n",
      "          [0.4811, 0.3963,   -inf],\n",
      "          [0.8210, 0.5995,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8927,   -inf,   -inf],\n",
      "          [0.4852,   -inf,   -inf],\n",
      "          [0.5056,   -inf,   -inf]],\n",
      "\n",
      "         [[0.2174,   -inf,   -inf],\n",
      "          [0.2418,   -inf,   -inf],\n",
      "          [0.2654,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4879, 0.5121, 0.0000],\n",
      "          [0.4480, 0.5520, 0.0000],\n",
      "          [0.4526, 0.5474, 0.0000]],\n",
      "\n",
      "         [[0.5966, 0.4034, 0.0000],\n",
      "          [0.5212, 0.4788, 0.0000],\n",
      "          [0.5552, 0.4448, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.9463, 0.4911, 0.4744, 0.3019],\n",
      "          [0.9487, 0.4882, 0.4811, 0.3231],\n",
      "          [0.9484, 0.4886, 0.4804, 0.3207]],\n",
      "\n",
      "         [[0.3711, 0.5966, 0.6293, 0.5688],\n",
      "          [0.3896, 0.5774, 0.6192, 0.5171],\n",
      "          [0.3813, 0.5861, 0.6238, 0.5404]]],\n",
      "\n",
      "\n",
      "        [[[0.8202, 0.5965, 0.0961, 0.8646],\n",
      "          [0.8202, 0.5965, 0.0961, 0.8646],\n",
      "          [0.8202, 0.5965, 0.0961, 0.8646]],\n",
      "\n",
      "         [[0.5152, 0.0899, 0.2010, 0.3476],\n",
      "          [0.5152, 0.0899, 0.2010, 0.3476],\n",
      "          [0.5152, 0.0899, 0.2010, 0.3476]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.9362, 0.7632, 0.0345, 0.1292],\n",
      "         [0.4492, 0.3772, 0.8145, 0.2452],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.4693, 0.1840, 0.0089, 0.5638],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.9362, 0.7632],\n",
      "          [0.0345, 0.1292]],\n",
      "\n",
      "         [[0.4492, 0.3772],\n",
      "          [0.8145, 0.2452]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4693, 0.1840],\n",
      "          [0.0089, 0.5638]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.9362, 0.7632],\n",
      "          [0.4492, 0.3772],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0345, 0.1292],\n",
      "          [0.8145, 0.2452],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4693, 0.1840],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0089, 0.5638],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.2205, 0.2719, 0.3457, 0.9113],\n",
      "         [0.0886, 0.7644, 0.4861, 0.5921],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.9891, 0.2482, 0.7358, 0.9436],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.2205, 0.2719],\n",
      "          [0.3457, 0.9113]],\n",
      "\n",
      "         [[0.0886, 0.7644],\n",
      "          [0.4861, 0.5921]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.9891, 0.2482],\n",
      "          [0.7358, 0.9436]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.2205, 0.2719],\n",
      "          [0.0886, 0.7644],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.3457, 0.9113],\n",
      "          [0.4861, 0.5921],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.9891, 0.2482],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.7358, 0.9436],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[0.1226, 0.2274,   -inf],\n",
      "          [0.2274, 0.5922,   -inf],\n",
      "          [0.1585, 0.3398,   -inf]],\n",
      "\n",
      "         [[0.9499, 0.7076,   -inf],\n",
      "          [0.7076, 0.5869,   -inf],\n",
      "          [0.6580, 0.5318,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[1.0400,   -inf,   -inf],\n",
      "          [0.6245,   -inf,   -inf],\n",
      "          [0.7247,   -inf,   -inf]],\n",
      "\n",
      "         [[1.4319,   -inf,   -inf],\n",
      "          [1.0484,   -inf,   -inf],\n",
      "          [1.0661,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x, verbose=True)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.2186e-01, 5.9362e-01, 7.6366e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [9.9286e-02, 4.0288e-01, 9.4348e-01, 6.2622e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [7.6083e-03, 8.4606e-01, 4.7163e-01, 4.3363e-01, 6.3603e-02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.9430e-01, 5.8806e-01, 5.0770e-02, 9.1432e-01, 1.7868e-02, 6.9063e-02]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.1616e-01, 8.2191e-01, 9.6550e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [5.9104e-01, 7.7429e-01, 8.2443e-02, 8.7411e-01, 9.7018e-03, 1.0000e+02],\n",
      "        [4.9149e-01, 7.8321e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [7.4295e-01, 3.3757e-01, 9.9551e-01, 9.0420e-01, 5.5733e-02, 8.4830e-02]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "tensor([[[ 0.1825,  0.4952, -0.2641,  0.0693, -0.0135, -0.0194],\n",
      "         [ 0.1825,  0.4952, -0.2641,  0.0693, -0.0135, -0.0194],\n",
      "         [ 0.1825,  0.4952, -0.2641,  0.0693, -0.0135, -0.0194],\n",
      "         [ 0.1825,  0.4952, -0.2641,  0.0693, -0.0135, -0.0194],\n",
      "         [ 0.1825,  0.4952, -0.2641,  0.0693, -0.0135, -0.0194]],\n",
      "\n",
      "        [[ 0.0600,  0.3445, -0.2016,  0.0677, -0.0371,  0.1043],\n",
      "         [ 0.0600,  0.3446, -0.2015,  0.0677, -0.0371,  0.1043],\n",
      "         [ 0.0599,  0.3446, -0.2014,  0.0678, -0.0370,  0.1044],\n",
      "         [ 0.0599,  0.3446, -0.2014,  0.0678, -0.0370,  0.1044],\n",
      "         [ 0.0600,  0.3445, -0.2016,  0.0677, -0.0371,  0.1043]],\n",
      "\n",
      "        [[ 0.1816,  0.3811, -0.2190,  0.1883, -0.1887,  0.1405],\n",
      "         [ 0.1831,  0.3807, -0.2171,  0.1906, -0.1894,  0.1390],\n",
      "         [ 0.1822,  0.3804, -0.2175,  0.1902, -0.1886,  0.1432],\n",
      "         [ 0.1829,  0.3799, -0.2172,  0.1897, -0.1884,  0.1423],\n",
      "         [ 0.1827,  0.3807, -0.2171,  0.1909, -0.1893,  0.1408]],\n",
      "\n",
      "        [[ 0.1752,  0.3547, -0.1968,  0.2178, -0.1641,  0.1671],\n",
      "         [ 0.1768,  0.3543, -0.1955,  0.2171, -0.1631,  0.1672],\n",
      "         [ 0.1767,  0.3547, -0.1940,  0.2204, -0.1637,  0.1724],\n",
      "         [ 0.1742,  0.3551, -0.1966,  0.2202, -0.1651,  0.1702],\n",
      "         [ 0.1775,  0.3545, -0.1945,  0.2185, -0.1630,  0.1696]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # q and k/v might be of different sizes if lengths of decoder and encoders inputs are different\n",
    "        bq, tq, dq = q.size()\n",
    "        bk, tk, dk = k.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(bq, tq, self.h, self.dh)\n",
    "        wk = wk.view(bk, tk, self.h, self.dh)\n",
    "        wv = wv.view(bk, tk, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(bq, self.h, tq, self.dh).transpose(1, 2).contiguous().view(bq, tq, dq)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729ae6f8-a3f8-4386-837d-f1e67f022b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, self_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "x = torch.rand(2, 3, 512)\n",
    "\n",
    "encoder_layer(x, self_mask=self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d, h) for _ in range(n)])\n",
    "\n",
    "    def forward(self, x, self_mask = None):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask=self_mask)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "encoder(x, self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = dec_x.size()\n",
    "        x = dec_x + self.attn_dropout(self.mhsa(dec_x, dec_x, dec_x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, enc_x, enc_x, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d, h) for _ in range(n)])\n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        b, t = dec_x.size()\n",
    "        x = self.embed(dec_x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_x, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "    def get_embed_weights(self):\n",
    "        return self.embed.weight\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b5aba42-4209-4fbb-8cc9-a9e8a0236e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Output(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, d: int = 512, ff_weight = None):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(d, vocab_size)\n",
    "        # weight tying with the decoder embedding\n",
    "        if ff_weight is not None:\n",
    "            self.ff.weight = ff_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2477aec-7287-498b-bf31-cbb0bdfe154d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: \n",
      " tensor([[1, 1, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 1, 1, 0]])\n",
      "torch.Size([3, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8, embed_tying=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        if embed_tying:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d, ff_weight = self.decoder.get_embed_weights())\n",
    "        else:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d)\n",
    "\n",
    "    def forward(self, enc_x, dec_x, enc_mask=None, dec_mask=None):\n",
    "        encoded = self.encoder(enc_x, enc_mask)\n",
    "        decoded = self.decoder(dec_x=dec_x, enc_x=encoded, self_mask=dec_mask, cross_mask=enc_mask)\n",
    "        return self.output(decoded)\n",
    "\n",
    "transformer = Transformer(vocab_size=32, n=2, d=16, h=2, embed_tying=False)\n",
    "enc_x = torch.tensor([[15, 7, 3], [10, 10, 0], [1, 0, 0]])\n",
    "dec_x = torch.tensor([[21, 8, 0, 0], [25, 0, 0, 0], [8, 1, 2, 3]])\n",
    "# dec_x = torch.tensor([[21, 8], [25, 0], [8, 1]])\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=0)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=0)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "print(transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f69bab-e9a5-44a7-af1d-f294f62d3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a06c43eb-c348-4905-9384-1b28378435b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9906,   4435, 100257, 100257, 100257],\n",
      "        [  2028,    374,    264,   4382,  11914],\n",
      "        [  7979, 100257, 100257, 100257, 100257]])\n",
      "tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "enc_mask: \n",
      " tensor([[1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 0, 0, 0]])\n",
      "output shape: torch.Size([3, 4, 100277])\n",
      "softmaxed[0, 0, :10]: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "predicted: \n",
      " tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "predicted decoded: \n",
      " ['Bonjour<|endoftext|><|endoftext|><|endoftext|>', \"C'est une phrase\", 'START<|endoftext|><|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sents = [\"Hello World\", \"This is a simple sentence\", \"Me\"]\n",
    "encoded_sents = [encoding.encode(s) for s in sents]\n",
    "enc_x = pad_sequence([torch.tensor(es) for es in encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(enc_x)\n",
    "dec_sents = [\"Bonjour\", \"C'est une phrase\", \"START\"]\n",
    "dec_encoded_sents = [encoding.encode(s) for s in dec_sents]\n",
    "dec_x = pad_sequence([torch.tensor(es) for es in dec_encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(dec_x)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=100257)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=100257)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "print(f\"output shape: {output.shape}\")\n",
    "softmaxed = F.softmax(output, dim=-1)\n",
    "print(f\"softmaxed[0, 0, :10]: {softmaxed[0, 0, :10]}\")\n",
    "predicted = softmaxed.argmax(dim=-1)\n",
    "print(f\"predicted: \\n {predicted}\")\n",
    "\n",
    "predicted_list = predicted.tolist()\n",
    "predicted_decoded = [encoding.decode(l) for l in predicted_list]\n",
    "print(f\"predicted decoded: \\n {predicted_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "693b16e4-6684-4dbb-b21b-8c3650a10bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 34, 34, 34, 34]\n",
      "predicted sentence: \n",
      " CCCCC\n"
     ]
    }
   ],
   "source": [
    "# Predicting next words\n",
    "sent = \"This is a simple sentence\"\n",
    "encoded_sent = encoding.encode(sent)\n",
    "enc_x = torch.tensor(encoded_sent).unsqueeze(0)\n",
    "dec_x = torch.tensor(encoding.encode(\"C\")).unsqueeze(0)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "predicted_tokens = []\n",
    "for _ in range(5):\n",
    "    output = transformer(enc_x=enc_x, dec_x=dec_x)\n",
    "    softmaxed = F.softmax(output, dim=-1)\n",
    "    predicted = softmaxed.argmax(dim=-1)\n",
    "    predicted_tokens.append(predicted.tolist()[-1][-1]) \n",
    "    dec_x = torch.cat((dec_x, predicted), dim=-1)\n",
    "\n",
    "print(predicted_tokens)\n",
    "print(f\"predicted sentence: \\n {encoding.decode(predicted_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5588cc54-99dc-481d-8a78-d7d9e8adfd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hi.', 'START Salut!')\n",
      "('Stop!', 'START Arrte-toi !')\n",
      "('Hi.', 'START Salut!')\n",
      "('Run!', 'START Cours\\u202f!')\n",
      "('Run!', 'START Courez\\u202f!')\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "\n",
    "class Tokens(Enum):\n",
    "    START = \"START \"\n",
    "    END = \"<|endoftext|>\"\n",
    "    PAD = \" PAD\"\n",
    "    START_NUM = 23380\n",
    "    END_NUM = 100257\n",
    "    PAD_NUM = 62854\n",
    "    \n",
    "\n",
    "class EnFrDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        with open(file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            # we want data indexes start from 0, but filter out the first header row\n",
    "            for i, row in enumerate(reader, start=-1):\n",
    "                if i == -1:\n",
    "                    continue\n",
    "                en = row[0]\n",
    "                fr = Tokens.START.value + row[1]\n",
    "                self._data.append(tuple([en, fr]))\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "\n",
    "    class Iterator():\n",
    "\n",
    "        def __init__(self, outer):\n",
    "            self.cur = 0\n",
    "            self.outer = outer\n",
    "\n",
    "        def __next__(self):\n",
    "            if self.cur == len(self.outer._data):\n",
    "                raise StopIteration()\n",
    "            cur = self.outer._data[self.cur]\n",
    "            self.cur += 1\n",
    "            return cur\n",
    "\n",
    "    def __iter__(self):\n",
    "        return EnFrDataset.Iterator(self)\n",
    "    \n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = EnFrDataset(\"../data/eng_-french.csv\", val_ratio=0.1)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d09eb729-53bb-420b-a608-73b1365e6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([13347, 13], [23380], [8375])\n",
      "([6869, 0], [23380], [18733])\n",
      "([6869, 0], [23380], [18733])\n",
      "([36981, 0], [23380, 64105], [64105, 64])\n",
      "([12978, 0], [23380], [65381])\n"
     ]
    }
   ],
   "source": [
    "class TokEnFrDataset(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def build_train_sample(en_str: str, dec_str: str):\n",
    "        en_encoded = encoding.encode(en_str)\n",
    "        dec_encoded = encoding.encode(dec_str)\n",
    "        dec_encoded.append(Tokens.END_NUM.value)\n",
    "        en_sents = []\n",
    "        dec_sents = []\n",
    "        target_sents = []\n",
    "        \n",
    "        for i in range(1, len(dec_encoded)):\n",
    "            dec_sents.append(dec_encoded[:i])\n",
    "            target_sents.append(dec_encoded[1: i + 1])\n",
    "        en_sents.extend([en_encoded] * len(dec_sents))\n",
    "        return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        self._dataset = EnFrDataset(file, partition, val_ratio=0)\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        i = 0\n",
    "        for en, fr in self._dataset:\n",
    "            for sample in self.build_train_sample(en, fr):\n",
    "                self._data.append(sample)\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "                i += 1\n",
    "\n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french.csv\", val_ratio=0.1)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16252c2e-6c54-44c5-a4aa-4cfb4de90897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 6869,     0],\n",
      "        [36981,     0],\n",
      "        [12978,     0],\n",
      "        [35079,    13],\n",
      "        [10903,     0]]), tensor([[23380, 62854, 62854],\n",
      "        [23380, 64105, 62854],\n",
      "        [23380, 62854, 62854],\n",
      "        [23380, 16233,  1088],\n",
      "        [23380, 62854, 62854]]), tensor([[18733, 62854, 62854],\n",
      "        [64105,    64, 62854],\n",
      "        [65381, 62854, 62854],\n",
      "        [16233,  1088,    13],\n",
      "        [14549, 62854, 62854]]), tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), tensor([[1, 0, 0],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 1],\n",
      "        [1, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    # print(batch)\n",
    "    _x, _y, _label = list(zip(*batch))\n",
    "    enc_x = pad_sequence([torch.tensor(t) for t in _x], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    dec_x = pad_sequence([torch.tensor(t) for t in _y], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    label = pad_sequence([torch.tensor(t) for t in _label], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    enc_mask = build_padding_mask(enc_x, pad_token=Tokens.PAD_NUM.value)\n",
    "    dec_mask = build_padding_mask(dec_x, pad_token=Tokens.PAD_NUM.value)\n",
    "    return enc_x, dec_x, label, enc_mask, dec_mask\n",
    "\n",
    "training_generator = DataLoader(dataset, collate_fn=collate, batch_size=5, num_workers=0)\n",
    "for batch in training_generator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98593c81-b0c3-4145-95bc-1965620ceb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Number of model's params: 26696885\n",
      "EPOCH 1:\n",
      "Average batch loss: 0.030220237731933594\n",
      "Average batch loss: 20.92554894065857\n",
      "Average batch loss: 7.629583003997802\n",
      "Average batch loss: 5.8763357303142545\n",
      "Average batch loss: 5.845679868459701\n",
      "Average batch loss: 5.8248831729888915\n",
      "Average batch loss: 6.160519654512405\n",
      "Average batch loss: 6.20350581240654\n",
      "Average batch loss: 10.101644933462143\n",
      "Average batch loss: 7.434640023708344\n",
      "Average batch loss: 6.75776136803627\n",
      "Average batch loss: 6.577257626056671\n",
      "Average batch loss: 5.954030453681946\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import torch\n",
    "import math\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# torch.set_default_device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# batch = 16 if memory is not enough\n",
    "training_params = {\n",
    "    'collate_fn': collate,\n",
    "    'batch_size': 16,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0}\n",
    "max_epochs = 5\n",
    "model_dir = \"../data/\"\n",
    "\n",
    "# Generators\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french.csv\", val_ratio=0.05)\n",
    "dataloader = DataLoader(dataset, **training_params)\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=2, d=128, h=4)\n",
    "print(f\"Number of model's params: {sum(p.numel() for p in transformer.parameters())}\")\n",
    "transformer = transformer.to(device)\n",
    "loss_fn = F.cross_entropy\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "def save_model(epoch, i):\n",
    "    model_path = model_dir + 'model_{}_{}.pt'.format(epoch, i)\n",
    "    torch.save(transformer.state_dict(), model_path)\n",
    "    \n",
    "\n",
    "def train_epoch(epoch):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    best_loss = math.inf\n",
    "\n",
    "    transformer.train(True)\n",
    "    dataset.partition = Partition.TRAIN\n",
    "\n",
    "    for i, data in enumerate(dataloader, 1):\n",
    "        enc_x, dec_x, label, enc_mask, dec_mask = data\n",
    "        enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "        # Clear grads\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "        loss = loss_fn(output.view(-1, encoding.n_vocab), label.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            last_loss = running_loss / 1000\n",
    "            print('Average batch loss: {}'.format(last_loss))\n",
    "            running_loss = 0.\n",
    "            if last_loss < best_loss:\n",
    "                save_model(epoch, \"intermediate\")\n",
    "    print('Average epoch loss: {}'.format(last_loss))\n",
    "    return last_loss\n",
    "\n",
    "def validate_epoch():\n",
    "    running_vloss = 0.0\n",
    "    transformer.eval()\n",
    "    dataset.partition = PARTITION.VAL\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = vdata\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "            output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "            vloss = loss_fn(output, label)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('Average valid loss {}'.format(avg_vloss))\n",
    "\n",
    "    return avg_vloss\n",
    "\n",
    "def train():\n",
    "    best_vloss = math.inf\n",
    "    for epoch in range(max_epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        avg_train_loss = train_epoch(epoch)\n",
    "        \n",
    "        avg_val_loss = validate_epoch()\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            save_model(epoch, \"final\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819338a-9df0-465b-b1cf-a0a6e8a59d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((2, 3, 32), requires_grad=True)\n",
    "print(input.view(-1, 32).shape)\n",
    "target = torch.empty((2, 3), dtype=torch.long).random_(32)\n",
    "print(target)\n",
    "loss = F.cross_entropy(input.view(-1, 32), target.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05248576-1b7f-4994-80fe-c26d2dc1a81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef7bbe-c8a8-493e-8677-49d9c02dbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def build_train_sample(en_str: str, dec_str: str):\n",
    "    en_encoded = encoding.encode(en_str)\n",
    "    dec_encoded = encoding.encode(dec_str)\n",
    "    dec_encoded.append(Tokens.END_NUM.value)\n",
    "    en_sents = []\n",
    "    dec_sents = []\n",
    "    target_sents = []\n",
    "    \n",
    "    for i in range(1, len(dec_encoded)):\n",
    "        dec_sents.append(dec_encoded[:i])\n",
    "        target_sents.append(dec_encoded[1: i + 1])\n",
    "    en_sents.extend([en_encoded] * len(dec_sents))\n",
    "    return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "build_train_sample('Hi.', 'START Salut!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfee8d-38b9-4abb-b127-5e620ea387f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# def collate_fn\n",
    "\n",
    "dataset.partition = Partition.TRAIN\n",
    "training_generator = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "for i, s in enumerate(training_generator):\n",
    "    print(s)\n",
    "    if i > 2:\n",
    "        break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d5995-4736-4fec-9e51-e3672a9dc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty((2, 3), dtype=torch.long).random_(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0f4c4-a05a-4353-b58d-f50a60157f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(transformer.encoder.layers[0].mhsa.wq.weight.device)\n",
    "print(next(transformer.parameters()).device)\n",
    "print(transformer.decoder.layers[0].mhsa.wq.weight.device)\n",
    "print(next(transformer.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd67961-4fe8-4c69-90ec-f351df169e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62b3c0-a20a-466d-b5fe-05b33071e963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0dbc9-7e82-48b4-9b90-0ddf28b10784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21175a-9751-4413-8cdb-579c52134778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207745b-3455-48f2-a6c1-d6546569344a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbfc3e-aca8-4edb-99e5-025c945d85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da787162-9955-4de5-a82d-60501d62ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08f9da-be37-4a19-992e-11b2c6e1ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a2346-9e74-4eab-aab3-bd5bb0a0d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.3]]), dim=-1)\n",
    "torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]).argmax(dim=-1)\n",
    "# torch.max(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]), dim=-1)\n",
    "\n",
    "t1 = torch.tensor([[0.1, 0.2]])\n",
    "t2 = torch.tensor([[0.3]])\n",
    "torch.cat((t1, t2), dim=1).tolist()[-1][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683599c0-fd00-40b0-8bb1-4e477a2114a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45e55c-6d60-409b-b26e-0b218224483a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e70b0-a647-455b-bdfb-332d095aafc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082af878-9425-4445-b4b4-592cef79a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba170-623a-486b-8b67-58360bde104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d1a8-69aa-4c60-b3b4-ebb2e31f1c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
