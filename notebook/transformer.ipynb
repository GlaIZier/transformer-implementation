{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkhokhlush/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n",
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    print(softmaxed_prod.shape)\n",
    "    # print(softmaxed_prod)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x)\n",
    "self_attention(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2289c87e-8e24-485d-9c54-b973ea4dbd76",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim)\n",
    "    if mask is not None:\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    # print(softmaxed_prod.shape)\n",
    "    print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.6038, 0.9061, 0.0223, 0.2182],\n",
      "          [0.7312, 0.7781, 0.9720, 0.9055]],\n",
      "\n",
      "         [[0.5674, 0.8741, 0.1080, 0.0871],\n",
      "          [0.7987, 0.0513, 0.1808, 0.7122]],\n",
      "\n",
      "         [[0.0271, 0.8195, 0.0456, 0.9934],\n",
      "          [0.9385, 0.2037, 0.4899, 0.9183]]],\n",
      "\n",
      "\n",
      "        [[[0.1145, 0.8044, 0.0047, 0.9019],\n",
      "          [0.2724, 0.5579, 0.0549, 0.0763]],\n",
      "\n",
      "         [[0.0198, 0.2975, 0.4556, 0.7489],\n",
      "          [0.1878, 0.8561, 0.8002, 0.4084]],\n",
      "\n",
      "         [[0.2506, 0.6017, 0.6371, 0.0298],\n",
      "          [0.0901, 0.4109, 0.5031, 0.4092]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6169, 0.5780,   -inf],\n",
      "          [0.5780, 0.5526,   -inf],\n",
      "          [0.4884, 0.4116,   -inf]],\n",
      "\n",
      "         [[1.4524,   -inf,   -inf],\n",
      "          [0.7223,   -inf,   -inf],\n",
      "          [1.0762,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7368, 0.4596,   -inf],\n",
      "          [0.4596, 0.4287,   -inf],\n",
      "          [0.2713, 0.2483,   -inf]],\n",
      "\n",
      "         [[0.1972,   -inf,   -inf],\n",
      "          [0.3019,   -inf,   -inf],\n",
      "          [0.1563,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5097, 0.4903, 0.0000],\n",
      "          [0.5064, 0.4936, 0.0000],\n",
      "          [0.5192, 0.4808, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5689, 0.4311, 0.0000],\n",
      "          [0.5077, 0.4923, 0.0000],\n",
      "          [0.5058, 0.4942, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "wrong a: \n",
      " tensor([[[[0.5859, 0.8904, 0.0643, 0.1539],\n",
      "          [0.5858, 0.8903, 0.0646, 0.1535],\n",
      "          [0.5863, 0.8907, 0.0635, 0.1552]],\n",
      "\n",
      "         [[0.7312, 0.7781, 0.9720, 0.9055],\n",
      "          [0.7312, 0.7781, 0.9720, 0.9055],\n",
      "          [0.7312, 0.7781, 0.9720, 0.9055]]],\n",
      "\n",
      "\n",
      "        [[[0.0736, 0.5859, 0.1991, 0.8359],\n",
      "          [0.0678, 0.5549, 0.2267, 0.8266],\n",
      "          [0.0677, 0.5539, 0.2276, 0.8263]],\n",
      "\n",
      "         [[0.2724, 0.5579, 0.0549, 0.0763],\n",
      "          [0.2724, 0.5579, 0.0549, 0.0763],\n",
      "          [0.2724, 0.5579, 0.0549, 0.0763]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6169, 0.5780,   -inf],\n",
      "          [0.5780, 0.5526,   -inf],\n",
      "          [0.4884, 0.4116,   -inf]],\n",
      "\n",
      "         [[1.4524, 0.7223,   -inf],\n",
      "          [0.7223, 0.5903,   -inf],\n",
      "          [1.0762, 0.7513,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7368,   -inf,   -inf],\n",
      "          [0.4596,   -inf,   -inf],\n",
      "          [0.2713,   -inf,   -inf]],\n",
      "\n",
      "         [[0.1972,   -inf,   -inf],\n",
      "          [0.3019,   -inf,   -inf],\n",
      "          [0.1563,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5097, 0.4903, 0.0000],\n",
      "          [0.5064, 0.4936, 0.0000],\n",
      "          [0.5192, 0.4808, 0.0000]],\n",
      "\n",
      "         [[0.6748, 0.3252, 0.0000],\n",
      "          [0.5330, 0.4670, 0.0000],\n",
      "          [0.5805, 0.4195, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.5859, 0.8904, 0.0643, 0.1539],\n",
      "          [0.5858, 0.8903, 0.0646, 0.1535],\n",
      "          [0.5863, 0.8907, 0.0635, 0.1552]],\n",
      "\n",
      "         [[0.7531, 0.5418, 0.7147, 0.8426],\n",
      "          [0.7627, 0.4387, 0.6025, 0.8152],\n",
      "          [0.7595, 0.4732, 0.6401, 0.8244]]],\n",
      "\n",
      "\n",
      "        [[[0.1145, 0.8044, 0.0047, 0.9019],\n",
      "          [0.1145, 0.8044, 0.0047, 0.9019],\n",
      "          [0.1145, 0.8044, 0.0047, 0.9019]],\n",
      "\n",
      "         [[0.2724, 0.5579, 0.0549, 0.0763],\n",
      "          [0.2724, 0.5579, 0.0549, 0.0763],\n",
      "          [0.2724, 0.5579, 0.0549, 0.0763]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.6038, 0.9061, 0.0223, 0.2182],\n",
      "          [0.7312, 0.7781, 0.9720, 0.9055]],\n",
      "\n",
      "         [[0.5674, 0.8741, 0.1080, 0.0871],\n",
      "          [0.7987, 0.0513, 0.1808, 0.7122]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.1145, 0.8044, 0.0047, 0.9019],\n",
      "          [0.2724, 0.5579, 0.0549, 0.0763]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6169, 0.5780,   -inf],\n",
      "          [0.5780, 0.5526,   -inf],\n",
      "          [0.4884, 0.4116,   -inf]],\n",
      "\n",
      "         [[1.4524, 0.7223,   -inf],\n",
      "          [0.7223, 0.5903,   -inf],\n",
      "          [1.0762, 0.7513,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7368,   -inf,   -inf],\n",
      "          [0.4596,   -inf,   -inf],\n",
      "          [0.2713,   -inf,   -inf]],\n",
      "\n",
      "         [[0.1972,   -inf,   -inf],\n",
      "          [0.3019,   -inf,   -inf],\n",
      "          [0.1563,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5097, 0.4903, 0.0000],\n",
      "          [0.5064, 0.4936, 0.0000],\n",
      "          [0.5192, 0.4808, 0.0000]],\n",
      "\n",
      "         [[0.6748, 0.3252, 0.0000],\n",
      "          [0.5330, 0.4670, 0.0000],\n",
      "          [0.5805, 0.4195, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.5859, 0.8904, 0.0643, 0.1539],\n",
      "          [0.5858, 0.8903, 0.0646, 0.1535],\n",
      "          [0.5863, 0.8907, 0.0635, 0.1552]],\n",
      "\n",
      "         [[0.7531, 0.5418, 0.7147, 0.8426],\n",
      "          [0.7627, 0.4387, 0.6025, 0.8152],\n",
      "          [0.7595, 0.4732, 0.6401, 0.8244]]],\n",
      "\n",
      "\n",
      "        [[[0.1145, 0.8044, 0.0047, 0.9019],\n",
      "          [0.1145, 0.8044, 0.0047, 0.9019],\n",
      "          [0.1145, 0.8044, 0.0047, 0.9019]],\n",
      "\n",
      "         [[0.2724, 0.5579, 0.0549, 0.0763],\n",
      "          [0.2724, 0.5579, 0.0549, 0.0763],\n",
      "          [0.2724, 0.5579, 0.0549, 0.0763]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.7030, 0.8708, 0.5693, 0.5231],\n",
      "         [0.5446, 0.3679, 0.7412, 0.2635],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6259, 0.3106, 0.1670, 0.7860],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.7030, 0.8708],\n",
      "          [0.5693, 0.5231]],\n",
      "\n",
      "         [[0.5446, 0.3679],\n",
      "          [0.7412, 0.2635]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6259, 0.3106],\n",
      "          [0.1670, 0.7860]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.7030, 0.8708],\n",
      "          [0.5446, 0.3679],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5693, 0.5231],\n",
      "          [0.7412, 0.2635],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6259, 0.3106],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1670, 0.7860],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.3554, 0.1856, 0.0947, 0.6602],\n",
      "         [0.3343, 0.1160, 0.7206, 0.2960],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.9348, 0.5162, 0.7498, 0.8977],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.3554, 0.1856],\n",
      "          [0.0947, 0.6602]],\n",
      "\n",
      "         [[0.3343, 0.1160],\n",
      "          [0.7206, 0.2960]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.9348, 0.5162],\n",
      "          [0.7498, 0.8977]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.3554, 0.1856],\n",
      "          [0.3343, 0.1160],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.0947, 0.6602],\n",
      "          [0.7206, 0.2960],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.9348, 0.5162],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.7498, 0.8977],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[0.1608, 0.1404,   -inf],\n",
      "          [0.1404, 0.1252,   -inf],\n",
      "          [0.2248, 0.1679,   -inf]],\n",
      "\n",
      "         [[0.4449, 0.2637,   -inf],\n",
      "          [0.2637, 0.6069,   -inf],\n",
      "          [0.2718, 0.1973,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[1.1403,   -inf,   -inf],\n",
      "          [0.2531,   -inf,   -inf],\n",
      "          [0.2422,   -inf,   -inf]],\n",
      "\n",
      "         [[1.3681,   -inf,   -inf],\n",
      "          [0.4573,   -inf,   -inf],\n",
      "          [0.9281,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.6239,   0.1096,   0.4997, 100.0000, 100.0000, 100.0000],\n",
      "        [  0.5607,   0.5704,   0.7592,   0.9896, 100.0000, 100.0000],\n",
      "        [  0.7303,   0.2095,   0.9176,   0.9260,   0.3309, 100.0000],\n",
      "        [100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000],\n",
      "        [  0.5517,   0.9559,   0.5202,   0.1366,   0.7509,   0.7458]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.0406e-03, 7.8678e-01, 7.9798e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.7361e-01, 6.8509e-01, 3.0781e-01, 1.2141e-01, 5.6360e-02, 1.0000e+02],\n",
      "        [4.4617e-01, 7.4608e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.7225e-01, 6.3982e-01, 3.8140e-01, 7.9139e-01, 1.7079e-01, 6.0773e-01]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(build_causal_mask(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0503,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0237,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1518,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0617,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0026,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1670,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0728,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2183,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1107,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1711,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2614,  0.1307,    -inf,    -inf,    -inf],\n",
      "          [-0.1097, -0.0715,    -inf,    -inf,    -inf],\n",
      "          [ 0.3845,  0.1752,    -inf,    -inf,    -inf],\n",
      "          [ 0.0332,  0.0114,    -inf,    -inf,    -inf],\n",
      "          [ 0.0409,  0.0053,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1178, -0.1734,    -inf,    -inf,    -inf],\n",
      "          [-0.0270, -0.0656,    -inf,    -inf,    -inf],\n",
      "          [-0.0282, -0.0883,    -inf,    -inf,    -inf],\n",
      "          [-0.0026, -0.0451,    -inf,    -inf,    -inf],\n",
      "          [-0.0479, -0.1220,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0149,  0.0804,  0.2957,    -inf,    -inf],\n",
      "          [-0.0264, -0.0305,  0.0915,    -inf,    -inf],\n",
      "          [ 0.2412,  0.2083,  0.4072,    -inf,    -inf],\n",
      "          [-0.0613, -0.0153,  0.1379,    -inf,    -inf],\n",
      "          [ 0.0081, -0.0154,  0.0098,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1106, -0.2499, -0.1348,    -inf,    -inf],\n",
      "          [-0.0253, -0.0569, -0.0238,    -inf,    -inf],\n",
      "          [-0.1192, -0.3230, -0.2928,    -inf,    -inf],\n",
      "          [-0.0675, -0.1455, -0.0603,    -inf,    -inf],\n",
      "          [-0.0690, -0.1992, -0.2034,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2619,  0.1491,  0.1670,  0.1665,    -inf],\n",
      "          [-0.1265, -0.0870, -0.0650, -0.1526,    -inf],\n",
      "          [-0.1109, -0.0596, -0.0531, -0.1068,    -inf],\n",
      "          [-0.0385, -0.0105,  0.0118, -0.0902,    -inf],\n",
      "          [ 0.1435,  0.0826,  0.0887,  0.1002,    -inf]],\n",
      "\n",
      "         [[-0.1968, -0.3313, -0.1964, -0.3310,    -inf],\n",
      "          [-0.0872, -0.2325, -0.1332, -0.1784,    -inf],\n",
      "          [-0.1167, -0.2393, -0.1459, -0.2115,    -inf],\n",
      "          [-0.0307, -0.0990, -0.0345, -0.0714,    -inf],\n",
      "          [-0.2457, -0.4128, -0.2594, -0.4115,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5326, 0.4674, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4904, 0.5096, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5521, 0.4479, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5055, 0.4945, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5089, 0.4911, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5139, 0.4861, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5096, 0.4904, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5150, 0.4850, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5106, 0.4894, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5185, 0.4815, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2948, 0.3148, 0.3904, 0.0000, 0.0000],\n",
      "          [0.3204, 0.3191, 0.3605, 0.0000, 0.0000],\n",
      "          [0.3176, 0.3074, 0.3750, 0.0000, 0.0000],\n",
      "          [0.3060, 0.3204, 0.3735, 0.0000, 0.0000],\n",
      "          [0.3358, 0.3279, 0.3363, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3514, 0.3057, 0.3430, 0.0000, 0.0000],\n",
      "          [0.3367, 0.3262, 0.3371, 0.0000, 0.0000],\n",
      "          [0.3765, 0.3071, 0.3165, 0.0000, 0.0000],\n",
      "          [0.3411, 0.3154, 0.3435, 0.0000, 0.0000],\n",
      "          [0.3634, 0.3190, 0.3177, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2694, 0.2407, 0.2450, 0.2449, 0.0000],\n",
      "          [0.2452, 0.2551, 0.2608, 0.2389, 0.0000],\n",
      "          [0.2429, 0.2557, 0.2574, 0.2439, 0.0000],\n",
      "          [0.2482, 0.2552, 0.2610, 0.2357, 0.0000],\n",
      "          [0.2601, 0.2447, 0.2462, 0.2490, 0.0000]],\n",
      "\n",
      "         [[0.2668, 0.2332, 0.2668, 0.2332, 0.0000],\n",
      "          [0.2679, 0.2317, 0.2559, 0.2446, 0.0000],\n",
      "          [0.2656, 0.2349, 0.2579, 0.2416, 0.0000],\n",
      "          [0.2571, 0.2401, 0.2561, 0.2468, 0.0000],\n",
      "          [0.2718, 0.2299, 0.2681, 0.2302, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-0.0139, -0.4566, -0.2607, -0.3961,  0.2121,  0.4578],\n",
      "         [-0.0139, -0.4566, -0.2607, -0.3961,  0.2121,  0.4578],\n",
      "         [-0.0139, -0.4566, -0.2607, -0.3961,  0.2121,  0.4578],\n",
      "         [-0.0139, -0.4566, -0.2607, -0.3961,  0.2121,  0.4578],\n",
      "         [-0.0139, -0.4566, -0.2607, -0.3961,  0.2121,  0.4578]],\n",
      "\n",
      "        [[ 0.0872, -0.3692, -0.2435, -0.3125,  0.2037,  0.4528],\n",
      "         [ 0.0848, -0.3708, -0.2383, -0.3074,  0.2087,  0.4523],\n",
      "         [ 0.0881, -0.3682, -0.2458, -0.3149,  0.2014,  0.4529],\n",
      "         [ 0.0855, -0.3701, -0.2401, -0.3092,  0.2069,  0.4524],\n",
      "         [ 0.0875, -0.3718, -0.2411, -0.3092,  0.2066,  0.4535]],\n",
      "\n",
      "        [[ 0.1115, -0.4752, -0.2375, -0.3001,  0.1809,  0.5320],\n",
      "         [ 0.1081, -0.4735, -0.2369, -0.3028,  0.1763,  0.5328],\n",
      "         [ 0.1106, -0.4693, -0.2388, -0.3036,  0.1764,  0.5327],\n",
      "         [ 0.1096, -0.4753, -0.2367, -0.3008,  0.1793,  0.5324],\n",
      "         [ 0.1064, -0.4705, -0.2356, -0.3034,  0.1753,  0.5334]],\n",
      "\n",
      "        [[-0.0183, -0.4045, -0.1000, -0.1921,  0.2981,  0.4310],\n",
      "         [-0.0139, -0.4071, -0.0987, -0.1863,  0.3034,  0.4316],\n",
      "         [-0.0159, -0.4067, -0.0975, -0.1861,  0.3040,  0.4308],\n",
      "         [-0.0176, -0.4040, -0.0986, -0.1886,  0.3027,  0.4291],\n",
      "         [-0.0167, -0.4066, -0.0988, -0.1893,  0.3002,  0.4319]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0503,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0237,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1518,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0617,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0026,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1670,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0728,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2183,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1107,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1711,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2614,  0.1307,    -inf,    -inf,    -inf],\n",
      "          [-0.1097, -0.0715,    -inf,    -inf,    -inf],\n",
      "          [ 0.3845,  0.1752,    -inf,    -inf,    -inf],\n",
      "          [ 0.0332,  0.0114,    -inf,    -inf,    -inf],\n",
      "          [ 0.0409,  0.0053,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1178, -0.1734,    -inf,    -inf,    -inf],\n",
      "          [-0.0270, -0.0656,    -inf,    -inf,    -inf],\n",
      "          [-0.0282, -0.0883,    -inf,    -inf,    -inf],\n",
      "          [-0.0026, -0.0451,    -inf,    -inf,    -inf],\n",
      "          [-0.0479, -0.1220,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0149,  0.0804,  0.2957,    -inf,    -inf],\n",
      "          [-0.0264, -0.0305,  0.0915,    -inf,    -inf],\n",
      "          [ 0.2412,  0.2083,  0.4072,    -inf,    -inf],\n",
      "          [-0.0613, -0.0153,  0.1379,    -inf,    -inf],\n",
      "          [ 0.0081, -0.0154,  0.0098,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1106, -0.2499, -0.1348,    -inf,    -inf],\n",
      "          [-0.0253, -0.0569, -0.0238,    -inf,    -inf],\n",
      "          [-0.1192, -0.3230, -0.2928,    -inf,    -inf],\n",
      "          [-0.0675, -0.1455, -0.0603,    -inf,    -inf],\n",
      "          [-0.0690, -0.1992, -0.2034,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2619,  0.1491,  0.1670,  0.1665,    -inf],\n",
      "          [-0.1265, -0.0870, -0.0650, -0.1526,    -inf],\n",
      "          [-0.1109, -0.0596, -0.0531, -0.1068,    -inf],\n",
      "          [-0.0385, -0.0105,  0.0118, -0.0902,    -inf],\n",
      "          [ 0.1435,  0.0826,  0.0887,  0.1002,    -inf]],\n",
      "\n",
      "         [[-0.1968, -0.3313, -0.1964, -0.3310,    -inf],\n",
      "          [-0.0872, -0.2325, -0.1332, -0.1784,    -inf],\n",
      "          [-0.1167, -0.2393, -0.1459, -0.2115,    -inf],\n",
      "          [-0.0307, -0.0990, -0.0345, -0.0714,    -inf],\n",
      "          [-0.2457, -0.4128, -0.2594, -0.4115,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5326, 0.4674, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4904, 0.5096, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5521, 0.4479, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5055, 0.4945, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5089, 0.4911, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5139, 0.4861, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5096, 0.4904, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5150, 0.4850, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5106, 0.4894, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5185, 0.4815, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2948, 0.3148, 0.3904, 0.0000, 0.0000],\n",
      "          [0.3204, 0.3191, 0.3605, 0.0000, 0.0000],\n",
      "          [0.3176, 0.3074, 0.3750, 0.0000, 0.0000],\n",
      "          [0.3060, 0.3204, 0.3735, 0.0000, 0.0000],\n",
      "          [0.3358, 0.3279, 0.3363, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3514, 0.3057, 0.3430, 0.0000, 0.0000],\n",
      "          [0.3367, 0.3262, 0.3371, 0.0000, 0.0000],\n",
      "          [0.3765, 0.3071, 0.3165, 0.0000, 0.0000],\n",
      "          [0.3411, 0.3154, 0.3435, 0.0000, 0.0000],\n",
      "          [0.3634, 0.3190, 0.3177, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2694, 0.2407, 0.2450, 0.2449, 0.0000],\n",
      "          [0.2452, 0.2551, 0.2608, 0.2389, 0.0000],\n",
      "          [0.2429, 0.2557, 0.2574, 0.2439, 0.0000],\n",
      "          [0.2482, 0.2552, 0.2610, 0.2357, 0.0000],\n",
      "          [0.2601, 0.2447, 0.2462, 0.2490, 0.0000]],\n",
      "\n",
      "         [[0.2668, 0.2332, 0.2668, 0.2332, 0.0000],\n",
      "          [0.2679, 0.2317, 0.2559, 0.2446, 0.0000],\n",
      "          [0.2656, 0.2349, 0.2579, 0.2416, 0.0000],\n",
      "          [0.2571, 0.2401, 0.2561, 0.2468, 0.0000],\n",
      "          [0.2718, 0.2299, 0.2681, 0.2302, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1294,    -inf,    -inf],\n",
      "          [-0.0403,    -inf,    -inf],\n",
      "          [ 0.0163,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0627,    -inf,    -inf],\n",
      "          [-0.0654,    -inf,    -inf],\n",
      "          [ 0.0311,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0233,    -inf,    -inf],\n",
      "          [ 0.0316,    -inf,    -inf],\n",
      "          [ 0.0274,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0814,    -inf,    -inf],\n",
      "          [-0.0368,    -inf,    -inf],\n",
      "          [-0.0181,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0228, -0.0602,    -inf],\n",
      "          [-0.0459, -0.0760,    -inf],\n",
      "          [-0.0327, -0.0627,    -inf]],\n",
      "\n",
      "         [[-0.0970, -0.2198,    -inf],\n",
      "          [-0.0606, -0.1507,    -inf],\n",
      "          [-0.1047, -0.1939,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5093, 0.4907, 0.0000],\n",
      "          [0.5075, 0.4925, 0.0000],\n",
      "          [0.5075, 0.4925, 0.0000]],\n",
      "\n",
      "         [[0.5306, 0.4694, 0.0000],\n",
      "          [0.5225, 0.4775, 0.0000],\n",
      "          [0.5223, 0.4777, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0805, -0.0722, -0.1255],\n",
      "          [-0.3263, -0.3656, -0.4183],\n",
      "          [-0.2998, -0.3639, -0.4523]],\n",
      "\n",
      "         [[ 0.0284,  0.0920,  0.3850],\n",
      "          [ 0.1778,  0.1551,  0.0174],\n",
      "          [ 0.1804,  0.2576,  0.1134]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2643,    -inf,    -inf],\n",
      "          [-0.0637,    -inf,    -inf],\n",
      "          [-0.1639,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0941,    -inf,    -inf],\n",
      "          [ 0.1755,    -inf,    -inf],\n",
      "          [ 0.0846,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.5550, -0.6788,    -inf],\n",
      "          [ 0.0588, -0.0268,    -inf],\n",
      "          [-0.0683, -0.1452,    -inf]],\n",
      "\n",
      "         [[ 0.2412,  0.0765,    -inf],\n",
      "          [ 0.3226,  0.1890,    -inf],\n",
      "          [ 0.2580,  0.1840,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3374, 0.3402, 0.3225],\n",
      "          [0.3480, 0.3346, 0.3174],\n",
      "          [0.3576, 0.3354, 0.3070]],\n",
      "\n",
      "         [[0.2862, 0.3050, 0.4088],\n",
      "          [0.3534, 0.3455, 0.3011],\n",
      "          [0.3316, 0.3582, 0.3101]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5309, 0.4691, 0.0000],\n",
      "          [0.5214, 0.4786, 0.0000],\n",
      "          [0.5192, 0.4808, 0.0000]],\n",
      "\n",
      "         [[0.5411, 0.4589, 0.0000],\n",
      "          [0.5333, 0.4667, 0.0000],\n",
      "          [0.5185, 0.4815, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, x, y, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, y, y, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1695,    -inf,    -inf],\n",
      "          [-0.2592,    -inf,    -inf],\n",
      "          [ 0.3734,    -inf,    -inf]],\n",
      "\n",
      "         [[-1.1252,    -inf,    -inf],\n",
      "          [ 0.1413,    -inf,    -inf],\n",
      "          [-1.2248,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3937,    -inf,    -inf],\n",
      "          [ 0.1028,    -inf,    -inf],\n",
      "          [ 0.6798,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.6382,    -inf,    -inf],\n",
      "          [-0.5393,    -inf,    -inf],\n",
      "          [-1.0249,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2710, -0.3315,    -inf],\n",
      "          [-0.1795, -0.2612,    -inf],\n",
      "          [ 0.5556, -0.2480,    -inf]],\n",
      "\n",
      "         [[-0.4502,  0.0666,    -inf],\n",
      "          [ 0.3784, -0.4838,    -inf],\n",
      "          [-0.1718, -1.0626,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5151, 0.4849, 0.0000],\n",
      "          [0.5204, 0.4796, 0.0000],\n",
      "          [0.6908, 0.3092, 0.0000]],\n",
      "\n",
      "         [[0.3736, 0.6264, 0.0000],\n",
      "          [0.7031, 0.2969, 0.0000],\n",
      "          [0.7091, 0.2909, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-2.5968e-02, -8.6579e-02, -7.1196e-02],\n",
      "          [-6.3374e-03,  5.9055e-02,  1.7881e-01],\n",
      "          [ 6.2938e-02, -8.4620e-02,  2.4503e-02]],\n",
      "\n",
      "         [[ 4.3967e-03, -1.1150e-02, -2.0313e-01],\n",
      "          [ 1.9349e-01,  1.5590e-01,  2.1522e-01],\n",
      "          [ 1.7448e-01,  2.3056e-04,  1.0492e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8419e-01,        -inf,        -inf],\n",
      "          [-8.9041e-02,        -inf,        -inf],\n",
      "          [-1.4680e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.1808e-01,        -inf,        -inf],\n",
      "          [ 1.7046e-02,        -inf,        -inf],\n",
      "          [ 2.7508e-02,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5112e-01,  2.3409e-01,        -inf],\n",
      "          [ 1.2499e-01,  1.8167e-01,        -inf],\n",
      "          [ 2.3994e-02,  7.9798e-02,        -inf]],\n",
      "\n",
      "         [[ 9.9494e-02,  1.1837e-01,        -inf],\n",
      "          [ 3.4066e-01,  3.4505e-01,        -inf],\n",
      "          [ 2.8982e-01,  1.8925e-01,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3452, 0.3249, 0.3299],\n",
      "          [0.3057, 0.3264, 0.3679],\n",
      "          [0.3540, 0.3054, 0.3406]],\n",
      "\n",
      "         [[0.3575, 0.3520, 0.2905],\n",
      "          [0.3350, 0.3226, 0.3424],\n",
      "          [0.3606, 0.3030, 0.3364]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4793, 0.5207, 0.0000],\n",
      "          [0.4858, 0.5142, 0.0000],\n",
      "          [0.4861, 0.5139, 0.0000]],\n",
      "\n",
      "         [[0.4953, 0.5047, 0.0000],\n",
      "          [0.4989, 0.5011, 0.0000],\n",
      "          [0.5251, 0.4749, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0325,    -inf,    -inf],\n",
      "          [-0.1064,    -inf,    -inf],\n",
      "          [-0.1051,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1436,    -inf,    -inf],\n",
      "          [ 0.0451,    -inf,    -inf],\n",
      "          [-0.0041,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2404,    -inf,    -inf],\n",
      "          [-0.2727,    -inf,    -inf],\n",
      "          [ 0.0047,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0390,    -inf,    -inf],\n",
      "          [ 0.1460,    -inf,    -inf],\n",
      "          [ 0.2331,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2640,  0.2388,    -inf],\n",
      "          [ 0.4296,  0.5112,    -inf],\n",
      "          [ 0.1899, -0.0025,    -inf]],\n",
      "\n",
      "         [[-0.9540, -0.6732,    -inf],\n",
      "          [ 0.1729,  0.1863,    -inf],\n",
      "          [-0.6060, -0.3322,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5063, 0.4937, 0.0000],\n",
      "          [0.4796, 0.5204, 0.0000],\n",
      "          [0.5479, 0.4521, 0.0000]],\n",
      "\n",
      "         [[0.4302, 0.5698, 0.0000],\n",
      "          [0.4967, 0.5033, 0.0000],\n",
      "          [0.4320, 0.5680, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.6097,  0.6320,  0.5962],\n",
      "          [-0.0768, -0.1569, -0.2754],\n",
      "          [ 0.5014,  0.4449,  0.2700]],\n",
      "\n",
      "         [[-0.2241, -0.0351, -0.1557],\n",
      "          [-0.1549, -0.2718, -0.5210],\n",
      "          [-0.1615, -0.1249, -0.3597]]],\n",
      "\n",
      "\n",
      "        [[[-0.2121,    -inf,    -inf],\n",
      "          [ 0.2414,    -inf,    -inf],\n",
      "          [ 0.3991,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2230,    -inf,    -inf],\n",
      "          [-0.2633,    -inf,    -inf],\n",
      "          [-0.1260,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3557,  0.2273,    -inf],\n",
      "          [ 0.5975,  0.5933,    -inf],\n",
      "          [ 0.7187,  0.5508,    -inf]],\n",
      "\n",
      "         [[-0.1854, -0.1304,    -inf],\n",
      "          [-0.0639, -0.0266,    -inf],\n",
      "          [-0.1765, -0.1404,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3323, 0.3398, 0.3279],\n",
      "          [0.3646, 0.3365, 0.2989],\n",
      "          [0.3652, 0.3451, 0.2897]],\n",
      "\n",
      "         [[0.3050, 0.3684, 0.3266],\n",
      "          [0.3871, 0.3444, 0.2684],\n",
      "          [0.3500, 0.3630, 0.2870]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5321, 0.4679, 0.0000],\n",
      "          [0.5011, 0.4989, 0.0000],\n",
      "          [0.5419, 0.4581, 0.0000]],\n",
      "\n",
      "         [[0.4863, 0.5137, 0.0000],\n",
      "          [0.4907, 0.5093, 0.0000],\n",
      "          [0.4910, 0.5090, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [DecoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x, y, self_mask=self_mask, cross_mask=cross_mask):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, y, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
