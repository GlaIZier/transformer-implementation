{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d7f736-b9af-48e3-adb5-18ab3536abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v, verbose=False):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    if verbose:\n",
    "        print(softmaxed_prod.shape)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x, verbose=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abcca093-9e66-4b46-8ec3-aded63ece6f8",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a9e6b-8461-4417-ab07-e5fbd79c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c5bf9-3800-4b22-b7d8-8886d4f15b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d626059-fd88-4a03-8b20-ffbf0115fe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayerWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayerWithoutMask()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851eff3d-4816-4410-9184-f30c157e4eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayerWithoutMask(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = EncoderWithoutMask()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(torch.tensor([[1, 0, 0], [1, 1, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With masks\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None, verbose=False):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    if verbose:\n",
    "        print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim), e.g. # b t -> b 1 1 t\n",
    "    if mask is not None:\n",
    "        mask = mask if scaled_prod.dim() == mask.dim() else reshape_mask(mask)\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    if verbose:\n",
    "        print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    if verbose:\n",
    "        print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eac5e23-ea43-4295-ba24-a7edbe1a1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0649, 0.4532, 0.0269, 0.6571],\n",
      "          [0.1638, 0.3517, 0.4630, 0.4557]],\n",
      "\n",
      "         [[0.2390, 0.4421, 0.5409, 0.0424],\n",
      "          [0.5394, 0.9341, 0.2859, 0.8955]],\n",
      "\n",
      "         [[0.5830, 0.8164, 0.5793, 0.5849],\n",
      "          [0.4117, 0.1702, 0.9291, 0.5779]]],\n",
      "\n",
      "\n",
      "        [[[0.4439, 0.2531, 0.2214, 0.6364],\n",
      "          [0.7501, 0.9811, 0.7093, 0.4409]],\n",
      "\n",
      "         [[0.9240, 0.9253, 0.6665, 0.0901],\n",
      "          [0.8349, 0.3842, 0.6492, 0.4291]],\n",
      "\n",
      "         [[0.3942, 0.9604, 0.7533, 0.8755],\n",
      "          [0.5429, 0.6555, 0.9184, 0.4396]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[[0.3210, 0.1291,   -inf],\n",
      "           [0.1291, 0.2735,   -inf],\n",
      "           [0.4038, 0.4192,   -inf]],\n",
      "\n",
      "          [[0.2863, 0.4787,   -inf],\n",
      "           [0.4787, 1.0236,   -inf],\n",
      "           [0.4104, 0.5821,   -inf]]],\n",
      "\n",
      "\n",
      "         [[[0.3576, 0.4246,   -inf],\n",
      "           [0.4246, 1.0811,   -inf],\n",
      "           [0.5710, 0.9169,   -inf]],\n",
      "\n",
      "          [[1.1114, 0.8265,   -inf],\n",
      "           [0.8265, 0.7252,   -inf],\n",
      "           [0.9478, 0.7450,   -inf]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.3210,   -inf,   -inf],\n",
      "           [0.1291,   -inf,   -inf],\n",
      "           [0.4038,   -inf,   -inf]],\n",
      "\n",
      "          [[0.2863,   -inf,   -inf],\n",
      "           [0.4787,   -inf,   -inf],\n",
      "           [0.4104,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "         [[[0.3576,   -inf,   -inf],\n",
      "           [0.4246,   -inf,   -inf],\n",
      "           [0.5710,   -inf,   -inf]],\n",
      "\n",
      "          [[1.1114,   -inf,   -inf],\n",
      "           [0.8265,   -inf,   -inf],\n",
      "           [0.9478,   -inf,   -inf]]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[[0.5478, 0.4522, 0.0000],\n",
      "           [0.4640, 0.5360, 0.0000],\n",
      "           [0.4962, 0.5038, 0.0000]],\n",
      "\n",
      "          [[0.4521, 0.5479, 0.0000],\n",
      "           [0.3671, 0.6329, 0.0000],\n",
      "           [0.4572, 0.5428, 0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.4832, 0.5168, 0.0000],\n",
      "           [0.3415, 0.6585, 0.0000],\n",
      "           [0.4144, 0.5856, 0.0000]],\n",
      "\n",
      "          [[0.5708, 0.4292, 0.0000],\n",
      "           [0.5253, 0.4747, 0.0000],\n",
      "           [0.5505, 0.4495, 0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "          [[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "         [[[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "          [[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]]]]])\n",
      "wrong a: \n",
      " tensor([[[[[0.1436, 0.4482, 0.2593, 0.3791],\n",
      "           [0.1582, 0.4472, 0.3024, 0.3276],\n",
      "           [0.1526, 0.4476, 0.2859, 0.3474]],\n",
      "\n",
      "          [[0.3696, 0.6708, 0.3660, 0.6967],\n",
      "           [0.4015, 0.7203, 0.3509, 0.7340],\n",
      "           [0.3677, 0.6678, 0.3669, 0.6944]]],\n",
      "\n",
      "\n",
      "         [[[0.6920, 0.6004, 0.4514, 0.3541],\n",
      "           [0.7600, 0.6957, 0.5145, 0.2767],\n",
      "           [0.7250, 0.6467, 0.4821, 0.3165]],\n",
      "\n",
      "          [[0.7865, 0.7249, 0.6835, 0.4358],\n",
      "           [0.7904, 0.6978, 0.6808, 0.4353],\n",
      "           [0.7883, 0.7129, 0.6823, 0.4356]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.0649, 0.4532, 0.0269, 0.6571],\n",
      "           [0.0649, 0.4532, 0.0269, 0.6571],\n",
      "           [0.0649, 0.4532, 0.0269, 0.6571]],\n",
      "\n",
      "          [[0.1638, 0.3517, 0.4630, 0.4557],\n",
      "           [0.1638, 0.3517, 0.4630, 0.4557],\n",
      "           [0.1638, 0.3517, 0.4630, 0.4557]]],\n",
      "\n",
      "\n",
      "         [[[0.4439, 0.2531, 0.2214, 0.6364],\n",
      "           [0.4439, 0.2531, 0.2214, 0.6364],\n",
      "           [0.4439, 0.2531, 0.2214, 0.6364]],\n",
      "\n",
      "          [[0.7501, 0.9811, 0.7093, 0.4409],\n",
      "           [0.7501, 0.9811, 0.7093, 0.4409],\n",
      "           [0.7501, 0.9811, 0.7093, 0.4409]]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.3210, 0.1291,   -inf],\n",
      "          [0.1291, 0.2735,   -inf],\n",
      "          [0.4038, 0.4192,   -inf]],\n",
      "\n",
      "         [[0.2863, 0.4787,   -inf],\n",
      "          [0.4787, 1.0236,   -inf],\n",
      "          [0.4104, 0.5821,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.3576,   -inf,   -inf],\n",
      "          [0.4246,   -inf,   -inf],\n",
      "          [0.5710,   -inf,   -inf]],\n",
      "\n",
      "         [[1.1114,   -inf,   -inf],\n",
      "          [0.8265,   -inf,   -inf],\n",
      "          [0.9478,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5478, 0.4522, 0.0000],\n",
      "          [0.4640, 0.5360, 0.0000],\n",
      "          [0.4962, 0.5038, 0.0000]],\n",
      "\n",
      "         [[0.4521, 0.5479, 0.0000],\n",
      "          [0.3671, 0.6329, 0.0000],\n",
      "          [0.4572, 0.5428, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.1436, 0.4482, 0.2593, 0.3791],\n",
      "          [0.1582, 0.4472, 0.3024, 0.3276],\n",
      "          [0.1526, 0.4476, 0.2859, 0.3474]],\n",
      "\n",
      "         [[0.3696, 0.6708, 0.3660, 0.6967],\n",
      "          [0.4015, 0.7203, 0.3509, 0.7340],\n",
      "          [0.3677, 0.6678, 0.3669, 0.6944]]],\n",
      "\n",
      "\n",
      "        [[[0.4439, 0.2531, 0.2214, 0.6364],\n",
      "          [0.4439, 0.2531, 0.2214, 0.6364],\n",
      "          [0.4439, 0.2531, 0.2214, 0.6364]],\n",
      "\n",
      "         [[0.7501, 0.9811, 0.7093, 0.4409],\n",
      "          [0.7501, 0.9811, 0.7093, 0.4409],\n",
      "          [0.7501, 0.9811, 0.7093, 0.4409]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask, verbose=True)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask, verbose=True)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.0649, 0.4532, 0.0269, 0.6571],\n",
      "          [0.1638, 0.3517, 0.4630, 0.4557]],\n",
      "\n",
      "         [[0.2390, 0.4421, 0.5409, 0.0424],\n",
      "          [0.5394, 0.9341, 0.2859, 0.8955]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4439, 0.2531, 0.2214, 0.6364],\n",
      "          [0.7501, 0.9811, 0.7093, 0.4409]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.3210, 0.1291,   -inf],\n",
      "          [0.1291, 0.2735,   -inf],\n",
      "          [0.4038, 0.4192,   -inf]],\n",
      "\n",
      "         [[0.2863, 0.4787,   -inf],\n",
      "          [0.4787, 1.0236,   -inf],\n",
      "          [0.4104, 0.5821,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.3576,   -inf,   -inf],\n",
      "          [0.4246,   -inf,   -inf],\n",
      "          [0.5710,   -inf,   -inf]],\n",
      "\n",
      "         [[1.1114,   -inf,   -inf],\n",
      "          [0.8265,   -inf,   -inf],\n",
      "          [0.9478,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5478, 0.4522, 0.0000],\n",
      "          [0.4640, 0.5360, 0.0000],\n",
      "          [0.4962, 0.5038, 0.0000]],\n",
      "\n",
      "         [[0.4521, 0.5479, 0.0000],\n",
      "          [0.3671, 0.6329, 0.0000],\n",
      "          [0.4572, 0.5428, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.1436, 0.4482, 0.2593, 0.3791],\n",
      "          [0.1582, 0.4472, 0.3024, 0.3276],\n",
      "          [0.1526, 0.4476, 0.2859, 0.3474]],\n",
      "\n",
      "         [[0.3696, 0.6708, 0.3660, 0.6967],\n",
      "          [0.4015, 0.7203, 0.3509, 0.7340],\n",
      "          [0.3677, 0.6678, 0.3669, 0.6944]]],\n",
      "\n",
      "\n",
      "        [[[0.4439, 0.2531, 0.2214, 0.6364],\n",
      "          [0.4439, 0.2531, 0.2214, 0.6364],\n",
      "          [0.4439, 0.2531, 0.2214, 0.6364]],\n",
      "\n",
      "         [[0.7501, 0.9811, 0.7093, 0.4409],\n",
      "          [0.7501, 0.9811, 0.7093, 0.4409],\n",
      "          [0.7501, 0.9811, 0.7093, 0.4409]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.5437, 0.7946, 0.0237, 0.1185],\n",
      "         [0.9189, 0.0629, 0.6813, 0.4581],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.7979, 0.0564, 0.1943, 0.4604],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.5437, 0.7946],\n",
      "          [0.0237, 0.1185]],\n",
      "\n",
      "         [[0.9189, 0.0629],\n",
      "          [0.6813, 0.4581]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.7979, 0.0564],\n",
      "          [0.1943, 0.4604]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.5437, 0.7946],\n",
      "          [0.9189, 0.0629],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0237, 0.1185],\n",
      "          [0.6813, 0.4581],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.7979, 0.0564],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1943, 0.4604],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.2862, 0.0050, 0.2003, 0.6581],\n",
      "         [0.3271, 0.8120, 0.1712, 0.9116],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.4063, 0.9023, 0.4127, 0.7544],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.2862, 0.0050],\n",
      "          [0.2003, 0.6581]],\n",
      "\n",
      "         [[0.3271, 0.8120],\n",
      "          [0.1712, 0.9116]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4063, 0.9023],\n",
      "          [0.4127, 0.7544]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.2862, 0.0050],\n",
      "          [0.3271, 0.8120],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.2003, 0.6581],\n",
      "          [0.1712, 0.9116],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4063, 0.9023],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.4127, 0.7544],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[0.0819, 0.0976,   -inf],\n",
      "          [0.0976, 0.7663,   -inf],\n",
      "          [0.0759, 0.6924,   -inf]],\n",
      "\n",
      "         [[0.4732, 0.6342,   -inf],\n",
      "          [0.6342, 0.8603,   -inf],\n",
      "          [0.4590, 0.5457,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.9792,   -inf,   -inf],\n",
      "          [0.2061,   -inf,   -inf],\n",
      "          [0.8247,   -inf,   -inf]],\n",
      "\n",
      "         [[0.7394,   -inf,   -inf],\n",
      "          [0.7347,   -inf,   -inf],\n",
      "          [0.0774,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x, verbose=True)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.9307e-02, 1.4554e-01, 9.6462e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.5495e-01, 1.9227e-01, 5.0974e-01, 8.1012e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [9.9241e-01, 3.5653e-01, 4.1188e-01, 8.7713e-02, 3.1299e-01, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.2223e-01, 6.8125e-01, 3.5470e-01, 2.3305e-01, 4.3886e-01, 8.0410e-01]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.8048,   0.7238,   0.8886, 100.0000, 100.0000, 100.0000],\n",
      "        [  0.1598,   0.4412,   0.8388,   0.4008,   0.4206, 100.0000],\n",
      "        [  0.1028,   0.8259, 100.0000, 100.0000, 100.0000, 100.0000],\n",
      "        [100.0000, 100.0000, 100.0000, 100.0000, 100.0000, 100.0000],\n",
      "        [  0.3451,   0.4113,   0.2248,   0.8294,   0.1381,   0.4272]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "tensor([[[-3.9958e-02, -2.3940e-01,  2.0452e-01, -1.1426e-02,  1.5735e-01,\n",
      "           4.9056e-01],\n",
      "         [-3.9958e-02, -2.3940e-01,  2.0452e-01, -1.1426e-02,  1.5735e-01,\n",
      "           4.9056e-01],\n",
      "         [-3.9958e-02, -2.3940e-01,  2.0452e-01, -1.1426e-02,  1.5735e-01,\n",
      "           4.9056e-01],\n",
      "         [-3.9958e-02, -2.3940e-01,  2.0452e-01, -1.1426e-02,  1.5735e-01,\n",
      "           4.9056e-01],\n",
      "         [-3.9958e-02, -2.3940e-01,  2.0452e-01, -1.1426e-02,  1.5735e-01,\n",
      "           4.9056e-01]],\n",
      "\n",
      "        [[-1.0700e-03, -3.1134e-01,  3.2312e-01, -7.2479e-02,  3.7977e-01,\n",
      "           6.6270e-01],\n",
      "         [-1.1019e-03, -3.1134e-01,  3.2309e-01, -7.2452e-02,  3.7976e-01,\n",
      "           6.6269e-01],\n",
      "         [-1.0845e-03, -3.1134e-01,  3.2310e-01, -7.2468e-02,  3.7976e-01,\n",
      "           6.6269e-01],\n",
      "         [-9.7659e-04, -3.1112e-01,  3.2272e-01, -7.2672e-02,  3.7904e-01,\n",
      "           6.6212e-01],\n",
      "         [-1.0928e-03, -3.1129e-01,  3.2296e-01, -7.2491e-02,  3.7956e-01,\n",
      "           6.6252e-01]],\n",
      "\n",
      "        [[-2.0688e-02, -3.0369e-01,  3.2527e-01, -6.1250e-02,  4.1332e-01,\n",
      "           6.9859e-01],\n",
      "         [-2.0673e-02, -3.0361e-01,  3.2774e-01, -6.1344e-02,  4.1693e-01,\n",
      "           7.0164e-01],\n",
      "         [-2.0488e-02, -3.0651e-01,  3.3239e-01, -6.2148e-02,  4.2323e-01,\n",
      "           7.0650e-01],\n",
      "         [-2.0333e-02, -3.0401e-01,  3.3141e-01, -6.2858e-02,  4.2240e-01,\n",
      "           7.0721e-01],\n",
      "         [-2.0729e-02, -3.0230e-01,  3.2378e-01, -6.0944e-02,  4.1118e-01,\n",
      "           6.9706e-01]],\n",
      "\n",
      "        [[-6.3578e-04, -2.5366e-01,  2.7932e-01, -9.1992e-02,  3.2806e-01,\n",
      "           6.6000e-01],\n",
      "         [-5.7345e-04, -2.5268e-01,  2.7829e-01, -9.2256e-02,  3.2671e-01,\n",
      "           6.5907e-01],\n",
      "         [-4.3157e-04, -2.5326e-01,  2.7835e-01, -9.2330e-02,  3.2636e-01,\n",
      "           6.5862e-01],\n",
      "         [-7.4256e-04, -2.5337e-01,  2.7945e-01, -9.1881e-02,  3.2847e-01,\n",
      "           6.6042e-01],\n",
      "         [-7.0166e-04, -2.5342e-01,  2.7946e-01, -9.1901e-02,  3.2836e-01,\n",
      "           6.6032e-01]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # q and k/v might be of different sizes if lengths of decoder and encoders inputs are different\n",
    "        bq, tq, dq = q.size()\n",
    "        bk, tk, dk = k.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(bq, tq, self.h, self.dh)\n",
    "        wk = wk.view(bk, tk, self.h, self.dh)\n",
    "        wv = wv.view(bk, tk, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(bq, self.h, tq, self.dh).transpose(1, 2).contiguous().view(bq, tq, dq)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729ae6f8-a3f8-4386-837d-f1e67f022b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, self_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "x = torch.rand(2, 3, 512)\n",
    "\n",
    "encoder_layer(x, self_mask=self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d, h) for _ in range(n)])\n",
    "\n",
    "    def forward(self, x, self_mask = None):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask=self_mask)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "encoder(x, self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = dec_x.size()\n",
    "        x = dec_x + self.attn_dropout(self.mhsa(dec_x, dec_x, dec_x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, enc_x, enc_x, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d, h) for _ in range(n)])\n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        b, t = dec_x.size()\n",
    "        x = self.embed(dec_x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_x, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "    def get_embed_weights(self):\n",
    "        return self.embed.weight\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b5aba42-4209-4fbb-8cc9-a9e8a0236e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Output(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, d: int = 512, ff_weight = None):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(d, vocab_size)\n",
    "        # weight tying with the decoder embedding\n",
    "        if ff_weight is not None:\n",
    "            self.ff.weight = ff_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2477aec-7287-498b-bf31-cbb0bdfe154d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: \n",
      " tensor([[1, 1, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 1, 1, 0]])\n",
      "torch.Size([3, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8, embed_tying=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        if embed_tying:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d, ff_weight = self.decoder.get_embed_weights())\n",
    "        else:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d)\n",
    "\n",
    "    def forward(self, enc_x, dec_x, enc_mask=None, dec_mask=None):\n",
    "        encoded = self.encoder(enc_x, enc_mask)\n",
    "        decoded = self.decoder(dec_x=dec_x, enc_x=encoded, self_mask=dec_mask, cross_mask=enc_mask)\n",
    "        return self.output(decoded)\n",
    "\n",
    "transformer = Transformer(vocab_size=32, n=2, d=16, h=2, embed_tying=False)\n",
    "enc_x = torch.tensor([[15, 7, 3], [10, 10, 0], [1, 0, 0]])\n",
    "dec_x = torch.tensor([[21, 8, 0, 0], [25, 0, 0, 0], [8, 1, 2, 3]])\n",
    "# dec_x = torch.tensor([[21, 8], [25, 0], [8, 1]])\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=0)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=0)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "print(transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f69bab-e9a5-44a7-af1d-f294f62d3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a06c43eb-c348-4905-9384-1b28378435b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9906,   4435, 100257, 100257, 100257],\n",
      "        [  2028,    374,    264,   4382,  11914],\n",
      "        [  7979, 100257, 100257, 100257, 100257]])\n",
      "tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "enc_mask: \n",
      " tensor([[1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 0, 0, 0]])\n",
      "output shape: torch.Size([3, 4, 100277])\n",
      "softmaxed[0, 0, :10]: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "predicted: \n",
      " tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "predicted decoded: \n",
      " ['Bonjour<|endoftext|><|endoftext|><|endoftext|>', \"C'est une phrase\", 'START<|endoftext|><|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sents = [\"Hello World\", \"This is a simple sentence\", \"Me\"]\n",
    "encoded_sents = [encoding.encode(s) for s in sents]\n",
    "enc_x = pad_sequence([torch.tensor(es) for es in encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(enc_x)\n",
    "dec_sents = [\"Bonjour\", \"C'est une phrase\", \"START\"]\n",
    "dec_encoded_sents = [encoding.encode(s) for s in dec_sents]\n",
    "dec_x = pad_sequence([torch.tensor(es) for es in dec_encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(dec_x)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=100257)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=100257)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "print(f\"output shape: {output.shape}\")\n",
    "softmaxed = F.softmax(output, dim=-1)\n",
    "print(f\"softmaxed[0, 0, :10]: {softmaxed[0, 0, :10]}\")\n",
    "predicted = softmaxed.argmax(dim=-1)\n",
    "print(f\"predicted: \\n {predicted}\")\n",
    "\n",
    "predicted_list = predicted.tolist()\n",
    "predicted_decoded = [encoding.decode(l) for l in predicted_list]\n",
    "print(f\"predicted decoded: \\n {predicted_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "693b16e4-6684-4dbb-b21b-8c3650a10bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 34, 34, 34, 34]\n",
      "predicted sentence: \n",
      " CCCCC\n"
     ]
    }
   ],
   "source": [
    "# Predicting next words\n",
    "sent = \"This is a simple sentence\"\n",
    "encoded_sent = encoding.encode(sent)\n",
    "enc_x = torch.tensor(encoded_sent).unsqueeze(0)\n",
    "dec_x = torch.tensor(encoding.encode(\"C\")).unsqueeze(0)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "predicted_tokens = []\n",
    "for _ in range(5):\n",
    "    output = transformer(enc_x=enc_x, dec_x=dec_x)\n",
    "    softmaxed = F.softmax(output, dim=-1)\n",
    "    predicted = softmaxed.argmax(dim=-1)\n",
    "    predicted_tokens.append(predicted.tolist()[-1][-1]) \n",
    "    dec_x = torch.cat((dec_x, predicted), dim=-1)\n",
    "\n",
    "print(predicted_tokens)\n",
    "print(f\"predicted sentence: \\n {encoding.decode(predicted_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5588cc54-99dc-481d-8a78-d7d9e8adfd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello!', 'START Salut !')\n",
      "('Attack!', 'START Attaquez !')\n",
      "('Hello!', 'START Salut !')\n",
      "('I see.', 'START Je comprends.')\n",
      "('I try.', \"START J'essaye.\")\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "\n",
    "class Tokens(Enum):\n",
    "    START = \"START \"\n",
    "    END = \"<|endoftext|>\"\n",
    "    PAD = \" PAD\"\n",
    "    START_NUM = 23380\n",
    "    END_NUM = 100257\n",
    "    PAD_NUM = 62854\n",
    "    \n",
    "\n",
    "class EnFrDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        with open(file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            # we want data indexes start from 0, but filter out the first header row\n",
    "            for i, row in enumerate(reader, start=-1):\n",
    "                if i == -1:\n",
    "                    continue\n",
    "                en = row[0]\n",
    "                fr = Tokens.START.value + row[1]\n",
    "                self._data.append(tuple([en, fr]))\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "\n",
    "    class Iterator():\n",
    "\n",
    "        def __init__(self, outer):\n",
    "            self.cur = 0\n",
    "            self.outer = outer\n",
    "\n",
    "        def __next__(self):\n",
    "            if self.cur == len(self.outer._data):\n",
    "                raise StopIteration()\n",
    "            cur = self.outer._data[self.cur]\n",
    "            self.cur += 1\n",
    "            return cur\n",
    "\n",
    "    def __iter__(self):\n",
    "        return EnFrDataset.Iterator(self)\n",
    "    \n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = EnFrDataset(\"../data/eng_-french-nano.csv\", val_ratio=0.3)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d09eb729-53bb-420b-a608-73b1365e6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([9906, 0], [23380], [8375])\n",
      "([40, 1518, 13], [23380], [14465])\n",
      "([40, 1518, 13], [23380], [14465])\n",
      "([40, 1518, 13], [23380, 14465, 60946, 82], [14465, 60946, 82, 13])\n",
      "([40, 1456, 13], [23380, 622], [622, 6])\n"
     ]
    }
   ],
   "source": [
    "class TokEnFrDataset(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def build_train_sample(en_str: str, dec_str: str):\n",
    "        en_encoded = encoding.encode(en_str)\n",
    "        dec_encoded = encoding.encode(dec_str)\n",
    "        dec_encoded.append(Tokens.END_NUM.value)\n",
    "        en_sents = []\n",
    "        dec_sents = []\n",
    "        target_sents = []\n",
    "        \n",
    "        for i in range(1, len(dec_encoded)):\n",
    "            dec_sents.append(dec_encoded[:i])\n",
    "            target_sents.append(dec_encoded[1: i + 1])\n",
    "        en_sents.extend([en_encoded] * len(dec_sents))\n",
    "        return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        self._dataset = EnFrDataset(file, partition, val_ratio=0)\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        i = 0\n",
    "        for en, fr in self._dataset:\n",
    "            for sample in self.build_train_sample(en, fr):\n",
    "                self._data.append(sample)\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "                i += 1\n",
    "\n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french-nano.csv\", val_ratio=0.3)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16252c2e-6c54-44c5-a4aa-4cfb4de90897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  40, 1518,   13],\n",
      "        [  40, 1518,   13],\n",
      "        [  40, 1456,   13],\n",
      "        [  40, 1456,   13],\n",
      "        [  40, 2834,    0]]), tensor([[23380, 62854, 62854, 62854, 62854, 62854],\n",
      "        [23380, 14465, 60946,    82, 62854, 62854],\n",
      "        [23380,   622, 62854, 62854, 62854, 62854],\n",
      "        [23380,   622,     6,   434, 61055,    13],\n",
      "        [23380,   622, 34155, 62854, 62854, 62854]]), tensor([[ 14465,  62854,  62854,  62854,  62854,  62854],\n",
      "        [ 14465,  60946,     82,     13,  62854,  62854],\n",
      "        [   622,      6,  62854,  62854,  62854,  62854],\n",
      "        [   622,      6,    434,  61055,     13, 100257],\n",
      "        [   622,  34155,    342,  62854,  62854,  62854]]), tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]]), tensor([[1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    # print(batch)\n",
    "    _x, _y, _label = list(zip(*batch))\n",
    "    enc_x = pad_sequence([torch.tensor(t) for t in _x], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    dec_x = pad_sequence([torch.tensor(t) for t in _y], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    label = pad_sequence([torch.tensor(t) for t in _label], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    enc_mask = build_padding_mask(enc_x, pad_token=Tokens.PAD_NUM.value)\n",
    "    dec_mask = build_padding_mask(dec_x, pad_token=Tokens.PAD_NUM.value)\n",
    "    return enc_x, dec_x, label, enc_mask, dec_mask\n",
    "\n",
    "training_generator = DataLoader(dataset, collate_fn=collate, batch_size=5, num_workers=0)\n",
    "for batch in training_generator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98593c81-b0c3-4145-95bc-1965620ceb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Number of model's params: 13169205\n",
      "EPOCH 1:\n",
      "Average epoch loss: 8.106599674123533\n",
      "Average valid loss 4.144688129425049\n",
      "EPOCH 11:\n",
      "Average epoch loss: 0.685813937924931\n",
      "Average valid loss 0.4073200821876526\n",
      "EPOCH 21:\n",
      "Average epoch loss: 0.33494060620131244\n",
      "Average valid loss 0.2065637856721878\n",
      "EPOCH 31:\n",
      "Average epoch loss: 0.20546557471294644\n",
      "Average valid loss 0.141575887799263\n",
      "EPOCH 41:\n",
      "Average epoch loss: 0.13993241453412203\n",
      "Average valid loss 0.10870311409235\n",
      "EPOCH 51:\n",
      "Average epoch loss: 0.10061976584453744\n",
      "Average valid loss 0.08961375802755356\n",
      "EPOCH 61:\n",
      "Average epoch loss: 0.08324929816304528\n",
      "Average valid loss 0.08223778009414673\n",
      "EPOCH 71:\n",
      "Average epoch loss: 0.06446457709770793\n",
      "Average valid loss 0.07006324082612991\n",
      "EPOCH 81:\n",
      "Average epoch loss: 0.05611894578347663\n",
      "Average valid loss 0.06859173625707626\n",
      "EPOCH 91:\n",
      "Average epoch loss: 0.04765229211448584\n",
      "Average valid loss 0.06146994233131409\n",
      "EPOCH 101:\n",
      "Average epoch loss: 0.042575410269769735\n",
      "Average valid loss 0.05580863356590271\n",
      "EPOCH 111:\n",
      "Average epoch loss: 0.040262312440332744\n",
      "Average valid loss 0.054910559207201004\n",
      "EPOCH 121:\n",
      "Average epoch loss: 0.03637315999564392\n",
      "Average valid loss 0.05688120052218437\n",
      "EPOCH 131:\n",
      "Average epoch loss: 0.03302219765898395\n",
      "Average valid loss 0.054144587367773056\n",
      "EPOCH 141:\n",
      "Average epoch loss: 0.030165677694300782\n",
      "Average valid loss 0.05107171833515167\n",
      "EPOCH 151:\n",
      "Average epoch loss: 0.029812287865917768\n",
      "Average valid loss 0.05064011365175247\n",
      "EPOCH 161:\n",
      "Average epoch loss: 0.059406669005246986\n",
      "Average valid loss 0.05767767131328583\n",
      "EPOCH 171:\n",
      "Average epoch loss: 0.024796097502028904\n",
      "Average valid loss 0.045482371002435684\n",
      "EPOCH 181:\n",
      "Average epoch loss: 0.026251178155040956\n",
      "Average valid loss 0.04874633997678757\n",
      "EPOCH 191:\n",
      "Average epoch loss: 0.024733023012550436\n",
      "Average valid loss 0.04872668534517288\n",
      "EPOCH 201:\n",
      "Average epoch loss: 0.02141428553263668\n",
      "Average valid loss 0.05146006867289543\n",
      "EPOCH 211:\n",
      "Average epoch loss: 0.02164777066925934\n",
      "Average valid loss 0.04836469516158104\n",
      "EPOCH 221:\n",
      "Average epoch loss: 0.020763018120427777\n",
      "Average valid loss 0.04583194851875305\n",
      "EPOCH 231:\n",
      "Average epoch loss: 0.01993767456071152\n",
      "Average valid loss 0.04567849636077881\n",
      "EPOCH 241:\n",
      "Average epoch loss: 0.020016117459197083\n",
      "Average valid loss 0.049119941890239716\n",
      "EPOCH 251:\n",
      "Average epoch loss: 0.01906507501933986\n",
      "Average valid loss 0.04395957663655281\n",
      "EPOCH 261:\n",
      "Average epoch loss: 0.016151751332840875\n",
      "Average valid loss 0.043576378375291824\n",
      "EPOCH 271:\n",
      "Average epoch loss: 0.01858293884741464\n",
      "Average valid loss 0.04417881369590759\n",
      "EPOCH 281:\n",
      "Average epoch loss: 0.01686855584149871\n",
      "Average valid loss 0.04293527081608772\n",
      "EPOCH 291:\n",
      "Average epoch loss: 0.017354142303638232\n",
      "Average valid loss 0.043023329228162766\n",
      "EPOCH 301:\n",
      "Average epoch loss: 0.01604776750081392\n",
      "Average valid loss 0.04510717839002609\n",
      "EPOCH 311:\n",
      "Average epoch loss: 0.01728457998993632\n",
      "Average valid loss 0.04710175469517708\n",
      "EPOCH 321:\n",
      "Average epoch loss: 0.015124478547154848\n",
      "Average valid loss 0.04358559101819992\n",
      "EPOCH 331:\n",
      "Average epoch loss: 0.014798453523623441\n",
      "Average valid loss 0.04261250048875809\n",
      "EPOCH 341:\n",
      "Average epoch loss: 0.015253439618642213\n",
      "Average valid loss 0.04357008635997772\n",
      "EPOCH 351:\n",
      "Average epoch loss: 0.01411587229338048\n",
      "Average valid loss 0.04370526969432831\n",
      "EPOCH 361:\n",
      "Average epoch loss: 0.0142162412780581\n",
      "Average valid loss 0.04473298788070679\n",
      "EPOCH 371:\n",
      "Average epoch loss: 0.014231249821274312\n",
      "Average valid loss 0.04293028637766838\n",
      "EPOCH 381:\n",
      "Average epoch loss: 0.013667436700061045\n",
      "Average valid loss 0.04415249079465866\n",
      "EPOCH 391:\n",
      "Average epoch loss: 0.013170929336737178\n",
      "Average valid loss 0.041588831692934036\n",
      "EPOCH 401:\n",
      "Average epoch loss: 0.013598400244764813\n",
      "Average valid loss 0.04113977402448654\n",
      "EPOCH 411:\n",
      "Average epoch loss: 0.012854211723693531\n",
      "Average valid loss 0.040119439363479614\n",
      "EPOCH 421:\n",
      "Average epoch loss: 0.012083334117519618\n",
      "Average valid loss 0.04071919620037079\n",
      "EPOCH 431:\n",
      "Average epoch loss: 0.011592399637745278\n",
      "Average valid loss 0.04139595851302147\n",
      "EPOCH 441:\n",
      "Average epoch loss: 0.012800988897251134\n",
      "Average valid loss 0.0389932282269001\n",
      "EPOCH 451:\n",
      "Average epoch loss: 0.012490032287621806\n",
      "Average valid loss 0.04225161299109459\n",
      "EPOCH 461:\n",
      "Average epoch loss: 0.012311412015700853\n",
      "Average valid loss 0.03914128988981247\n",
      "EPOCH 471:\n",
      "Average epoch loss: 0.011290866434847772\n",
      "Average valid loss 0.04148925095796585\n",
      "EPOCH 481:\n",
      "Average epoch loss: 0.011861395201803331\n",
      "Average valid loss 0.03964927792549133\n",
      "EPOCH 491:\n",
      "Average epoch loss: 0.011428532447550132\n",
      "Average valid loss 0.04104817658662796\n",
      "EPOCH 501:\n",
      "Average epoch loss: 0.010490128440607881\n",
      "Average valid loss 0.04192206263542175\n",
      "EPOCH 511:\n",
      "Average epoch loss: 0.011170914409958738\n",
      "Average valid loss 0.041467756032943726\n",
      "EPOCH 521:\n",
      "Average epoch loss: 0.010570066472686145\n",
      "Average valid loss 0.03862183168530464\n",
      "EPOCH 531:\n",
      "Average epoch loss: 0.010988794562293515\n",
      "Average valid loss 0.040894780308008194\n",
      "EPOCH 541:\n",
      "Average epoch loss: 0.010070331365810129\n",
      "Average valid loss 0.039839595556259155\n",
      "EPOCH 551:\n",
      "Average epoch loss: 0.009887728843649208\n",
      "Average valid loss 0.038713905960321426\n",
      "EPOCH 561:\n",
      "Average epoch loss: 0.00982390826275114\n",
      "Average valid loss 0.03755166754126549\n",
      "EPOCH 571:\n",
      "Average epoch loss: 0.01012675562199959\n",
      "Average valid loss 0.04181039333343506\n",
      "EPOCH 581:\n",
      "Average epoch loss: 0.009359611861548755\n",
      "Average valid loss 0.041548360139131546\n",
      "EPOCH 591:\n",
      "Average epoch loss: 0.009610769552875008\n",
      "Average valid loss 0.03909565508365631\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m             best_val_loss \u001b[38;5;241m=\u001b[39m avg_val_loss\n\u001b[1;32m     95\u001b[0m             save_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 90\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 90\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m validate_epoch(epoch)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[35], line 49\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, encoding\u001b[38;5;241m.\u001b[39mn_vocab), label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     48\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 49\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    233\u001b[0m         group,\n\u001b[1;32m    234\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         state_steps,\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/optim/adamw.py:425\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    422\u001b[0m     device_beta1 \u001b[38;5;241m=\u001b[39m beta1\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import torch\n",
    "import math\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# torch.set_default_device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# batch = 16 if memory is not enough\n",
    "training_params = {\n",
    "    'collate_fn': collate,\n",
    "    'batch_size': 16,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0}\n",
    "max_epochs = 1000\n",
    "model_dir = \"../model/\"\n",
    "model_final_path = model_dir + \"model-final-small.pt\"\n",
    "model_intermediate_path = model_dir + \"model-intermediate-small.pt\"\n",
    "\n",
    "# Generators\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french-small.csv\", val_ratio=0.1)\n",
    "dataloader = DataLoader(dataset, **training_params)\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=2, d=64, h=4)\n",
    "print(f\"Number of model's params: {sum(p.numel() for p in transformer.parameters())}\")\n",
    "transformer = transformer.to(device)\n",
    "loss_fn = F.cross_entropy # bug: I don't ignore padding tokens. Without that, training still worked, fixed in py code\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "def save_model(model_path):\n",
    "    torch.save(transformer.state_dict(), model_path)\n",
    "    \n",
    "def train_epoch(epoch):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    best_loss = math.inf\n",
    "\n",
    "    transformer.train(True)\n",
    "    dataset.partition = Partition.TRAIN\n",
    "\n",
    "    for i, data in enumerate(dataloader, 1):\n",
    "        enc_x, dec_x, label, enc_mask, dec_mask = data\n",
    "        enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "        # Clear grads\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "        loss = loss_fn(output.view(-1, encoding.n_vocab), label.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            last_loss = running_loss / 1000\n",
    "            print('Average batch loss: {}'.format(last_loss))\n",
    "            running_loss = 0.\n",
    "            # if last_loss < best_loss:\n",
    "            #     save_model()\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            torch.mps.empty_cache()\n",
    "    if epoch % 10 == 0:\n",
    "        print('Average epoch loss: {}'.format(last_loss if last_loss > 0 else running_loss / i))\n",
    "    return last_loss\n",
    "\n",
    "def validate_epoch(epoch):\n",
    "    running_vloss = 0.0\n",
    "    transformer.eval()\n",
    "    dataset.partition = Partition.VAL\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(dataloader):\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = vdata\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "            output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "            vloss = loss_fn(output.view(-1, encoding.n_vocab), label.view(-1))\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Average valid loss {}'.format(avg_vloss))\n",
    "\n",
    "    return avg_vloss\n",
    "\n",
    "def train():\n",
    "    best_val_loss = math.inf\n",
    "    for epoch in range(max_epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print('EPOCH {}:'.format(epoch + 1))\n",
    "        avg_train_loss = train_epoch(epoch)\n",
    "        \n",
    "        avg_val_loss = validate_epoch(epoch)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_model(model_final_path)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "955bb4ae-5548-4843-b7a2-5ece5b653035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24703, 4632, 1826, 10253, 60404, 13, 100257]\n",
      "predicted sentence: \n",
      "  Ce film est aux enfants.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Predicting next words\n",
    "sent = \"This movies is for children\"\n",
    "encoded_sent = encoding.encode(sent)\n",
    "enc_x = torch.tensor(encoded_sent).unsqueeze(0)\n",
    "enc_x = enc_x.to(device)\n",
    "dec_x = torch.tensor(encoding.encode(\"START\")).unsqueeze(0)\n",
    "dec_x = dec_x.to(device)\n",
    "\n",
    "transformer_inference = transformer\n",
    "transformer_inference.load_state_dict(torch.load(\"../data/model-final-small.pt\"))\n",
    "# transformer_inference.load_state_dict(torch.load(model_final_path))\n",
    "transformer_inference.eval()\n",
    "\n",
    "predicted_tokens = []\n",
    "for _ in range(20):\n",
    "    output = transformer_inference(enc_x=enc_x, dec_x=dec_x)\n",
    "    softmaxed = F.softmax(output, dim=-1)\n",
    "    predicted = softmaxed.argmax(dim=-1)\n",
    "    # FIXME bug here: wrong vector is appended. See inference.py\n",
    "    predicted_tokens.append(predicted.tolist()[-1][-1]) \n",
    "    dec_x = torch.cat((dec_x, predicted), dim=-1)\n",
    "    if predicted[-1, -1] == encoding.eot_token:\n",
    "        break\n",
    "\n",
    "print(predicted_tokens)\n",
    "print(f\"predicted sentence: \\n {encoding.decode(predicted_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819338a-9df0-465b-b1cf-a0a6e8a59d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((2, 3, 32), requires_grad=True)\n",
    "print(input.view(-1, 32).shape)\n",
    "target = torch.empty((2, 3), dtype=torch.long).random_(32)\n",
    "print(target)\n",
    "loss = F.cross_entropy(input.view(-1, 32), target.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05248576-1b7f-4994-80fe-c26d2dc1a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffa =\"afsa\"\n",
    "\"fdsa {}\".format(ffa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef7bbe-c8a8-493e-8677-49d9c02dbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def build_train_sample(en_str: str, dec_str: str):\n",
    "    en_encoded = encoding.encode(en_str)\n",
    "    dec_encoded = encoding.encode(dec_str)\n",
    "    dec_encoded.append(Tokens.END_NUM.value)\n",
    "    en_sents = []\n",
    "    dec_sents = []\n",
    "    target_sents = []\n",
    "    \n",
    "    for i in range(1, len(dec_encoded)):\n",
    "        dec_sents.append(dec_encoded[:i])\n",
    "        target_sents.append(dec_encoded[1: i + 1])\n",
    "    en_sents.extend([en_encoded] * len(dec_sents))\n",
    "    return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "build_train_sample('Hi.', 'START Salut!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfee8d-38b9-4abb-b127-5e620ea387f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# def collate_fn\n",
    "\n",
    "dataset.partition = Partition.TRAIN\n",
    "training_generator = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "for i, s in enumerate(training_generator):\n",
    "    print(s)\n",
    "    if i > 2:\n",
    "        break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d5995-4736-4fec-9e51-e3672a9dc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty((2, 3), dtype=torch.long).random_(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0f4c4-a05a-4353-b58d-f50a60157f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(transformer.encoder.layers[0].mhsa.wq.weight.device)\n",
    "print(next(transformer.parameters()).device)\n",
    "print(transformer.decoder.layers[0].mhsa.wq.weight.device)\n",
    "print(next(transformer.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd67961-4fe8-4c69-90ec-f351df169e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62b3c0-a20a-466d-b5fe-05b33071e963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0dbc9-7e82-48b4-9b90-0ddf28b10784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21175a-9751-4413-8cdb-579c52134778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207745b-3455-48f2-a6c1-d6546569344a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbfc3e-aca8-4edb-99e5-025c945d85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da787162-9955-4de5-a82d-60501d62ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08f9da-be37-4a19-992e-11b2c6e1ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a2346-9e74-4eab-aab3-bd5bb0a0d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.3]]), dim=-1)\n",
    "torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]).argmax(dim=-1)\n",
    "# torch.max(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]), dim=-1)\n",
    "\n",
    "t1 = torch.tensor([[0.1, 0.2]])\n",
    "t2 = torch.tensor([[0.3]])\n",
    "torch.cat((t1, t2), dim=1).tolist()[-1][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683599c0-fd00-40b0-8bb1-4e477a2114a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45e55c-6d60-409b-b26e-0b218224483a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e70b0-a647-455b-bdfb-332d095aafc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082af878-9425-4445-b4b4-592cef79a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba170-623a-486b-8b67-58360bde104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d1a8-69aa-4c60-b3b4-ebb2e31f1c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
