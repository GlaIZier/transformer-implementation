{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d7f736-b9af-48e3-adb5-18ab3536abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n",
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    print(softmaxed_prod.shape)\n",
    "    # print(softmaxed_prod)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x)\n",
    "self_attention(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abcca093-9e66-4b46-8ec3-aded63ece6f8",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a9e6b-8461-4417-ab07-e5fbd79c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c5bf9-3800-4b22-b7d8-8886d4f15b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d626059-fd88-4a03-8b20-ffbf0115fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayerWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayerWithoutMask()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851eff3d-4816-4410-9184-f30c157e4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayerWithoutMask(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = EncoderWithoutMask()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With masks\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim)\n",
    "    if mask is not None:\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    # print(softmaxed_prod.shape)\n",
    "    print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eac5e23-ea43-4295-ba24-a7edbe1a1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0337, 0.7546, 0.4892, 0.4788],\n",
      "          [0.8496, 0.1342, 0.4397, 0.3711]],\n",
      "\n",
      "         [[0.4465, 0.0874, 0.8815, 0.0073],\n",
      "          [0.0447, 0.7758, 0.3264, 0.7583]],\n",
      "\n",
      "         [[0.7725, 0.8947, 0.0285, 0.5003],\n",
      "          [0.3127, 0.1241, 0.1790, 0.8432]]],\n",
      "\n",
      "\n",
      "        [[[0.7086, 0.6079, 0.4308, 0.6200],\n",
      "          [0.4433, 0.9256, 0.3265, 0.1734]],\n",
      "\n",
      "         [[0.6303, 0.8848, 0.8056, 0.7326],\n",
      "          [0.8529, 0.0713, 0.8927, 0.0179]],\n",
      "\n",
      "         [[0.4129, 0.5835, 0.8728, 0.9875],\n",
      "          [0.9557, 0.0736, 0.9581, 0.0886]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.5195, 0.2578,   -inf],\n",
      "          [0.2578, 0.4920,   -inf],\n",
      "          [0.4773, 0.2259,   -inf]],\n",
      "\n",
      "         [[0.5354,   -inf,   -inf],\n",
      "          [0.2835,   -inf,   -inf],\n",
      "          [0.3370,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7208, 0.8929,   -inf],\n",
      "          [0.8929, 1.1830,   -inf],\n",
      "          [0.8178, 1.1016,   -inf]],\n",
      "\n",
      "         [[0.5950,   -inf,   -inf],\n",
      "          [0.3694,   -inf,   -inf],\n",
      "          [0.4100,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5650, 0.4350, 0.0000],\n",
      "          [0.4417, 0.5583, 0.0000],\n",
      "          [0.5625, 0.4375, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4571, 0.5429, 0.0000],\n",
      "          [0.4280, 0.5720, 0.0000],\n",
      "          [0.4295, 0.5705, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "wrong a: \n",
      " tensor([[[[0.2132, 0.4644, 0.6598, 0.2737],\n",
      "          [0.2641, 0.3821, 0.7082, 0.2155],\n",
      "          [0.2143, 0.4627, 0.6608, 0.2725]],\n",
      "\n",
      "         [[0.8496, 0.1342, 0.4397, 0.3711],\n",
      "          [0.8496, 0.1342, 0.4397, 0.3711],\n",
      "          [0.8496, 0.1342, 0.4397, 0.3711]]],\n",
      "\n",
      "\n",
      "        [[[0.6661, 0.7583, 0.6343, 0.6811],\n",
      "          [0.6638, 0.7663, 0.6452, 0.6844],\n",
      "          [0.6639, 0.7659, 0.6446, 0.6843]],\n",
      "\n",
      "         [[0.4433, 0.9256, 0.3265, 0.1734],\n",
      "          [0.4433, 0.9256, 0.3265, 0.1734],\n",
      "          [0.4433, 0.9256, 0.3265, 0.1734]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.5195, 0.2578,   -inf],\n",
      "          [0.2578, 0.4920,   -inf],\n",
      "          [0.4773, 0.2259,   -inf]],\n",
      "\n",
      "         [[0.5354, 0.2835,   -inf],\n",
      "          [0.2835, 0.6427,   -inf],\n",
      "          [0.3370, 0.4040,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7208,   -inf,   -inf],\n",
      "          [0.8929,   -inf,   -inf],\n",
      "          [0.8178,   -inf,   -inf]],\n",
      "\n",
      "         [[0.5950,   -inf,   -inf],\n",
      "          [0.3694,   -inf,   -inf],\n",
      "          [0.4100,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5650, 0.4350, 0.0000],\n",
      "          [0.4417, 0.5583, 0.0000],\n",
      "          [0.5625, 0.4375, 0.0000]],\n",
      "\n",
      "         [[0.5626, 0.4374, 0.0000],\n",
      "          [0.4112, 0.5888, 0.0000],\n",
      "          [0.4832, 0.5168, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.2132, 0.4644, 0.6598, 0.2737],\n",
      "          [0.2641, 0.3821, 0.7082, 0.2155],\n",
      "          [0.2143, 0.4627, 0.6608, 0.2725]],\n",
      "\n",
      "         [[0.4976, 0.4148, 0.3901, 0.5405],\n",
      "          [0.3756, 0.5120, 0.3730, 0.5991],\n",
      "          [0.4336, 0.4657, 0.3811, 0.5712]]],\n",
      "\n",
      "\n",
      "        [[[0.7086, 0.6079, 0.4308, 0.6200],\n",
      "          [0.7086, 0.6079, 0.4308, 0.6200],\n",
      "          [0.7086, 0.6079, 0.4308, 0.6200]],\n",
      "\n",
      "         [[0.4433, 0.9256, 0.3265, 0.1734],\n",
      "          [0.4433, 0.9256, 0.3265, 0.1734],\n",
      "          [0.4433, 0.9256, 0.3265, 0.1734]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.0337, 0.7546, 0.4892, 0.4788],\n",
      "          [0.8496, 0.1342, 0.4397, 0.3711]],\n",
      "\n",
      "         [[0.4465, 0.0874, 0.8815, 0.0073],\n",
      "          [0.0447, 0.7758, 0.3264, 0.7583]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7086, 0.6079, 0.4308, 0.6200],\n",
      "          [0.4433, 0.9256, 0.3265, 0.1734]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.5195, 0.2578,   -inf],\n",
      "          [0.2578, 0.4920,   -inf],\n",
      "          [0.4773, 0.2259,   -inf]],\n",
      "\n",
      "         [[0.5354, 0.2835,   -inf],\n",
      "          [0.2835, 0.6427,   -inf],\n",
      "          [0.3370, 0.4040,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7208,   -inf,   -inf],\n",
      "          [0.8929,   -inf,   -inf],\n",
      "          [0.8178,   -inf,   -inf]],\n",
      "\n",
      "         [[0.5950,   -inf,   -inf],\n",
      "          [0.3694,   -inf,   -inf],\n",
      "          [0.4100,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5650, 0.4350, 0.0000],\n",
      "          [0.4417, 0.5583, 0.0000],\n",
      "          [0.5625, 0.4375, 0.0000]],\n",
      "\n",
      "         [[0.5626, 0.4374, 0.0000],\n",
      "          [0.4112, 0.5888, 0.0000],\n",
      "          [0.4832, 0.5168, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.2132, 0.4644, 0.6598, 0.2737],\n",
      "          [0.2641, 0.3821, 0.7082, 0.2155],\n",
      "          [0.2143, 0.4627, 0.6608, 0.2725]],\n",
      "\n",
      "         [[0.4976, 0.4148, 0.3901, 0.5405],\n",
      "          [0.3756, 0.5120, 0.3730, 0.5991],\n",
      "          [0.4336, 0.4657, 0.3811, 0.5712]]],\n",
      "\n",
      "\n",
      "        [[[0.7086, 0.6079, 0.4308, 0.6200],\n",
      "          [0.7086, 0.6079, 0.4308, 0.6200],\n",
      "          [0.7086, 0.6079, 0.4308, 0.6200]],\n",
      "\n",
      "         [[0.4433, 0.9256, 0.3265, 0.1734],\n",
      "          [0.4433, 0.9256, 0.3265, 0.1734],\n",
      "          [0.4433, 0.9256, 0.3265, 0.1734]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.8342, 0.5709, 0.6455, 0.5264],\n",
      "         [0.9785, 0.6350, 0.5114, 0.8188],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.7650, 0.2118, 0.8914, 0.7744],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.8342, 0.5709],\n",
      "          [0.6455, 0.5264]],\n",
      "\n",
      "         [[0.9785, 0.6350],\n",
      "          [0.5114, 0.8188]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.7650, 0.2118],\n",
      "          [0.8914, 0.7744]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.8342, 0.5709],\n",
      "          [0.9785, 0.6350],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.6455, 0.5264],\n",
      "          [0.5114, 0.8188],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.7650, 0.2118],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.8914, 0.7744],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.7092, 0.6447, 0.9705, 0.4897],\n",
      "         [0.4249, 0.8320, 0.5064, 0.2433],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.6591, 0.0514, 0.2106, 0.7436],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.7092, 0.6447],\n",
      "          [0.9705, 0.4897]],\n",
      "\n",
      "         [[0.4249, 0.8320],\n",
      "          [0.5064, 0.2433]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.6591, 0.0514],\n",
      "          [0.2106, 0.7436]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.7092, 0.6447],\n",
      "          [0.4249, 0.8320],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.9705, 0.4897],\n",
      "          [0.5064, 0.2433],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.6591, 0.0514],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.2106, 0.7436],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[0.9186, 0.8378,   -inf],\n",
      "          [0.8378, 0.8729,   -inf],\n",
      "          [0.7760, 0.5433,   -inf]],\n",
      "\n",
      "         [[1.1818, 0.6106,   -inf],\n",
      "          [0.6106, 0.3156,   -inf],\n",
      "          [1.2551, 0.6434,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4371,   -inf,   -inf],\n",
      "          [0.2027,   -inf,   -inf],\n",
      "          [0.4525,   -inf,   -inf]],\n",
      "\n",
      "         [[0.5973,   -inf,   -inf],\n",
      "          [0.7179,   -inf,   -inf],\n",
      "          [0.4900,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.7393e-01, 8.6882e-01, 5.8112e-02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [2.7302e-01, 5.3599e-01, 1.1629e-01, 1.8084e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [5.5423e-01, 6.4841e-01, 5.7348e-01, 9.0290e-01, 1.6250e-01, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [5.9095e-02, 7.3004e-01, 3.1840e-01, 8.1220e-01, 3.3544e-01, 7.4174e-01]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.1384e-02, 7.2508e-01, 4.4776e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [6.8923e-01, 8.2164e-01, 6.4738e-01, 4.7209e-01, 2.8932e-01, 1.0000e+02],\n",
      "        [2.2299e-01, 7.4573e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [2.8344e-01, 9.0366e-01, 9.7752e-01, 7.2346e-01, 8.8364e-02, 4.3336e-01]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(build_causal_mask(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0516,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0597,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1129,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0537,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0477,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1523,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2162,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2725,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0150,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0455,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0599, -0.0933,    -inf,    -inf,    -inf],\n",
      "          [-0.0713, -0.1141,    -inf,    -inf,    -inf],\n",
      "          [ 0.0077, -0.0550,    -inf,    -inf,    -inf],\n",
      "          [-0.0549, -0.0844,    -inf,    -inf,    -inf],\n",
      "          [-0.0694, -0.1183,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3200,  0.2003,    -inf,    -inf,    -inf],\n",
      "          [-0.1637, -0.1077,    -inf,    -inf,    -inf],\n",
      "          [ 0.1481,  0.0826,    -inf,    -inf,    -inf],\n",
      "          [ 0.1733,  0.0753,    -inf,    -inf,    -inf],\n",
      "          [ 0.0484,  0.0176,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1606, -0.1166, -0.1301,    -inf,    -inf],\n",
      "          [-0.2297, -0.1532, -0.1930,    -inf,    -inf],\n",
      "          [ 0.0523, -0.0233,  0.0617,    -inf,    -inf],\n",
      "          [-0.0922, -0.0571, -0.0820,    -inf,    -inf],\n",
      "          [-0.2924, -0.1742, -0.2399,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0119,  0.0961,  0.1023,    -inf,    -inf],\n",
      "          [-0.0268,  0.0785,  0.0420,    -inf,    -inf],\n",
      "          [-0.3124, -0.1815, -0.2019,    -inf,    -inf],\n",
      "          [ 0.2007,  0.2211,  0.2051,    -inf,    -inf],\n",
      "          [ 0.1178,  0.2259,  0.2510,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0513, -0.0315, -0.0355, -0.0135,    -inf],\n",
      "          [-0.1528, -0.0183, -0.1418, -0.0842,    -inf],\n",
      "          [-0.0558, -0.0248, -0.0279, -0.0143,    -inf],\n",
      "          [ 0.0269, -0.0167,  0.0618,  0.0370,    -inf],\n",
      "          [-0.2317, -0.0286, -0.2300, -0.1332,    -inf]],\n",
      "\n",
      "         [[ 0.0982,  0.0499,  0.1260,  0.2634,    -inf],\n",
      "          [ 0.1710,  0.0774,  0.1016,  0.1483,    -inf],\n",
      "          [ 0.0625,  0.0039,  0.0323,  0.1212,    -inf],\n",
      "          [ 0.0418, -0.0013,  0.0182,  0.0842,    -inf],\n",
      "          [ 0.2245,  0.1202,  0.1693,  0.2373,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5084, 0.4916, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5107, 0.4893, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5157, 0.4843, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5074, 0.4926, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5122, 0.4878, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5299, 0.4701, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4860, 0.5140, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5164, 0.4836, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5245, 0.4755, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5077, 0.4923, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3251, 0.3397, 0.3352, 0.0000, 0.0000],\n",
      "          [0.3208, 0.3463, 0.3328, 0.0000, 0.0000],\n",
      "          [0.3405, 0.3157, 0.3437, 0.0000, 0.0000],\n",
      "          [0.3283, 0.3400, 0.3317, 0.0000, 0.0000],\n",
      "          [0.3145, 0.3540, 0.3315, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3091, 0.3444, 0.3465, 0.0000, 0.0000],\n",
      "          [0.3142, 0.3491, 0.3366, 0.0000, 0.0000],\n",
      "          [0.3071, 0.3500, 0.3429, 0.0000, 0.0000],\n",
      "          [0.3306, 0.3374, 0.3320, 0.0000, 0.0000],\n",
      "          [0.3071, 0.3421, 0.3508, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2454, 0.2503, 0.2494, 0.2549, 0.0000],\n",
      "          [0.2366, 0.2707, 0.2392, 0.2534, 0.0000],\n",
      "          [0.2438, 0.2514, 0.2507, 0.2541, 0.0000],\n",
      "          [0.2498, 0.2392, 0.2587, 0.2524, 0.0000],\n",
      "          [0.2309, 0.2829, 0.2313, 0.2548, 0.0000]],\n",
      "\n",
      "         [[0.2403, 0.2290, 0.2471, 0.2835, 0.0000],\n",
      "          [0.2617, 0.2383, 0.2442, 0.2558, 0.0000],\n",
      "          [0.2516, 0.2373, 0.2442, 0.2669, 0.0000],\n",
      "          [0.2514, 0.2408, 0.2455, 0.2623, 0.0000],\n",
      "          [0.2591, 0.2334, 0.2451, 0.2624, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-0.1800,  0.9685,  0.6245,  0.0101, -0.5205,  0.1797],\n",
      "         [-0.1800,  0.9685,  0.6245,  0.0101, -0.5205,  0.1797],\n",
      "         [-0.1800,  0.9685,  0.6245,  0.0101, -0.5205,  0.1797],\n",
      "         [-0.1800,  0.9685,  0.6245,  0.0101, -0.5205,  0.1797],\n",
      "         [-0.1800,  0.9685,  0.6245,  0.0101, -0.5205,  0.1797]],\n",
      "\n",
      "        [[-0.2756,  0.8712,  0.6094,  0.0626, -0.4135,  0.0843],\n",
      "         [-0.2654,  0.8802,  0.6147,  0.0612, -0.4198,  0.0862],\n",
      "         [-0.2727,  0.8742,  0.6118,  0.0630, -0.4159,  0.0846],\n",
      "         [-0.2743,  0.8722,  0.6099,  0.0623, -0.4141,  0.0846],\n",
      "         [-0.2706,  0.8758,  0.6124,  0.0622, -0.4168,  0.0851]],\n",
      "\n",
      "        [[-0.2070,  0.9410,  0.6304, -0.0385, -0.4801,  0.0958],\n",
      "         [-0.2053,  0.9415,  0.6315, -0.0375, -0.4809,  0.0960],\n",
      "         [-0.2085,  0.9438,  0.6306, -0.0390, -0.4809,  0.0965],\n",
      "         [-0.2052,  0.9428,  0.6327, -0.0377, -0.4819,  0.0954],\n",
      "         [-0.2064,  0.9391,  0.6299, -0.0383, -0.4792,  0.0955]],\n",
      "\n",
      "        [[-0.3447,  0.7379,  0.5939,  0.0590, -0.3805,  0.0191],\n",
      "         [-0.3389,  0.7436,  0.5995,  0.0612, -0.3861,  0.0199],\n",
      "         [-0.3415,  0.7416,  0.5969,  0.0606, -0.3834,  0.0198],\n",
      "         [-0.3408,  0.7425,  0.5973,  0.0612, -0.3836,  0.0200],\n",
      "         [-0.3400,  0.7422,  0.5985,  0.0602, -0.3856,  0.0195]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0516,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0597,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1129,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0537,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0477,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1523,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2162,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2725,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0150,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0455,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0599, -0.0933,    -inf,    -inf,    -inf],\n",
      "          [-0.0713, -0.1141,    -inf,    -inf,    -inf],\n",
      "          [ 0.0077, -0.0550,    -inf,    -inf,    -inf],\n",
      "          [-0.0549, -0.0844,    -inf,    -inf,    -inf],\n",
      "          [-0.0694, -0.1183,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3200,  0.2003,    -inf,    -inf,    -inf],\n",
      "          [-0.1637, -0.1077,    -inf,    -inf,    -inf],\n",
      "          [ 0.1481,  0.0826,    -inf,    -inf,    -inf],\n",
      "          [ 0.1733,  0.0753,    -inf,    -inf,    -inf],\n",
      "          [ 0.0484,  0.0176,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1606, -0.1166, -0.1301,    -inf,    -inf],\n",
      "          [-0.2297, -0.1532, -0.1930,    -inf,    -inf],\n",
      "          [ 0.0523, -0.0233,  0.0617,    -inf,    -inf],\n",
      "          [-0.0922, -0.0571, -0.0820,    -inf,    -inf],\n",
      "          [-0.2924, -0.1742, -0.2399,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0119,  0.0961,  0.1023,    -inf,    -inf],\n",
      "          [-0.0268,  0.0785,  0.0420,    -inf,    -inf],\n",
      "          [-0.3124, -0.1815, -0.2019,    -inf,    -inf],\n",
      "          [ 0.2007,  0.2211,  0.2051,    -inf,    -inf],\n",
      "          [ 0.1178,  0.2259,  0.2510,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0513, -0.0315, -0.0355, -0.0135,    -inf],\n",
      "          [-0.1528, -0.0183, -0.1418, -0.0842,    -inf],\n",
      "          [-0.0558, -0.0248, -0.0279, -0.0143,    -inf],\n",
      "          [ 0.0269, -0.0167,  0.0618,  0.0370,    -inf],\n",
      "          [-0.2317, -0.0286, -0.2300, -0.1332,    -inf]],\n",
      "\n",
      "         [[ 0.0982,  0.0499,  0.1260,  0.2634,    -inf],\n",
      "          [ 0.1710,  0.0774,  0.1016,  0.1483,    -inf],\n",
      "          [ 0.0625,  0.0039,  0.0323,  0.1212,    -inf],\n",
      "          [ 0.0418, -0.0013,  0.0182,  0.0842,    -inf],\n",
      "          [ 0.2245,  0.1202,  0.1693,  0.2373,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5084, 0.4916, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5107, 0.4893, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5157, 0.4843, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5074, 0.4926, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5122, 0.4878, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5299, 0.4701, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4860, 0.5140, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5164, 0.4836, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5245, 0.4755, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5077, 0.4923, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3251, 0.3397, 0.3352, 0.0000, 0.0000],\n",
      "          [0.3208, 0.3463, 0.3328, 0.0000, 0.0000],\n",
      "          [0.3405, 0.3157, 0.3437, 0.0000, 0.0000],\n",
      "          [0.3283, 0.3400, 0.3317, 0.0000, 0.0000],\n",
      "          [0.3145, 0.3540, 0.3315, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3091, 0.3444, 0.3465, 0.0000, 0.0000],\n",
      "          [0.3142, 0.3491, 0.3366, 0.0000, 0.0000],\n",
      "          [0.3071, 0.3500, 0.3429, 0.0000, 0.0000],\n",
      "          [0.3306, 0.3374, 0.3320, 0.0000, 0.0000],\n",
      "          [0.3071, 0.3421, 0.3508, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2454, 0.2503, 0.2494, 0.2549, 0.0000],\n",
      "          [0.2366, 0.2707, 0.2392, 0.2534, 0.0000],\n",
      "          [0.2438, 0.2514, 0.2507, 0.2541, 0.0000],\n",
      "          [0.2498, 0.2392, 0.2587, 0.2524, 0.0000],\n",
      "          [0.2309, 0.2829, 0.2313, 0.2548, 0.0000]],\n",
      "\n",
      "         [[0.2403, 0.2290, 0.2471, 0.2835, 0.0000],\n",
      "          [0.2617, 0.2383, 0.2442, 0.2558, 0.0000],\n",
      "          [0.2516, 0.2373, 0.2442, 0.2669, 0.0000],\n",
      "          [0.2514, 0.2408, 0.2455, 0.2623, 0.0000],\n",
      "          [0.2591, 0.2334, 0.2451, 0.2624, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # q and k/v might be of different sizes if lengths of decoder and encoders inputs are different\n",
    "        bq, tq, dq = q.size()\n",
    "        bk, tk, dk = k.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(bq, tq, self.h, self.dh)\n",
    "        wk = wk.view(bk, tk, self.h, self.dh)\n",
    "        wv = wv.view(bk, tk, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(bq, self.h, tq, self.dh).transpose(1, 2).contiguous().view(bq, tq, dq)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729ae6f8-a3f8-4386-837d-f1e67f022b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0513,  0.1129,    -inf],\n",
      "          [ 0.1323,  0.1834,    -inf],\n",
      "          [ 0.0197,  0.1434,    -inf]],\n",
      "\n",
      "         [[-0.0898,  0.0950,    -inf],\n",
      "          [-0.4022, -0.1767,    -inf],\n",
      "          [-0.0960,  0.0368,    -inf]],\n",
      "\n",
      "         [[-0.1026, -0.0266,    -inf],\n",
      "          [ 0.0562,  0.0133,    -inf],\n",
      "          [-0.0551, -0.0541,    -inf]],\n",
      "\n",
      "         [[-0.2408, -0.1780,    -inf],\n",
      "          [-0.2047, -0.1277,    -inf],\n",
      "          [-0.1683, -0.1773,    -inf]],\n",
      "\n",
      "         [[-0.0363, -0.0677,    -inf],\n",
      "          [ 0.0074, -0.1115,    -inf],\n",
      "          [ 0.0314,  0.0471,    -inf]],\n",
      "\n",
      "         [[ 0.0248,  0.0846,    -inf],\n",
      "          [-0.0082,  0.0698,    -inf],\n",
      "          [ 0.1005,  0.1536,    -inf]],\n",
      "\n",
      "         [[-0.2319, -0.1965,    -inf],\n",
      "          [-0.1059, -0.0988,    -inf],\n",
      "          [-0.0935, -0.0825,    -inf]],\n",
      "\n",
      "         [[-0.1003, -0.1075,    -inf],\n",
      "          [-0.1691, -0.1837,    -inf],\n",
      "          [-0.1610, -0.1094,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0569,    -inf,    -inf],\n",
      "          [ 0.2117,    -inf,    -inf],\n",
      "          [ 0.0889,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0625,    -inf,    -inf],\n",
      "          [-0.0297,    -inf,    -inf],\n",
      "          [-0.1715,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0322,    -inf,    -inf],\n",
      "          [ 0.0078,    -inf,    -inf],\n",
      "          [-0.1518,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0961,    -inf,    -inf],\n",
      "          [-0.2070,    -inf,    -inf],\n",
      "          [-0.1801,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0858,    -inf,    -inf],\n",
      "          [-0.0445,    -inf,    -inf],\n",
      "          [ 0.0581,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1440,    -inf,    -inf],\n",
      "          [ 0.1307,    -inf,    -inf],\n",
      "          [ 0.1641,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0701,    -inf,    -inf],\n",
      "          [-0.0493,    -inf,    -inf],\n",
      "          [-0.1872,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0750,    -inf,    -inf],\n",
      "          [-0.1378,    -inf,    -inf],\n",
      "          [-0.1381,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4590, 0.5410, 0.0000],\n",
      "          [0.4872, 0.5128, 0.0000],\n",
      "          [0.4691, 0.5309, 0.0000]],\n",
      "\n",
      "         [[0.4539, 0.5461, 0.0000],\n",
      "          [0.4439, 0.5561, 0.0000],\n",
      "          [0.4668, 0.5332, 0.0000]],\n",
      "\n",
      "         [[0.4810, 0.5190, 0.0000],\n",
      "          [0.5107, 0.4893, 0.0000],\n",
      "          [0.4998, 0.5002, 0.0000]],\n",
      "\n",
      "         [[0.4843, 0.5157, 0.0000],\n",
      "          [0.4808, 0.5192, 0.0000],\n",
      "          [0.5023, 0.4977, 0.0000]],\n",
      "\n",
      "         [[0.5079, 0.4921, 0.0000],\n",
      "          [0.5297, 0.4703, 0.0000],\n",
      "          [0.4961, 0.5039, 0.0000]],\n",
      "\n",
      "         [[0.4850, 0.5150, 0.0000],\n",
      "          [0.4805, 0.5195, 0.0000],\n",
      "          [0.4867, 0.5133, 0.0000]],\n",
      "\n",
      "         [[0.4911, 0.5089, 0.0000],\n",
      "          [0.4982, 0.5018, 0.0000],\n",
      "          [0.4972, 0.5028, 0.0000]],\n",
      "\n",
      "         [[0.5018, 0.4982, 0.0000],\n",
      "          [0.5036, 0.4964, 0.0000],\n",
      "          [0.4871, 0.5129, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, self_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "x = torch.rand(2, 3, 512)\n",
    "\n",
    "encoder_layer(x, self_mask=self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.2800,  0.2781,    -inf],\n",
      "          [ 0.2125,  0.1966,    -inf],\n",
      "          [-0.5418,  0.4564,    -inf]],\n",
      "\n",
      "         [[-1.1555, -1.2278,    -inf],\n",
      "          [-0.5386,  0.1059,    -inf],\n",
      "          [-0.1897, -0.6077,    -inf]],\n",
      "\n",
      "         [[ 0.2123, -0.1201,    -inf],\n",
      "          [-0.2568, -0.1359,    -inf],\n",
      "          [ 0.6022,  0.3646,    -inf]],\n",
      "\n",
      "         [[-0.2766, -0.1987,    -inf],\n",
      "          [-0.3950, -0.2208,    -inf],\n",
      "          [ 0.3022,  0.8284,    -inf]],\n",
      "\n",
      "         [[-0.4127, -0.3550,    -inf],\n",
      "          [-0.2292,  0.0885,    -inf],\n",
      "          [ 0.2484, -0.2246,    -inf]],\n",
      "\n",
      "         [[ 0.3299, -0.3460,    -inf],\n",
      "          [ 0.3055, -0.1399,    -inf],\n",
      "          [ 0.5072, -0.3074,    -inf]],\n",
      "\n",
      "         [[ 0.4501,  0.0128,    -inf],\n",
      "          [-0.8774, -1.1258,    -inf],\n",
      "          [-0.2468,  0.3027,    -inf]],\n",
      "\n",
      "         [[ 0.0208,  1.1188,    -inf],\n",
      "          [ 0.1792,  0.8513,    -inf],\n",
      "          [ 0.0819, -0.5866,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1754,    -inf,    -inf],\n",
      "          [ 0.2623,    -inf,    -inf],\n",
      "          [ 0.5625,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4276,    -inf,    -inf],\n",
      "          [ 0.6513,    -inf,    -inf],\n",
      "          [ 0.4629,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.9038,    -inf,    -inf],\n",
      "          [-0.3370,    -inf,    -inf],\n",
      "          [ 0.2888,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0283,    -inf,    -inf],\n",
      "          [-0.7162,    -inf,    -inf],\n",
      "          [-0.7027,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1692,    -inf,    -inf],\n",
      "          [-0.4889,    -inf,    -inf],\n",
      "          [ 0.2677,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3031,    -inf,    -inf],\n",
      "          [ 0.0691,    -inf,    -inf],\n",
      "          [ 0.4723,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5032,    -inf,    -inf],\n",
      "          [ 0.2272,    -inf,    -inf],\n",
      "          [ 1.0167,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1221,    -inf,    -inf],\n",
      "          [-0.4792,    -inf,    -inf],\n",
      "          [-0.7413,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5005, 0.4995, 0.0000],\n",
      "          [0.5040, 0.4960, 0.0000],\n",
      "          [0.2693, 0.7307, 0.0000]],\n",
      "\n",
      "         [[0.5181, 0.4819, 0.0000],\n",
      "          [0.3442, 0.6558, 0.0000],\n",
      "          [0.6030, 0.3970, 0.0000]],\n",
      "\n",
      "         [[0.5823, 0.4177, 0.0000],\n",
      "          [0.4698, 0.5302, 0.0000],\n",
      "          [0.5591, 0.4409, 0.0000]],\n",
      "\n",
      "         [[0.4806, 0.5194, 0.0000],\n",
      "          [0.4566, 0.5434, 0.0000],\n",
      "          [0.3714, 0.6286, 0.0000]],\n",
      "\n",
      "         [[0.4856, 0.5144, 0.0000],\n",
      "          [0.4212, 0.5788, 0.0000],\n",
      "          [0.6161, 0.3839, 0.0000]],\n",
      "\n",
      "         [[0.6628, 0.3372, 0.0000],\n",
      "          [0.6095, 0.3905, 0.0000],\n",
      "          [0.6931, 0.3069, 0.0000]],\n",
      "\n",
      "         [[0.6076, 0.3924, 0.0000],\n",
      "          [0.5618, 0.4382, 0.0000],\n",
      "          [0.3660, 0.6340, 0.0000]],\n",
      "\n",
      "         [[0.2501, 0.7499, 0.0000],\n",
      "          [0.3380, 0.6620, 0.0000],\n",
      "          [0.6612, 0.3388, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.3148,  0.6372,    -inf],\n",
      "          [-0.0905,  0.1849,    -inf],\n",
      "          [-0.0542, -0.0181,    -inf]],\n",
      "\n",
      "         [[-0.0325, -0.3843,    -inf],\n",
      "          [-0.4920,  0.1875,    -inf],\n",
      "          [ 0.8832,  0.2878,    -inf]],\n",
      "\n",
      "         [[ 0.0876, -0.3863,    -inf],\n",
      "          [-0.4287, -0.2669,    -inf],\n",
      "          [ 0.2264,  0.3886,    -inf]],\n",
      "\n",
      "         [[ 0.1132, -0.2444,    -inf],\n",
      "          [-0.3277, -0.2229,    -inf],\n",
      "          [-0.4868, -0.4462,    -inf]],\n",
      "\n",
      "         [[-0.0130,  0.1154,    -inf],\n",
      "          [-0.4036, -0.1775,    -inf],\n",
      "          [ 0.6601,  1.1752,    -inf]],\n",
      "\n",
      "         [[-0.4611,  0.2834,    -inf],\n",
      "          [ 0.1210,  0.5280,    -inf],\n",
      "          [ 0.0649,  0.3428,    -inf]],\n",
      "\n",
      "         [[ 0.3559,  0.4656,    -inf],\n",
      "          [-0.0980, -0.1574,    -inf],\n",
      "          [-0.2788, -0.6230,    -inf]],\n",
      "\n",
      "         [[ 0.4017,  0.3026,    -inf],\n",
      "          [ 0.6014,  0.5934,    -inf],\n",
      "          [ 0.2021,  0.2285,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3213,    -inf,    -inf],\n",
      "          [ 0.1066,    -inf,    -inf],\n",
      "          [ 0.1669,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2250,    -inf,    -inf],\n",
      "          [ 0.4046,    -inf,    -inf],\n",
      "          [-0.1100,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2174,    -inf,    -inf],\n",
      "          [-0.7366,    -inf,    -inf],\n",
      "          [-0.3382,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.6385,    -inf,    -inf],\n",
      "          [ 0.1259,    -inf,    -inf],\n",
      "          [-0.3634,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1691,    -inf,    -inf],\n",
      "          [ 0.3799,    -inf,    -inf],\n",
      "          [ 0.1301,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2881,    -inf,    -inf],\n",
      "          [-0.1105,    -inf,    -inf],\n",
      "          [-0.2105,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3515,    -inf,    -inf],\n",
      "          [ 0.2846,    -inf,    -inf],\n",
      "          [ 0.0050,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2127,    -inf,    -inf],\n",
      "          [ 0.2122,    -inf,    -inf],\n",
      "          [ 0.1983,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4201, 0.5799, 0.0000],\n",
      "          [0.4316, 0.5684, 0.0000],\n",
      "          [0.4910, 0.5090, 0.0000]],\n",
      "\n",
      "         [[0.5871, 0.4129, 0.0000],\n",
      "          [0.3364, 0.6636, 0.0000],\n",
      "          [0.6446, 0.3554, 0.0000]],\n",
      "\n",
      "         [[0.6163, 0.3837, 0.0000],\n",
      "          [0.4596, 0.5404, 0.0000],\n",
      "          [0.4595, 0.5405, 0.0000]],\n",
      "\n",
      "         [[0.5885, 0.4115, 0.0000],\n",
      "          [0.4738, 0.5262, 0.0000],\n",
      "          [0.4898, 0.5102, 0.0000]],\n",
      "\n",
      "         [[0.4679, 0.5321, 0.0000],\n",
      "          [0.4437, 0.5563, 0.0000],\n",
      "          [0.3740, 0.6260, 0.0000]],\n",
      "\n",
      "         [[0.3220, 0.6780, 0.0000],\n",
      "          [0.3996, 0.6004, 0.0000],\n",
      "          [0.4310, 0.5690, 0.0000]],\n",
      "\n",
      "         [[0.4726, 0.5274, 0.0000],\n",
      "          [0.5148, 0.4852, 0.0000],\n",
      "          [0.5852, 0.4148, 0.0000]],\n",
      "\n",
      "         [[0.5247, 0.4753, 0.0000],\n",
      "          [0.5020, 0.4980, 0.0000],\n",
      "          [0.4934, 0.5066, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1053, -0.0799,    -inf],\n",
      "          [ 0.3346, -0.2916,    -inf],\n",
      "          [-0.6528, -0.6452,    -inf]],\n",
      "\n",
      "         [[-0.5750, -0.2713,    -inf],\n",
      "          [-0.0026, -0.1679,    -inf],\n",
      "          [-0.0326, -0.0944,    -inf]],\n",
      "\n",
      "         [[-0.0056,  0.1939,    -inf],\n",
      "          [ 0.2891,  0.1542,    -inf],\n",
      "          [ 0.3084, -0.3696,    -inf]],\n",
      "\n",
      "         [[ 1.0263,  0.3496,    -inf],\n",
      "          [ 0.6615,  0.1341,    -inf],\n",
      "          [ 0.5141, -0.0689,    -inf]],\n",
      "\n",
      "         [[ 0.0520, -0.1429,    -inf],\n",
      "          [ 0.2392,  0.3727,    -inf],\n",
      "          [ 0.2121,  0.1550,    -inf]],\n",
      "\n",
      "         [[-0.0871,  0.0705,    -inf],\n",
      "          [ 0.1443,  0.1498,    -inf],\n",
      "          [-0.2140, -0.0104,    -inf]],\n",
      "\n",
      "         [[ 0.0662,  0.5001,    -inf],\n",
      "          [ 0.7545,  0.8974,    -inf],\n",
      "          [-0.2247, -0.4810,    -inf]],\n",
      "\n",
      "         [[ 0.2303, -0.6919,    -inf],\n",
      "          [-0.0179,  0.1535,    -inf],\n",
      "          [ 0.4409, -0.2707,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0391,    -inf,    -inf],\n",
      "          [-0.2732,    -inf,    -inf],\n",
      "          [-0.2411,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0254,    -inf,    -inf],\n",
      "          [-0.1887,    -inf,    -inf],\n",
      "          [-0.4559,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4623,    -inf,    -inf],\n",
      "          [-0.0556,    -inf,    -inf],\n",
      "          [-0.4324,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4437,    -inf,    -inf],\n",
      "          [ 0.0401,    -inf,    -inf],\n",
      "          [-0.0744,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6222,    -inf,    -inf],\n",
      "          [-0.2189,    -inf,    -inf],\n",
      "          [ 0.0298,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1463,    -inf,    -inf],\n",
      "          [ 0.0306,    -inf,    -inf],\n",
      "          [-0.2706,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6358,    -inf,    -inf],\n",
      "          [ 0.4615,    -inf,    -inf],\n",
      "          [ 0.5673,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0273,    -inf,    -inf],\n",
      "          [-0.3517,    -inf,    -inf],\n",
      "          [ 0.2364,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5462, 0.4538, 0.0000],\n",
      "          [0.6516, 0.3484, 0.0000],\n",
      "          [0.4981, 0.5019, 0.0000]],\n",
      "\n",
      "         [[0.4247, 0.5753, 0.0000],\n",
      "          [0.5412, 0.4588, 0.0000],\n",
      "          [0.5155, 0.4845, 0.0000]],\n",
      "\n",
      "         [[0.4503, 0.5497, 0.0000],\n",
      "          [0.5337, 0.4663, 0.0000],\n",
      "          [0.6633, 0.3367, 0.0000]],\n",
      "\n",
      "         [[0.6630, 0.3370, 0.0000],\n",
      "          [0.6289, 0.3711, 0.0000],\n",
      "          [0.6418, 0.3582, 0.0000]],\n",
      "\n",
      "         [[0.5486, 0.4514, 0.0000],\n",
      "          [0.4667, 0.5333, 0.0000],\n",
      "          [0.5143, 0.4857, 0.0000]],\n",
      "\n",
      "         [[0.4607, 0.5393, 0.0000],\n",
      "          [0.4986, 0.5014, 0.0000],\n",
      "          [0.4493, 0.5507, 0.0000]],\n",
      "\n",
      "         [[0.3932, 0.6068, 0.0000],\n",
      "          [0.4643, 0.5357, 0.0000],\n",
      "          [0.5637, 0.4363, 0.0000]],\n",
      "\n",
      "         [[0.7155, 0.2845, 0.0000],\n",
      "          [0.4573, 0.5427, 0.0000],\n",
      "          [0.6707, 0.3293, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0209,  0.4248,    -inf],\n",
      "          [-0.2960, -0.0187,    -inf],\n",
      "          [ 0.1129,  0.3315,    -inf]],\n",
      "\n",
      "         [[ 0.0312,  0.1911,    -inf],\n",
      "          [-0.0279, -0.1837,    -inf],\n",
      "          [-0.1479, -0.2697,    -inf]],\n",
      "\n",
      "         [[ 0.0413,  0.2755,    -inf],\n",
      "          [ 0.0568, -0.0697,    -inf],\n",
      "          [ 0.4871,  0.3305,    -inf]],\n",
      "\n",
      "         [[-0.3789, -0.0474,    -inf],\n",
      "          [-0.0074, -0.4489,    -inf],\n",
      "          [-0.2202, -0.5139,    -inf]],\n",
      "\n",
      "         [[-0.2448, -0.0862,    -inf],\n",
      "          [ 0.0785, -0.4818,    -inf],\n",
      "          [ 0.3341, -0.0557,    -inf]],\n",
      "\n",
      "         [[ 0.0563,  0.3712,    -inf],\n",
      "          [-0.2368,  0.1812,    -inf],\n",
      "          [-0.4099, -0.0028,    -inf]],\n",
      "\n",
      "         [[-0.2662, -0.4641,    -inf],\n",
      "          [-0.3043,  0.1019,    -inf],\n",
      "          [ 0.1484,  0.0590,    -inf]],\n",
      "\n",
      "         [[ 0.0970,  0.1394,    -inf],\n",
      "          [ 0.0936,  0.2794,    -inf],\n",
      "          [ 0.0546,  0.3135,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3408,    -inf,    -inf],\n",
      "          [ 0.6714,    -inf,    -inf],\n",
      "          [-0.1972,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0804,    -inf,    -inf],\n",
      "          [ 0.1883,    -inf,    -inf],\n",
      "          [ 0.0905,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2009,    -inf,    -inf],\n",
      "          [-0.3413,    -inf,    -inf],\n",
      "          [-0.1242,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5912,    -inf,    -inf],\n",
      "          [ 0.2384,    -inf,    -inf],\n",
      "          [ 0.2356,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0356,    -inf,    -inf],\n",
      "          [ 0.3092,    -inf,    -inf],\n",
      "          [ 0.1569,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4578,    -inf,    -inf],\n",
      "          [ 0.4409,    -inf,    -inf],\n",
      "          [ 0.6487,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5405,    -inf,    -inf],\n",
      "          [ 0.5511,    -inf,    -inf],\n",
      "          [-0.1203,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0512,    -inf,    -inf],\n",
      "          [ 0.4087,    -inf,    -inf],\n",
      "          [ 0.2863,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3904, 0.6096, 0.0000],\n",
      "          [0.4311, 0.5689, 0.0000],\n",
      "          [0.4455, 0.5545, 0.0000]],\n",
      "\n",
      "         [[0.4601, 0.5399, 0.0000],\n",
      "          [0.5389, 0.4611, 0.0000],\n",
      "          [0.5304, 0.4696, 0.0000]],\n",
      "\n",
      "         [[0.4417, 0.5583, 0.0000],\n",
      "          [0.5316, 0.4684, 0.0000],\n",
      "          [0.5391, 0.4609, 0.0000]],\n",
      "\n",
      "         [[0.4179, 0.5821, 0.0000],\n",
      "          [0.6086, 0.3914, 0.0000],\n",
      "          [0.5729, 0.4271, 0.0000]],\n",
      "\n",
      "         [[0.4604, 0.5396, 0.0000],\n",
      "          [0.6365, 0.3635, 0.0000],\n",
      "          [0.5962, 0.4038, 0.0000]],\n",
      "\n",
      "         [[0.4219, 0.5781, 0.0000],\n",
      "          [0.3970, 0.6030, 0.0000],\n",
      "          [0.3996, 0.6004, 0.0000]],\n",
      "\n",
      "         [[0.5493, 0.4507, 0.0000],\n",
      "          [0.3998, 0.6002, 0.0000],\n",
      "          [0.5223, 0.4777, 0.0000]],\n",
      "\n",
      "         [[0.4894, 0.5106, 0.0000],\n",
      "          [0.4537, 0.5463, 0.0000],\n",
      "          [0.4356, 0.5644, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0444, -0.0720,    -inf],\n",
      "          [-0.0617, -0.1318,    -inf],\n",
      "          [ 0.2621,  0.4534,    -inf]],\n",
      "\n",
      "         [[-0.2281,  0.4050,    -inf],\n",
      "          [ 0.1123,  0.3365,    -inf],\n",
      "          [ 0.0679,  0.4361,    -inf]],\n",
      "\n",
      "         [[ 0.6262, -0.1610,    -inf],\n",
      "          [-0.4130,  0.0850,    -inf],\n",
      "          [-0.0184, -0.1601,    -inf]],\n",
      "\n",
      "         [[ 0.6134,  0.0541,    -inf],\n",
      "          [ 0.1218, -0.3213,    -inf],\n",
      "          [-0.4038, -0.4278,    -inf]],\n",
      "\n",
      "         [[-0.7590, -0.6799,    -inf],\n",
      "          [ 0.0995, -0.3729,    -inf],\n",
      "          [-0.0632,  0.2346,    -inf]],\n",
      "\n",
      "         [[ 0.4980,  0.2306,    -inf],\n",
      "          [ 0.2408,  0.1849,    -inf],\n",
      "          [ 0.6713,  0.8196,    -inf]],\n",
      "\n",
      "         [[ 0.3793,  0.2313,    -inf],\n",
      "          [ 0.3441,  0.1685,    -inf],\n",
      "          [ 1.0274,  0.3991,    -inf]],\n",
      "\n",
      "         [[-0.2869, -0.0978,    -inf],\n",
      "          [-0.1965, -0.2723,    -inf],\n",
      "          [-0.3097,  0.3548,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3797,    -inf,    -inf],\n",
      "          [ 0.5153,    -inf,    -inf],\n",
      "          [ 0.3891,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.8258,    -inf,    -inf],\n",
      "          [-0.2079,    -inf,    -inf],\n",
      "          [-0.4291,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3914,    -inf,    -inf],\n",
      "          [-0.4338,    -inf,    -inf],\n",
      "          [-0.3101,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5151,    -inf,    -inf],\n",
      "          [ 0.5426,    -inf,    -inf],\n",
      "          [ 0.6629,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1858,    -inf,    -inf],\n",
      "          [ 0.0725,    -inf,    -inf],\n",
      "          [-0.4225,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1414,    -inf,    -inf],\n",
      "          [-0.1535,    -inf,    -inf],\n",
      "          [-0.0730,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0183,    -inf,    -inf],\n",
      "          [ 0.4135,    -inf,    -inf],\n",
      "          [ 0.4438,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4244,    -inf,    -inf],\n",
      "          [ 0.4400,    -inf,    -inf],\n",
      "          [ 0.4933,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5069, 0.4931, 0.0000],\n",
      "          [0.5175, 0.4825, 0.0000],\n",
      "          [0.4523, 0.5477, 0.0000]],\n",
      "\n",
      "         [[0.3468, 0.6532, 0.0000],\n",
      "          [0.4442, 0.5558, 0.0000],\n",
      "          [0.4090, 0.5910, 0.0000]],\n",
      "\n",
      "         [[0.6872, 0.3128, 0.0000],\n",
      "          [0.3780, 0.6220, 0.0000],\n",
      "          [0.5354, 0.4646, 0.0000]],\n",
      "\n",
      "         [[0.6363, 0.3637, 0.0000],\n",
      "          [0.6090, 0.3910, 0.0000],\n",
      "          [0.5060, 0.4940, 0.0000]],\n",
      "\n",
      "         [[0.4802, 0.5198, 0.0000],\n",
      "          [0.6160, 0.3840, 0.0000],\n",
      "          [0.4261, 0.5739, 0.0000]],\n",
      "\n",
      "         [[0.5664, 0.4336, 0.0000],\n",
      "          [0.5140, 0.4860, 0.0000],\n",
      "          [0.4630, 0.5370, 0.0000]],\n",
      "\n",
      "         [[0.5369, 0.4631, 0.0000],\n",
      "          [0.5438, 0.4562, 0.0000],\n",
      "          [0.6521, 0.3479, 0.0000]],\n",
      "\n",
      "         [[0.4529, 0.5471, 0.0000],\n",
      "          [0.5189, 0.4811, 0.0000],\n",
      "          [0.3397, 0.6603, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0809, -0.0834,    -inf],\n",
      "          [ 0.0957,  0.0011,    -inf],\n",
      "          [-0.0768, -0.2786,    -inf]],\n",
      "\n",
      "         [[-0.2599, -0.2602,    -inf],\n",
      "          [ 0.1712,  0.1499,    -inf],\n",
      "          [-0.2772,  0.2661,    -inf]],\n",
      "\n",
      "         [[-0.4553, -0.3622,    -inf],\n",
      "          [-0.5300, -0.3371,    -inf],\n",
      "          [-0.5991, -0.3182,    -inf]],\n",
      "\n",
      "         [[-0.2931, -0.5774,    -inf],\n",
      "          [-0.1173, -0.6395,    -inf],\n",
      "          [ 0.2052,  0.3711,    -inf]],\n",
      "\n",
      "         [[-0.3983, -0.2138,    -inf],\n",
      "          [ 0.3889,  0.3059,    -inf],\n",
      "          [ 0.1058, -0.0901,    -inf]],\n",
      "\n",
      "         [[-0.0682, -0.1318,    -inf],\n",
      "          [-0.0691,  0.0052,    -inf],\n",
      "          [ 0.0834, -0.0219,    -inf]],\n",
      "\n",
      "         [[-0.0719, -0.1495,    -inf],\n",
      "          [ 0.0102, -0.0125,    -inf],\n",
      "          [-0.3610, -0.0151,    -inf]],\n",
      "\n",
      "         [[ 0.2155,  0.0574,    -inf],\n",
      "          [ 0.2530, -0.2026,    -inf],\n",
      "          [ 0.2647, -0.0490,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3284,    -inf,    -inf],\n",
      "          [-0.1144,    -inf,    -inf],\n",
      "          [-0.3783,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1000,    -inf,    -inf],\n",
      "          [ 0.5244,    -inf,    -inf],\n",
      "          [ 0.1896,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5504,    -inf,    -inf],\n",
      "          [-0.3713,    -inf,    -inf],\n",
      "          [-0.3106,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3931,    -inf,    -inf],\n",
      "          [ 0.0011,    -inf,    -inf],\n",
      "          [-0.2974,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5891,    -inf,    -inf],\n",
      "          [-0.4601,    -inf,    -inf],\n",
      "          [-0.1222,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1218,    -inf,    -inf],\n",
      "          [-0.1443,    -inf,    -inf],\n",
      "          [-0.2567,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4610,    -inf,    -inf],\n",
      "          [-0.3628,    -inf,    -inf],\n",
      "          [-0.2198,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.7931,    -inf,    -inf],\n",
      "          [ 0.1528,    -inf,    -inf],\n",
      "          [ 0.2114,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5410, 0.4590, 0.0000],\n",
      "          [0.5236, 0.4764, 0.0000],\n",
      "          [0.5503, 0.4497, 0.0000]],\n",
      "\n",
      "         [[0.5001, 0.4999, 0.0000],\n",
      "          [0.5053, 0.4947, 0.0000],\n",
      "          [0.3674, 0.6326, 0.0000]],\n",
      "\n",
      "         [[0.4768, 0.5232, 0.0000],\n",
      "          [0.4519, 0.5481, 0.0000],\n",
      "          [0.4302, 0.5698, 0.0000]],\n",
      "\n",
      "         [[0.5706, 0.4294, 0.0000],\n",
      "          [0.6277, 0.3723, 0.0000],\n",
      "          [0.4586, 0.5414, 0.0000]],\n",
      "\n",
      "         [[0.4540, 0.5460, 0.0000],\n",
      "          [0.5207, 0.4793, 0.0000],\n",
      "          [0.5488, 0.4512, 0.0000]],\n",
      "\n",
      "         [[0.5159, 0.4841, 0.0000],\n",
      "          [0.4814, 0.5186, 0.0000],\n",
      "          [0.5263, 0.4737, 0.0000]],\n",
      "\n",
      "         [[0.5194, 0.4806, 0.0000],\n",
      "          [0.5057, 0.4943, 0.0000],\n",
      "          [0.4144, 0.5856, 0.0000]],\n",
      "\n",
      "         [[0.5394, 0.4606, 0.0000],\n",
      "          [0.6120, 0.3880, 0.0000],\n",
      "          [0.5778, 0.4222, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x, self_mask = None):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask=self_mask)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "encoder(x, self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 3.1532e-02,        -inf,        -inf],\n",
      "          [-3.7478e-02,        -inf,        -inf],\n",
      "          [ 8.2237e-05,        -inf,        -inf]],\n",
      "\n",
      "         [[-1.3564e-01,        -inf,        -inf],\n",
      "          [-2.4686e-01,        -inf,        -inf],\n",
      "          [-1.1991e-01,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6749e-02,        -inf,        -inf],\n",
      "          [-1.3989e-01,        -inf,        -inf],\n",
      "          [-1.5754e-02,        -inf,        -inf]],\n",
      "\n",
      "         [[-1.4997e-01,        -inf,        -inf],\n",
      "          [-2.1577e-01,        -inf,        -inf],\n",
      "          [-1.8619e-01,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.5190e-03, -2.1841e-02,        -inf],\n",
      "          [ 2.8389e-02,  8.5901e-02,        -inf],\n",
      "          [-1.9216e-02,  9.8823e-03,        -inf]],\n",
      "\n",
      "         [[-1.2005e-01, -1.4576e-01,        -inf],\n",
      "          [-9.1510e-02, -1.3961e-01,        -inf],\n",
      "          [-1.1285e-01, -1.5842e-01,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5051, 0.4949, 0.0000],\n",
      "          [0.4856, 0.5144, 0.0000],\n",
      "          [0.4927, 0.5073, 0.0000]],\n",
      "\n",
      "         [[0.5064, 0.4936, 0.0000],\n",
      "          [0.5120, 0.4880, 0.0000],\n",
      "          [0.5114, 0.4886, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0047, -0.1408, -0.0781],\n",
      "          [ 0.0917, -0.0700, -0.0564],\n",
      "          [-0.0802, -0.1593, -0.2407]],\n",
      "\n",
      "         [[ 0.1372,  0.1482,  0.1205],\n",
      "          [ 0.0290, -0.0372, -0.0877],\n",
      "          [ 0.2094,  0.0413,  0.1015]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0569,    -inf,    -inf],\n",
      "          [-0.2541,    -inf,    -inf],\n",
      "          [-0.0969,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0852,    -inf,    -inf],\n",
      "          [-0.0641,    -inf,    -inf],\n",
      "          [-0.1964,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0331, -0.0794,    -inf],\n",
      "          [-0.0822, -0.0169,    -inf],\n",
      "          [ 0.1368,  0.1231,    -inf]],\n",
      "\n",
      "         [[ 0.3440,  0.1607,    -inf],\n",
      "          [-0.0279,  0.1663,    -inf],\n",
      "          [ 0.3287,  0.2651,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3590, 0.3104, 0.3305],\n",
      "          [0.3686, 0.3136, 0.3178],\n",
      "          [0.3603, 0.3329, 0.3069]],\n",
      "\n",
      "         [[0.3339, 0.3377, 0.3284],\n",
      "          [0.3539, 0.3312, 0.3149],\n",
      "          [0.3646, 0.3082, 0.3273]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5116, 0.4884, 0.0000],\n",
      "          [0.4837, 0.5163, 0.0000],\n",
      "          [0.5034, 0.4966, 0.0000]],\n",
      "\n",
      "         [[0.5457, 0.4543, 0.0000],\n",
      "          [0.4516, 0.5484, 0.0000],\n",
      "          [0.5159, 0.4841, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = dec_x.size()\n",
    "        x = dec_x + self.attn_dropout(self.mhsa(dec_x, dec_x, dec_x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, enc_x, enc_x, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0415,    -inf,    -inf],\n",
      "          [ 0.0093,    -inf,    -inf],\n",
      "          [ 0.0646,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0149,    -inf,    -inf],\n",
      "          [ 0.4591,    -inf,    -inf],\n",
      "          [ 0.1039,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0671,    -inf,    -inf],\n",
      "          [-0.1172,    -inf,    -inf],\n",
      "          [-0.1062,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4025,    -inf,    -inf],\n",
      "          [ 0.1725,    -inf,    -inf],\n",
      "          [-0.0420,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3909, -1.0185,    -inf],\n",
      "          [ 0.0167,  0.1806,    -inf],\n",
      "          [-0.1434,  0.1272,    -inf]],\n",
      "\n",
      "         [[ 0.3901, -0.2190,    -inf],\n",
      "          [ 0.2759, -0.5510,    -inf],\n",
      "          [ 0.2039, -0.2474,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6519, 0.3481, 0.0000],\n",
      "          [0.4591, 0.5409, 0.0000],\n",
      "          [0.4327, 0.5673, 0.0000]],\n",
      "\n",
      "         [[0.6477, 0.3523, 0.0000],\n",
      "          [0.6957, 0.3043, 0.0000],\n",
      "          [0.6109, 0.3891, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1393,  0.1681,  0.3583],\n",
      "          [ 0.2900,  0.3778,  0.4204],\n",
      "          [ 0.1423,  0.1518,  0.1219]],\n",
      "\n",
      "         [[-0.2395, -0.5574, -0.4483],\n",
      "          [ 0.1783,  0.1859,  0.1744],\n",
      "          [-0.1039, -0.3050, -0.2056]]],\n",
      "\n",
      "\n",
      "        [[[-0.2161,    -inf,    -inf],\n",
      "          [ 0.1900,    -inf,    -inf],\n",
      "          [ 0.1570,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4109,    -inf,    -inf],\n",
      "          [-0.2270,    -inf,    -inf],\n",
      "          [-0.1700,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2030,  0.1510,    -inf],\n",
      "          [ 0.3623,  0.2970,    -inf],\n",
      "          [ 0.2770,  0.1708,    -inf]],\n",
      "\n",
      "         [[-0.0749, -0.2293,    -inf],\n",
      "          [ 0.0908,  0.0842,    -inf],\n",
      "          [ 0.0665,  0.0069,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3054, 0.3144, 0.3802],\n",
      "          [0.3095, 0.3379, 0.3526],\n",
      "          [0.3345, 0.3377, 0.3278]],\n",
      "\n",
      "         [[0.3938, 0.2866, 0.3196],\n",
      "          [0.3329, 0.3355, 0.3316],\n",
      "          [0.3675, 0.3006, 0.3320]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5130, 0.4870, 0.0000],\n",
      "          [0.5163, 0.4837, 0.0000],\n",
      "          [0.5265, 0.4735, 0.0000]],\n",
      "\n",
      "         [[0.5385, 0.4615, 0.0000],\n",
      "          [0.5017, 0.4983, 0.0000],\n",
      "          [0.5149, 0.4851, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1117,    -inf,    -inf],\n",
      "          [-0.1472,    -inf,    -inf],\n",
      "          [-0.1124,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1908,    -inf,    -inf],\n",
      "          [-0.1371,    -inf,    -inf],\n",
      "          [-0.2489,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.4340,    -inf,    -inf],\n",
      "          [-0.0487,    -inf,    -inf],\n",
      "          [ 0.0708,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2198,    -inf,    -inf],\n",
      "          [-0.0472,    -inf,    -inf],\n",
      "          [-0.0291,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0710,  0.0878,    -inf],\n",
      "          [ 0.3354,  0.0779,    -inf],\n",
      "          [-0.2343, -0.0570,    -inf]],\n",
      "\n",
      "         [[-0.3868, -0.0230,    -inf],\n",
      "          [-0.3298, -0.4490,    -inf],\n",
      "          [ 0.2259, -0.2259,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4958, 0.5042, 0.0000],\n",
      "          [0.5640, 0.4360, 0.0000],\n",
      "          [0.4558, 0.5442, 0.0000]],\n",
      "\n",
      "         [[0.4100, 0.5900, 0.0000],\n",
      "          [0.5298, 0.4702, 0.0000],\n",
      "          [0.6111, 0.3889, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1054, -0.0264, -0.0886],\n",
      "          [-0.0637, -0.0031,  0.1276],\n",
      "          [-0.1108, -0.0038,  0.0427]],\n",
      "\n",
      "         [[ 0.4658,  0.3773,  0.2392],\n",
      "          [-0.2176, -0.3799, -0.2160],\n",
      "          [ 0.0510, -0.0114,  0.0386]]],\n",
      "\n",
      "\n",
      "        [[[-0.0102,    -inf,    -inf],\n",
      "          [-0.1353,    -inf,    -inf],\n",
      "          [-0.0530,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2939,    -inf,    -inf],\n",
      "          [-0.0926,    -inf,    -inf],\n",
      "          [ 0.2468,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0942,  0.0689,    -inf],\n",
      "          [-0.1477,  0.1507,    -inf],\n",
      "          [-0.1118,  0.1678,    -inf]],\n",
      "\n",
      "         [[-0.2005, -0.1207,    -inf],\n",
      "          [ 0.2880,  0.4686,    -inf],\n",
      "          [ 0.1427,  0.2542,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3227, 0.3492, 0.3281],\n",
      "          [0.3055, 0.3246, 0.3699],\n",
      "          [0.3050, 0.3394, 0.3556]],\n",
      "\n",
      "         [[0.3686, 0.3374, 0.2939],\n",
      "          [0.3507, 0.2981, 0.3512],\n",
      "          [0.3416, 0.3210, 0.3374]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4593, 0.5407, 0.0000],\n",
      "          [0.4260, 0.5740, 0.0000],\n",
      "          [0.4306, 0.5694, 0.0000]],\n",
      "\n",
      "         [[0.4801, 0.5199, 0.0000],\n",
      "          [0.4550, 0.5450, 0.0000],\n",
      "          [0.4722, 0.5278, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [DecoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        b, t = dec_x.size()\n",
    "        x = self.embed(dec_x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_x, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "    def get_embed_weights(self):\n",
    "        return self.embed.weight\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b5aba42-4209-4fbb-8cc9-a9e8a0236e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Output(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, d: int = 512, ff_weight = None):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(d, vocab_size)\n",
    "        # weight tying with the decoder embedding\n",
    "        if ff_weight is not None:\n",
    "            self.ff.weight = ff_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2477aec-7287-498b-bf31-cbb0bdfe154d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: \n",
      " tensor([[1, 1, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 1, 1, 0]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.3601,  0.4424, -0.1869],\n",
      "          [ 0.2509,  0.6753, -0.0283],\n",
      "          [-0.1065,  0.0561,  0.1603]],\n",
      "\n",
      "         [[ 0.1025,  0.0941,  0.0812],\n",
      "          [-0.2064, -0.4366, -0.5385],\n",
      "          [-1.0990,  0.1992, -0.3857]]],\n",
      "\n",
      "\n",
      "        [[[-0.7150, -0.4924,    -inf],\n",
      "          [-0.7683, -0.5302,    -inf],\n",
      "          [ 0.3388,  0.4855,    -inf]],\n",
      "\n",
      "         [[-0.5713, -0.5045,    -inf],\n",
      "          [-0.6221, -0.6162,    -inf],\n",
      "          [-0.6058, -0.8342,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.9824,    -inf,    -inf],\n",
      "          [ 0.0867,    -inf,    -inf],\n",
      "          [-0.1841,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.7236,    -inf,    -inf],\n",
      "          [-0.2878,    -inf,    -inf],\n",
      "          [-0.2302,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3753, 0.4075, 0.2172],\n",
      "          [0.3044, 0.4653, 0.2303],\n",
      "          [0.2872, 0.3379, 0.3750]],\n",
      "\n",
      "         [[0.3366, 0.3338, 0.3296],\n",
      "          [0.3981, 0.3163, 0.2856],\n",
      "          [0.1492, 0.5464, 0.3044]]],\n",
      "\n",
      "\n",
      "        [[[0.4446, 0.5554, 0.0000],\n",
      "          [0.4407, 0.5593, 0.0000],\n",
      "          [0.4634, 0.5366, 0.0000]],\n",
      "\n",
      "         [[0.4833, 0.5167, 0.0000],\n",
      "          [0.4985, 0.5015, 0.0000],\n",
      "          [0.5568, 0.4432, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1283, -0.3720, -0.1171],\n",
      "          [ 0.4376, -0.1176,  0.2812],\n",
      "          [ 0.4813, -0.0362,  0.0987]],\n",
      "\n",
      "         [[ 0.0618, -0.0763, -0.1337],\n",
      "          [ 0.0092,  0.2502,  0.2303],\n",
      "          [-0.0428, -0.4422, -0.2864]]],\n",
      "\n",
      "\n",
      "        [[[-0.3872, -0.4305,    -inf],\n",
      "          [-0.4335, -0.4879,    -inf],\n",
      "          [ 0.3230,  0.2548,    -inf]],\n",
      "\n",
      "         [[ 0.7111,  0.6193,    -inf],\n",
      "          [ 0.4484,  0.3856,    -inf],\n",
      "          [-0.8562, -0.7522,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2912,    -inf,    -inf],\n",
      "          [-0.8747,    -inf,    -inf],\n",
      "          [-0.1902,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3207,    -inf,    -inf],\n",
      "          [ 0.3851,    -inf,    -inf],\n",
      "          [-0.5900,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3578, 0.2804, 0.3618],\n",
      "          [0.4116, 0.2363, 0.3521],\n",
      "          [0.4390, 0.2616, 0.2994]],\n",
      "\n",
      "         [[0.3713, 0.3234, 0.3053],\n",
      "          [0.2841, 0.3615, 0.3544],\n",
      "          [0.4074, 0.2733, 0.3193]]],\n",
      "\n",
      "\n",
      "        [[[0.5108, 0.4892, 0.0000],\n",
      "          [0.5136, 0.4864, 0.0000],\n",
      "          [0.5171, 0.4829, 0.0000]],\n",
      "\n",
      "         [[0.5229, 0.4771, 0.0000],\n",
      "          [0.5157, 0.4843, 0.0000],\n",
      "          [0.4740, 0.5260, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2179,    -inf,    -inf,    -inf],\n",
      "          [ 0.0719,    -inf,    -inf,    -inf],\n",
      "          [ 0.3459,    -inf,    -inf,    -inf],\n",
      "          [-0.0421,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3542,    -inf,    -inf,    -inf],\n",
      "          [-0.4961,    -inf,    -inf,    -inf],\n",
      "          [-0.8240,    -inf,    -inf,    -inf],\n",
      "          [-0.5221,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1525,    -inf,    -inf,    -inf],\n",
      "          [ 0.0754,    -inf,    -inf,    -inf],\n",
      "          [ 0.2531,    -inf,    -inf,    -inf],\n",
      "          [-0.0319,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5494,    -inf,    -inf,    -inf],\n",
      "          [-0.1135,    -inf,    -inf,    -inf],\n",
      "          [-0.0130,    -inf,    -inf,    -inf],\n",
      "          [ 0.1516,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1105, -0.2813, -0.0286,    -inf],\n",
      "          [ 0.4770,  0.3847, -0.4405,    -inf],\n",
      "          [-0.0327,  0.0833,  0.0241,    -inf],\n",
      "          [ 0.0331, -0.1874,  0.0925,    -inf]],\n",
      "\n",
      "         [[ 0.6076, -0.2201,  0.3540,    -inf],\n",
      "          [ 0.1075,  0.1989, -0.4287,    -inf],\n",
      "          [ 0.2808, -0.1961,  0.3731,    -inf],\n",
      "          [-0.0734, -0.3862, -0.3505,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3928, 0.2655, 0.3418, 0.0000],\n",
      "          [0.4326, 0.3945, 0.1729, 0.0000],\n",
      "          [0.3143, 0.3530, 0.3327, 0.0000],\n",
      "          [0.3493, 0.2801, 0.3706, 0.0000]],\n",
      "\n",
      "         [[0.4519, 0.1975, 0.3507, 0.0000],\n",
      "          [0.3730, 0.4087, 0.2182, 0.0000],\n",
      "          [0.3680, 0.2284, 0.4036, 0.0000],\n",
      "          [0.4017, 0.2938, 0.3045, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 4, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.4122,  0.2990,  0.7601],\n",
      "          [-0.4083,  0.0141,  0.5523],\n",
      "          [ 0.0650, -0.0581,  0.1139],\n",
      "          [-0.0742,  0.0481,  0.1603]],\n",
      "\n",
      "         [[-0.1605, -0.4811, -0.4097],\n",
      "          [-0.2041,  0.0252, -0.2692],\n",
      "          [-0.6157,  0.2857, -0.1599],\n",
      "          [-0.4939,  0.2250, -0.3007]]],\n",
      "\n",
      "\n",
      "        [[[-0.3195, -0.3211,    -inf],\n",
      "          [ 0.1726, -0.0063,    -inf],\n",
      "          [ 0.2231,  0.1004,    -inf],\n",
      "          [ 0.2448,  0.0430,    -inf]],\n",
      "\n",
      "         [[-0.7718, -0.7834,    -inf],\n",
      "          [-0.1464, -0.0357,    -inf],\n",
      "          [-0.3792, -0.3121,    -inf],\n",
      "          [-0.4704, -0.3701,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2046,    -inf,    -inf],\n",
      "          [ 0.0654,    -inf,    -inf],\n",
      "          [-0.2353,    -inf,    -inf],\n",
      "          [ 0.0203,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2653,    -inf,    -inf],\n",
      "          [-0.3688,    -inf,    -inf],\n",
      "          [-0.1057,    -inf,    -inf],\n",
      "          [-0.5218,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1596, 0.3250, 0.5154],\n",
      "          [0.1946, 0.2969, 0.5085],\n",
      "          [0.3408, 0.3013, 0.3579],\n",
      "          [0.2946, 0.3329, 0.3725]],\n",
      "\n",
      "         [[0.3992, 0.2897, 0.3111],\n",
      "          [0.3130, 0.3937, 0.2933],\n",
      "          [0.1984, 0.4886, 0.3130],\n",
      "          [0.2344, 0.4811, 0.2844]]],\n",
      "\n",
      "\n",
      "        [[[0.5004, 0.4996, 0.0000],\n",
      "          [0.5446, 0.4554, 0.0000],\n",
      "          [0.5306, 0.4694, 0.0000],\n",
      "          [0.5503, 0.4497, 0.0000]],\n",
      "\n",
      "         [[0.5029, 0.4971, 0.0000],\n",
      "          [0.4724, 0.5276, 0.0000],\n",
      "          [0.4832, 0.5168, 0.0000],\n",
      "          [0.4749, 0.5251, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.8282,    -inf,    -inf,    -inf],\n",
      "          [-0.3183,    -inf,    -inf,    -inf],\n",
      "          [-0.9385,    -inf,    -inf,    -inf],\n",
      "          [-0.7554,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3174,    -inf,    -inf,    -inf],\n",
      "          [-0.1528,    -inf,    -inf,    -inf],\n",
      "          [-0.1296,    -inf,    -inf,    -inf],\n",
      "          [-0.2690,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1994,    -inf,    -inf,    -inf],\n",
      "          [-0.4375,    -inf,    -inf,    -inf],\n",
      "          [-0.3363,    -inf,    -inf,    -inf],\n",
      "          [-0.5035,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0074,    -inf,    -inf,    -inf],\n",
      "          [ 0.3492,    -inf,    -inf,    -inf],\n",
      "          [ 0.3381,    -inf,    -inf,    -inf],\n",
      "          [ 0.4067,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1497,  0.2003, -0.0794,    -inf],\n",
      "          [-0.0096,  0.3486,  0.4172,    -inf],\n",
      "          [-0.0226,  0.3743, -0.0428,    -inf],\n",
      "          [ 0.0905,  0.6355,  0.1288,    -inf]],\n",
      "\n",
      "         [[-0.1828, -0.1218,  0.0087,    -inf],\n",
      "          [-0.2956, -0.1494, -0.0304,    -inf],\n",
      "          [-0.0077, -0.2222,  0.2993,    -inf],\n",
      "          [-0.2836, -0.2108,  0.0205,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2864, 0.4064, 0.3072, 0.0000],\n",
      "          [0.2523, 0.3610, 0.3867, 0.0000],\n",
      "          [0.2884, 0.4289, 0.2826, 0.0000],\n",
      "          [0.2657, 0.4582, 0.2761, 0.0000]],\n",
      "\n",
      "         [[0.3054, 0.3247, 0.3699, 0.0000],\n",
      "          [0.2889, 0.3344, 0.3767, 0.0000],\n",
      "          [0.3158, 0.2549, 0.4293, 0.0000],\n",
      "          [0.2915, 0.3135, 0.3950, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 4, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.6343, -0.2279, -0.0593],\n",
      "          [-0.1945, -0.4563, -0.0773],\n",
      "          [-0.6140, -0.2496, -0.1950],\n",
      "          [-0.3007, -0.1333, -0.1429]],\n",
      "\n",
      "         [[ 0.5597,  0.1303,  0.4988],\n",
      "          [ 0.0719,  0.2311,  0.4595],\n",
      "          [-0.1801, -0.2150,  0.2184],\n",
      "          [-0.1556,  0.3000,  0.8246]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1242,  0.0428,    -inf],\n",
      "          [-0.4401, -0.3693,    -inf],\n",
      "          [-0.6254, -0.5680,    -inf],\n",
      "          [-0.3551, -0.2866,    -inf]],\n",
      "\n",
      "         [[ 0.1157,  0.1028,    -inf],\n",
      "          [-0.4354, -0.3661,    -inf],\n",
      "          [-0.1598, -0.1396,    -inf],\n",
      "          [ 0.2394,  0.2525,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0602,    -inf,    -inf],\n",
      "          [-0.1914,    -inf,    -inf],\n",
      "          [-0.2043,    -inf,    -inf],\n",
      "          [-0.3371,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1816,    -inf,    -inf],\n",
      "          [ 0.2239,    -inf,    -inf],\n",
      "          [ 0.0671,    -inf,    -inf],\n",
      "          [ 0.4188,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2337, 0.3509, 0.4154],\n",
      "          [0.3455, 0.2659, 0.3885],\n",
      "          [0.2525, 0.3636, 0.3839],\n",
      "          [0.2982, 0.3526, 0.3492]],\n",
      "\n",
      "         [[0.3858, 0.2511, 0.3630],\n",
      "          [0.2743, 0.3216, 0.4041],\n",
      "          [0.2894, 0.2795, 0.4311],\n",
      "          [0.1908, 0.3009, 0.5084]]],\n",
      "\n",
      "\n",
      "        [[[0.5203, 0.4797, 0.0000],\n",
      "          [0.4823, 0.5177, 0.0000],\n",
      "          [0.4856, 0.5144, 0.0000],\n",
      "          [0.4829, 0.5171, 0.0000]],\n",
      "\n",
      "         [[0.5032, 0.4968, 0.0000],\n",
      "          [0.4827, 0.5173, 0.0000],\n",
      "          [0.4950, 0.5050, 0.0000],\n",
      "          [0.4967, 0.5033, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8, embed_tying=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        if embed_tying:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d, ff_weight = self.decoder.get_embed_weights())\n",
    "        else:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d)\n",
    "\n",
    "    def forward(self, enc_x, dec_x, enc_mask=None, dec_mask=None):\n",
    "        encoded = self.encoder(enc_x, enc_mask)\n",
    "        decoded = self.decoder(dec_x=dec_x, enc_x=encoded, self_mask=dec_mask, cross_mask=enc_mask)\n",
    "        return self.output(decoded)\n",
    "\n",
    "transformer = Transformer(vocab_size=32, n=2, d=16, h=2, embed_tying=False)\n",
    "enc_x = torch.tensor([[15, 7, 3], [10, 10, 0], [1, 0, 0]])\n",
    "dec_x = torch.tensor([[21, 8, 0, 0], [25, 0, 0, 0], [8, 1, 2, 3]])\n",
    "# dec_x = torch.tensor([[21, 8], [25, 0], [8, 1]])\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=0)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=0)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "print(transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f69bab-e9a5-44a7-af1d-f294f62d3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a06c43eb-c348-4905-9384-1b28378435b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9906,   4435, 100257, 100257, 100257],\n",
      "        [  2028,    374,    264,   4382,  11914],\n",
      "        [  7979, 100257, 100257, 100257, 100257]])\n",
      "tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "enc_mask: \n",
      " tensor([[1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 0, 0, 0]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.7350, -0.3909,    -inf,    -inf,    -inf],\n",
      "          [-0.9860, -0.1054,    -inf,    -inf,    -inf],\n",
      "          [-1.1852,  0.2724,    -inf,    -inf,    -inf],\n",
      "          [-0.9579,  0.2561,    -inf,    -inf,    -inf],\n",
      "          [-1.0597, -0.1453,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2757, -0.3340,    -inf,    -inf,    -inf],\n",
      "          [ 0.5064, -0.7422,    -inf,    -inf,    -inf],\n",
      "          [ 0.2303,  0.1088,    -inf,    -inf,    -inf],\n",
      "          [ 0.4199,  0.0891,    -inf,    -inf,    -inf],\n",
      "          [ 0.0835,  0.2813,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.8605, -0.2785,    -inf,    -inf,    -inf],\n",
      "          [-0.0555, -0.5012,    -inf,    -inf,    -inf],\n",
      "          [-0.1236, -0.1671,    -inf,    -inf,    -inf],\n",
      "          [-0.3463, -0.4420,    -inf,    -inf,    -inf],\n",
      "          [-0.4610, -0.3677,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 1.0548, -0.2712,    -inf,    -inf,    -inf],\n",
      "          [ 1.7936, -0.0394,    -inf,    -inf,    -inf],\n",
      "          [ 0.5733, -0.1003,    -inf,    -inf,    -inf],\n",
      "          [ 0.5331, -0.4436,    -inf,    -inf,    -inf],\n",
      "          [ 0.3642, -0.3027,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.6160, -0.5043, -1.1641, -0.5857, -0.2701],\n",
      "          [-0.1335, -0.7867, -0.4945, -0.6627, -0.4282],\n",
      "          [-1.0382, -0.7973, -0.6612, -0.3300, -0.3679],\n",
      "          [-0.4749, -1.2001, -0.1890, -0.2138,  0.0362],\n",
      "          [-0.3279, -0.3678, -0.3734, -0.1224, -0.8328]],\n",
      "\n",
      "         [[-0.8538,  0.5330,  0.3510,  0.0508,  0.0410],\n",
      "          [-0.4415, -0.0190,  0.2540, -0.4642,  0.0807],\n",
      "          [-0.7984, -0.6657, -0.1496,  0.0865, -0.1540],\n",
      "          [ 0.1976,  0.1076,  0.4163, -0.1999,  0.5420],\n",
      "          [-0.5872, -0.0196,  0.8839,  0.7004, -0.4880]],\n",
      "\n",
      "         [[-0.9953, -0.2276,  0.9527, -0.4342,  0.1019],\n",
      "          [-0.7987, -0.5538,  0.3296,  0.0613,  0.1177],\n",
      "          [-0.3442, -0.1295, -0.6292, -0.0844, -1.2949],\n",
      "          [-0.3853, -0.1318, -0.6648, -0.2582, -0.8805],\n",
      "          [ 0.0702, -0.1252,  0.1164, -0.0140,  0.8357]],\n",
      "\n",
      "         [[ 0.6207, -0.1718, -0.5524, -0.0142, -0.6070],\n",
      "          [-0.2637,  0.5850,  0.1500,  0.4044, -0.5636],\n",
      "          [-0.1070, -0.6821, -0.4205,  1.5163, -0.3670],\n",
      "          [-0.8254, -0.6212, -0.3345,  0.0788, -0.5252],\n",
      "          [ 0.2319,  0.2101, -0.0068,  1.1990,  0.1470]]],\n",
      "\n",
      "\n",
      "        [[[-0.4638,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5733,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3556,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2899,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.4122,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1672,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1847,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2485,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2293,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1509,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5007,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.9141,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.8714,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.9677,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-1.2744,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1303,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5454,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5986,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.7062,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2694,    -inf,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4148, 0.5852, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2930, 0.7070, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1888, 0.8112, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2290, 0.7710, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2861, 0.7139, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.6479, 0.3521, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7771, 0.2229, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5303, 0.4697, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5820, 0.4180, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4507, 0.5493, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3585, 0.6415, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6096, 0.3904, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5109, 0.4891, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5239, 0.4761, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4767, 0.5233, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.7902, 0.2098, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8621, 0.1379, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6623, 0.3377, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7265, 0.2735, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6608, 0.3392, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1945, 0.2175, 0.1125, 0.2005, 0.2749],\n",
      "          [0.2816, 0.1465, 0.1963, 0.1659, 0.2097],\n",
      "          [0.1296, 0.1649, 0.1890, 0.2632, 0.2534],\n",
      "          [0.1730, 0.0838, 0.2302, 0.2246, 0.2884],\n",
      "          [0.2106, 0.2024, 0.2012, 0.2587, 0.1271]],\n",
      "\n",
      "         [[0.0754, 0.3019, 0.2517, 0.1864, 0.1846],\n",
      "          [0.1390, 0.2121, 0.2787, 0.1359, 0.2343],\n",
      "          [0.1193, 0.1362, 0.2282, 0.2890, 0.2272],\n",
      "          [0.1908, 0.1744, 0.2374, 0.1282, 0.2692],\n",
      "          [0.0844, 0.1489, 0.3676, 0.3059, 0.0932]],\n",
      "\n",
      "         [[0.0670, 0.1444, 0.4702, 0.1175, 0.2008],\n",
      "          [0.0977, 0.1249, 0.3021, 0.2310, 0.2444],\n",
      "          [0.2139, 0.2652, 0.1609, 0.2774, 0.0827],\n",
      "          [0.2088, 0.2690, 0.1579, 0.2371, 0.1272],\n",
      "          [0.1684, 0.1385, 0.1763, 0.1548, 0.3620]],\n",
      "\n",
      "         [[0.3868, 0.1751, 0.1197, 0.2050, 0.1133],\n",
      "          [0.1326, 0.3099, 0.2006, 0.2587, 0.0983],\n",
      "          [0.1229, 0.0692, 0.0899, 0.6233, 0.0948],\n",
      "          [0.1302, 0.1597, 0.2127, 0.3216, 0.1758],\n",
      "          [0.1583, 0.1549, 0.1247, 0.4165, 0.1455]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1302,  0.6404,    -inf,    -inf,    -inf],\n",
      "          [-0.2154, -0.5994,    -inf,    -inf,    -inf],\n",
      "          [-0.3210,  0.4200,    -inf,    -inf,    -inf],\n",
      "          [-0.3652,  0.2958,    -inf,    -inf,    -inf],\n",
      "          [-0.1177,  0.3406,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4871, -0.2726,    -inf,    -inf,    -inf],\n",
      "          [ 0.0608,  0.4007,    -inf,    -inf,    -inf],\n",
      "          [ 0.2079,  0.1139,    -inf,    -inf,    -inf],\n",
      "          [ 0.0547, -0.0137,    -inf,    -inf,    -inf],\n",
      "          [-0.1964,  0.1479,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1139,  0.2043,    -inf,    -inf,    -inf],\n",
      "          [ 0.1239,  0.1987,    -inf,    -inf,    -inf],\n",
      "          [ 0.0348,  0.5053,    -inf,    -inf,    -inf],\n",
      "          [ 0.0424,  0.4642,    -inf,    -inf,    -inf],\n",
      "          [-0.1070,  0.6022,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0887, -0.2322,    -inf,    -inf,    -inf],\n",
      "          [ 0.2025,  0.3780,    -inf,    -inf,    -inf],\n",
      "          [ 0.4241, -0.4321,    -inf,    -inf,    -inf],\n",
      "          [ 0.5691, -0.4186,    -inf,    -inf,    -inf],\n",
      "          [ 0.3472, -0.2575,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0522,  0.0923, -0.3141, -0.1615,  0.1005],\n",
      "          [ 0.1130,  0.5441,  0.0757, -0.4012,  0.1247],\n",
      "          [ 0.2504, -0.0562,  0.0972, -0.1042,  0.1514],\n",
      "          [ 0.4446,  0.4585, -0.2022, -0.5103, -0.0013],\n",
      "          [-0.2582,  0.0250, -0.2834, -0.3435, -0.1168]],\n",
      "\n",
      "         [[ 0.4064, -0.0846,  0.1212,  0.3731,  0.2894],\n",
      "          [ 0.0683,  0.2244,  0.0927,  0.0058, -0.1248],\n",
      "          [-0.1895,  0.3579,  0.4484, -0.2640,  0.0168],\n",
      "          [ 0.4674,  0.5901,  0.0117, -0.0118,  0.2437],\n",
      "          [ 0.3629,  0.5932,  0.1141, -0.1190,  0.3943]],\n",
      "\n",
      "         [[ 0.2483, -0.1403, -0.0227, -0.4212,  0.1176],\n",
      "          [-0.0738, -0.2472, -0.2366, -0.1028,  0.2045],\n",
      "          [ 0.1514, -0.0880,  0.1291, -0.0077, -0.0659],\n",
      "          [-0.0279, -0.0183,  0.1575,  0.1464,  0.2703],\n",
      "          [ 0.0252, -0.1219,  0.1466,  0.2716,  0.1200]],\n",
      "\n",
      "         [[ 0.1540,  0.0533, -0.0271,  0.0935, -0.1759],\n",
      "          [ 0.0041, -0.0030,  0.1437,  0.4401, -0.5497],\n",
      "          [ 0.3116,  0.0898, -0.3533, -0.1662, -0.1045],\n",
      "          [ 0.0270,  0.0303,  0.2404,  0.3965, -0.1438],\n",
      "          [ 0.2447,  0.3968,  0.2797, -0.1350,  0.5116]]],\n",
      "\n",
      "\n",
      "        [[[-0.5589,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2999,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1118,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1860,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3011,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1239,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0534,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0638,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2135,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2069,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2722,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3515,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3657,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3605,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3837,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2626,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5703,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3034,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5263,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3617,    -inf,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3163, 0.6837, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5948, 0.4052, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3228, 0.6772, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3405, 0.6595, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3874, 0.6126, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.6813, 0.3187, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4158, 0.5842, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5235, 0.4765, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5171, 0.4829, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4148, 0.5852, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4211, 0.5789, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4813, 0.5187, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3845, 0.6155, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3961, 0.6039, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3298, 0.6702, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5795, 0.4205, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4562, 0.5438, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7019, 0.2981, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7286, 0.2714, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6467, 0.3533, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2005, 0.2317, 0.1543, 0.1798, 0.2336],\n",
      "          [0.1956, 0.3010, 0.1885, 0.1170, 0.1979],\n",
      "          [0.2380, 0.1752, 0.2042, 0.1670, 0.2156],\n",
      "          [0.2807, 0.2846, 0.1470, 0.1080, 0.1797],\n",
      "          [0.1861, 0.2471, 0.1815, 0.1709, 0.2144]],\n",
      "\n",
      "         [[0.2369, 0.1450, 0.1781, 0.2292, 0.2108],\n",
      "          [0.2017, 0.2358, 0.2067, 0.1895, 0.1663],\n",
      "          [0.1475, 0.2550, 0.2792, 0.1369, 0.1813],\n",
      "          [0.2390, 0.2702, 0.1516, 0.1480, 0.1911],\n",
      "          [0.2133, 0.2686, 0.1663, 0.1317, 0.2201]],\n",
      "\n",
      "         [[0.2611, 0.1770, 0.1991, 0.1337, 0.2291],\n",
      "          [0.2007, 0.1687, 0.1705, 0.1950, 0.2651],\n",
      "          [0.2261, 0.1780, 0.2211, 0.1928, 0.1820],\n",
      "          [0.1739, 0.1756, 0.2093, 0.2070, 0.2343],\n",
      "          [0.1862, 0.1607, 0.2102, 0.2382, 0.2047]],\n",
      "\n",
      "         [[0.2273, 0.2056, 0.1897, 0.2140, 0.1635],\n",
      "          [0.1900, 0.1886, 0.2184, 0.2938, 0.1092],\n",
      "          [0.2781, 0.2228, 0.1431, 0.1725, 0.1835],\n",
      "          [0.1808, 0.1814, 0.2238, 0.2616, 0.1524],\n",
      "          [0.1927, 0.2243, 0.1995, 0.1318, 0.2516]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.3195,  0.2829,    -inf,    -inf,    -inf],\n",
      "          [ 0.5084,  0.3129,    -inf,    -inf,    -inf],\n",
      "          [ 0.4631,  0.0065,    -inf,    -inf,    -inf],\n",
      "          [ 0.3195,  0.0848,    -inf,    -inf,    -inf],\n",
      "          [ 0.2507,  0.1826,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1437,  0.9602,    -inf,    -inf,    -inf],\n",
      "          [-0.0246,  0.1041,    -inf,    -inf,    -inf],\n",
      "          [ 0.2677, -0.0261,    -inf,    -inf,    -inf],\n",
      "          [ 0.3153,  0.0698,    -inf,    -inf,    -inf],\n",
      "          [ 0.1940,  0.0400,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2244,  0.0944,    -inf,    -inf,    -inf],\n",
      "          [-0.0101,  0.5749,    -inf,    -inf,    -inf],\n",
      "          [ 0.1724,  0.1890,    -inf,    -inf,    -inf],\n",
      "          [-0.1799, -0.0649,    -inf,    -inf,    -inf],\n",
      "          [ 0.0780,  0.1766,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5421,  0.2834,    -inf,    -inf,    -inf],\n",
      "          [-0.2045,  0.5447,    -inf,    -inf,    -inf],\n",
      "          [-0.0816,  0.4620,    -inf,    -inf,    -inf],\n",
      "          [-0.1728,  0.1291,    -inf,    -inf,    -inf],\n",
      "          [ 0.0935,  0.3981,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1986, -0.4792, -0.1451,  0.6205, -0.3482],\n",
      "          [ 0.1285, -0.2116, -0.5737,  0.3703, -0.3874],\n",
      "          [ 0.6501,  0.4324,  0.6154, -0.1872,  0.3813],\n",
      "          [-0.0271,  0.2560, -0.3887,  0.1689, -0.1050],\n",
      "          [ 0.1762, -0.2120, -0.2214,  0.2818, -0.0732]],\n",
      "\n",
      "         [[-0.6216, -0.2408, -0.5380, -0.1891, -0.1663],\n",
      "          [ 0.0910, -0.0541,  0.4417,  0.3999,  0.4340],\n",
      "          [ 0.3922, -0.6551, -0.0501, -0.4352,  0.4695],\n",
      "          [-0.0265, -0.5240, -0.2870,  0.0010,  0.1414],\n",
      "          [ 0.1311, -0.4054,  0.1279, -0.3153,  0.5813]],\n",
      "\n",
      "         [[-0.3346,  0.2482, -0.3315,  0.1476, -0.7723],\n",
      "          [-0.4429, -0.1191, -0.3934, -0.2883, -0.5845],\n",
      "          [-0.7792, -0.4166, -0.4010, -0.3751, -0.5108],\n",
      "          [-0.5843,  0.1685, -0.4460, -0.2003, -0.0110],\n",
      "          [-0.3449, -0.4264, -0.2159, -0.3932,  0.0742]],\n",
      "\n",
      "         [[ 0.2964, -0.1621,  0.0977, -0.2318,  0.0145],\n",
      "          [ 0.7678,  0.0512,  0.3959, -0.3006, -0.2038],\n",
      "          [-0.1647, -0.2929,  0.0376, -0.6590, -0.3860],\n",
      "          [-0.0580,  0.1421,  0.2722, -0.3445, -0.2552],\n",
      "          [ 0.6629,  0.3202, -0.0294,  0.0978,  0.5237]]],\n",
      "\n",
      "\n",
      "        [[[-0.1664,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3143,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.4845,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3233,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.4399,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3850,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1895,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3644,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3685,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2076,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0039,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2113,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1512,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0789,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1178,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1177,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2382,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1875,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0461,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3838,    -inf,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5091, 0.4909, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5487, 0.4513, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6122, 0.3878, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5584, 0.4416, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5170, 0.4830, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3065, 0.6935, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4679, 0.5321, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5729, 0.4271, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5611, 0.4389, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5384, 0.4616, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4210, 0.5790, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3578, 0.6422, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4958, 0.5042, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4713, 0.5287, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4754, 0.5246, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3046, 0.6954, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3210, 0.6790, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3673, 0.6327, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4251, 0.5749, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4244, 0.5756, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1684, 0.1272, 0.1776, 0.3819, 0.1450],\n",
      "          [0.2452, 0.1745, 0.1215, 0.3123, 0.1464],\n",
      "          [0.2520, 0.2027, 0.2435, 0.1091, 0.1926],\n",
      "          [0.1936, 0.2569, 0.1349, 0.2355, 0.1791],\n",
      "          [0.2358, 0.1599, 0.1584, 0.2621, 0.1837]],\n",
      "\n",
      "         [[0.1500, 0.2195, 0.1630, 0.2311, 0.2364],\n",
      "          [0.1651, 0.1428, 0.2345, 0.2249, 0.2327],\n",
      "          [0.2848, 0.0999, 0.1830, 0.1245, 0.3077],\n",
      "          [0.2179, 0.1325, 0.1679, 0.2240, 0.2577],\n",
      "          [0.2088, 0.1221, 0.2081, 0.1336, 0.3275]],\n",
      "\n",
      "         [[0.1650, 0.2956, 0.1655, 0.2673, 0.1065],\n",
      "          [0.1829, 0.2528, 0.1922, 0.2134, 0.1587],\n",
      "          [0.1492, 0.2144, 0.2178, 0.2235, 0.1951],\n",
      "          [0.1331, 0.2825, 0.1528, 0.1954, 0.2361],\n",
      "          [0.1807, 0.1666, 0.2056, 0.1722, 0.2748]],\n",
      "\n",
      "         [[0.2634, 0.1666, 0.2160, 0.1553, 0.1987],\n",
      "          [0.3448, 0.1684, 0.2377, 0.1185, 0.1305],\n",
      "          [0.2215, 0.1948, 0.2711, 0.1351, 0.1775],\n",
      "          [0.1929, 0.2356, 0.2683, 0.1448, 0.1584],\n",
      "          [0.2741, 0.1946, 0.1372, 0.1558, 0.2385]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.9275,    -inf,    -inf,    -inf],\n",
      "          [ 0.2653,    -inf,    -inf,    -inf],\n",
      "          [ 0.3947,    -inf,    -inf,    -inf],\n",
      "          [ 0.3986,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5294,    -inf,    -inf,    -inf],\n",
      "          [ 0.2293,    -inf,    -inf,    -inf],\n",
      "          [ 0.3780,    -inf,    -inf,    -inf],\n",
      "          [ 0.0645,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2557,    -inf,    -inf,    -inf],\n",
      "          [-0.0926,    -inf,    -inf,    -inf],\n",
      "          [-0.2373,    -inf,    -inf,    -inf],\n",
      "          [-0.4989,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0598,    -inf,    -inf,    -inf],\n",
      "          [ 0.6075,    -inf,    -inf,    -inf],\n",
      "          [ 0.4207,    -inf,    -inf,    -inf],\n",
      "          [ 0.8603,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0312, -0.3935,    -inf,    -inf],\n",
      "          [-0.8300,  0.1062,    -inf,    -inf],\n",
      "          [-0.4256, -0.4104,    -inf,    -inf],\n",
      "          [-0.0801, -0.4435,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5381,  0.4691,    -inf,    -inf],\n",
      "          [ 0.6457, -0.0028,    -inf,    -inf],\n",
      "          [-0.9473, -0.7541,    -inf,    -inf],\n",
      "          [ 0.4058,  0.5040,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2212, -0.9109,    -inf,    -inf],\n",
      "          [ 0.0180, -0.1044,    -inf,    -inf],\n",
      "          [ 0.3014, -0.2400,    -inf,    -inf],\n",
      "          [ 0.2556, -0.0957,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.9144, -0.9614,    -inf,    -inf],\n",
      "          [-0.4599, -0.0410,    -inf,    -inf],\n",
      "          [-1.0830, -0.9043,    -inf,    -inf],\n",
      "          [-0.0223, -0.5165,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.9399,    -inf,    -inf,    -inf],\n",
      "          [-0.1567,    -inf,    -inf,    -inf],\n",
      "          [-0.6023,    -inf,    -inf,    -inf],\n",
      "          [-0.2317,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2374,    -inf,    -inf,    -inf],\n",
      "          [ 0.4079,    -inf,    -inf,    -inf],\n",
      "          [ 0.5644,    -inf,    -inf,    -inf],\n",
      "          [ 0.3860,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0744,    -inf,    -inf,    -inf],\n",
      "          [ 0.2233,    -inf,    -inf,    -inf],\n",
      "          [ 0.2628,    -inf,    -inf,    -inf],\n",
      "          [ 0.4867,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2939,    -inf,    -inf,    -inf],\n",
      "          [-0.1819,    -inf,    -inf,    -inf],\n",
      "          [ 0.0920,    -inf,    -inf,    -inf],\n",
      "          [ 0.1139,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5896, 0.4104, 0.0000, 0.0000],\n",
      "          [0.2817, 0.7183, 0.0000, 0.0000],\n",
      "          [0.4962, 0.5038, 0.0000, 0.0000],\n",
      "          [0.5899, 0.4101, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.2675, 0.7325, 0.0000, 0.0000],\n",
      "          [0.6567, 0.3433, 0.0000, 0.0000],\n",
      "          [0.4519, 0.5481, 0.0000, 0.0000],\n",
      "          [0.4755, 0.5245, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.7562, 0.2438, 0.0000, 0.0000],\n",
      "          [0.5306, 0.4694, 0.0000, 0.0000],\n",
      "          [0.6321, 0.3679, 0.0000, 0.0000],\n",
      "          [0.5869, 0.4131, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5118, 0.4882, 0.0000, 0.0000],\n",
      "          [0.3968, 0.6032, 0.0000, 0.0000],\n",
      "          [0.4555, 0.5445, 0.0000, 0.0000],\n",
      "          [0.6211, 0.3789, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.8389, -0.2059,    -inf,    -inf,    -inf],\n",
      "          [-0.1560, -0.2225,    -inf,    -inf,    -inf],\n",
      "          [-0.4163, -0.3317,    -inf,    -inf,    -inf],\n",
      "          [-0.1695, -0.2151,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2995, -0.3196,    -inf,    -inf,    -inf],\n",
      "          [ 0.0623,  0.2958,    -inf,    -inf,    -inf],\n",
      "          [-0.1609,  0.3043,    -inf,    -inf,    -inf],\n",
      "          [-0.1204,  0.2164,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.7076, -0.2733,    -inf,    -inf,    -inf],\n",
      "          [-0.3962, -0.4828,    -inf,    -inf,    -inf],\n",
      "          [-0.5286, -0.7225,    -inf,    -inf,    -inf],\n",
      "          [-0.6143, -0.5210,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3754, -0.0368,    -inf,    -inf,    -inf],\n",
      "          [-0.0616,  0.6433,    -inf,    -inf,    -inf],\n",
      "          [ 0.0557,  0.4866,    -inf,    -inf,    -inf],\n",
      "          [-0.1095,  0.6437,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3682, -0.3469, -0.0840,  0.4210,  0.0513],\n",
      "          [ 0.1072, -0.5574,  0.1270,  0.1160,  0.3664],\n",
      "          [-0.2005, -0.2864, -0.0748, -0.0986, -0.2394],\n",
      "          [-0.2628, -0.5114, -0.1300,  0.0335, -0.3912]],\n",
      "\n",
      "         [[-0.0982,  0.0536, -0.4088,  0.4887, -0.3572],\n",
      "          [-0.8342,  0.3628, -0.0615,  0.0395, -0.7046],\n",
      "          [-0.5310, -0.1917,  0.0681,  0.2139, -0.2879],\n",
      "          [-0.0704,  0.0175, -0.0757,  0.0302,  0.0415]],\n",
      "\n",
      "         [[ 0.1493,  0.3571, -0.0775,  0.1610, -0.1934],\n",
      "          [-0.0304,  0.2341,  0.0074,  0.5609,  0.5535],\n",
      "          [-0.3965, -0.5057,  0.1354, -0.1013, -1.0796],\n",
      "          [-0.1681, -0.1109, -0.1061,  0.1272,  0.1478]],\n",
      "\n",
      "         [[ 0.2072, -0.1804, -0.0963,  0.2577, -0.0741],\n",
      "          [-0.1799, -0.3586,  0.3429, -0.2328,  0.2751],\n",
      "          [-0.2044,  0.0295,  0.2944,  0.3939,  0.4687],\n",
      "          [-0.0779, -0.0117, -0.2338, -0.4017, -0.2835]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4246,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1975,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1912,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0966,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0699,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3331,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.4012,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2113,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0064,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2869,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3459,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2660,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2773,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.6333,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.6507,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.5568,    -inf,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3468, 0.6532, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5166, 0.4834, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4789, 0.5211, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5114, 0.4886, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5050, 0.4950, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4419, 0.5581, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3858, 0.6142, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4166, 0.5834, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3931, 0.6069, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5216, 0.4784, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5483, 0.4517, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4767, 0.5233, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4162, 0.5838, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3307, 0.6693, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3939, 0.6061, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3201, 0.6799, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1414, 0.1444, 0.1879, 0.3113, 0.2151],\n",
      "          [0.2067, 0.1063, 0.2108, 0.2085, 0.2678],\n",
      "          [0.1953, 0.1792, 0.2214, 0.2162, 0.1878],\n",
      "          [0.1943, 0.1515, 0.2219, 0.2613, 0.1709]],\n",
      "\n",
      "         [[0.1829, 0.2129, 0.1341, 0.3289, 0.1412],\n",
      "          [0.0999, 0.3307, 0.2163, 0.2393, 0.1137],\n",
      "          [0.1315, 0.1846, 0.2394, 0.2769, 0.1677],\n",
      "          [0.1883, 0.2056, 0.1873, 0.2082, 0.2106]],\n",
      "\n",
      "         [[0.2105, 0.2592, 0.1678, 0.2130, 0.1494],\n",
      "          [0.1441, 0.1877, 0.1496, 0.2603, 0.2583],\n",
      "          [0.1836, 0.1646, 0.3125, 0.2466, 0.0927],\n",
      "          [0.1713, 0.1814, 0.1823, 0.2301, 0.2349]],\n",
      "\n",
      "         [[0.2368, 0.1607, 0.1748, 0.2490, 0.1787],\n",
      "          [0.1654, 0.1383, 0.2789, 0.1568, 0.2606],\n",
      "          [0.1300, 0.1643, 0.2142, 0.2366, 0.2549],\n",
      "          [0.2241, 0.2395, 0.1918, 0.1621, 0.1825]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2878,    -inf,    -inf,    -inf],\n",
      "          [ 0.1054,    -inf,    -inf,    -inf],\n",
      "          [-0.0169,    -inf,    -inf,    -inf],\n",
      "          [ 0.3927,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2384,    -inf,    -inf,    -inf],\n",
      "          [ 0.0320,    -inf,    -inf,    -inf],\n",
      "          [ 0.1711,    -inf,    -inf,    -inf],\n",
      "          [ 0.1582,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0588,    -inf,    -inf,    -inf],\n",
      "          [-0.1314,    -inf,    -inf,    -inf],\n",
      "          [-0.1144,    -inf,    -inf,    -inf],\n",
      "          [ 0.1808,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3094,    -inf,    -inf,    -inf],\n",
      "          [ 0.0839,    -inf,    -inf,    -inf],\n",
      "          [-0.1069,    -inf,    -inf,    -inf],\n",
      "          [-0.2594,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3240,  0.2363,    -inf,    -inf],\n",
      "          [-0.3979,  0.2569,    -inf,    -inf],\n",
      "          [-0.2474,  0.1115,    -inf,    -inf],\n",
      "          [-0.1850,  0.2762,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2937, -0.0313,    -inf,    -inf],\n",
      "          [-0.2815, -0.0486,    -inf,    -inf],\n",
      "          [-0.2392, -0.4227,    -inf,    -inf],\n",
      "          [ 0.2507, -0.1837,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2411,  0.0349,    -inf,    -inf],\n",
      "          [-0.5981, -0.2914,    -inf,    -inf],\n",
      "          [-0.3302,  0.2292,    -inf,    -inf],\n",
      "          [-0.7373, -0.2391,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1799,  0.2728,    -inf,    -inf],\n",
      "          [ 0.2193, -0.0902,    -inf,    -inf],\n",
      "          [ 0.2931,  0.0097,    -inf,    -inf],\n",
      "          [ 0.3612,  0.0681,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3124,    -inf,    -inf,    -inf],\n",
      "          [ 0.0702,    -inf,    -inf,    -inf],\n",
      "          [ 0.2627,    -inf,    -inf,    -inf],\n",
      "          [ 0.2957,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0386,    -inf,    -inf,    -inf],\n",
      "          [-0.1963,    -inf,    -inf,    -inf],\n",
      "          [-0.0805,    -inf,    -inf,    -inf],\n",
      "          [-0.0693,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0093,    -inf,    -inf,    -inf],\n",
      "          [ 0.3002,    -inf,    -inf,    -inf],\n",
      "          [ 0.4933,    -inf,    -inf,    -inf],\n",
      "          [ 0.3853,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0101,    -inf,    -inf,    -inf],\n",
      "          [-0.0559,    -inf,    -inf,    -inf],\n",
      "          [-0.3393,    -inf,    -inf,    -inf],\n",
      "          [-0.2879,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3635, 0.6365, 0.0000, 0.0000],\n",
      "          [0.3419, 0.6581, 0.0000, 0.0000],\n",
      "          [0.4112, 0.5888, 0.0000, 0.0000],\n",
      "          [0.3867, 0.6133, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4348, 0.5652, 0.0000, 0.0000],\n",
      "          [0.4420, 0.5580, 0.0000, 0.0000],\n",
      "          [0.5457, 0.4543, 0.0000, 0.0000],\n",
      "          [0.6069, 0.3931, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4314, 0.5686, 0.0000, 0.0000],\n",
      "          [0.4239, 0.5761, 0.0000, 0.0000],\n",
      "          [0.3637, 0.6363, 0.0000, 0.0000],\n",
      "          [0.3780, 0.6220, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4768, 0.5232, 0.0000, 0.0000],\n",
      "          [0.5768, 0.4232, 0.0000, 0.0000],\n",
      "          [0.5704, 0.4296, 0.0000, 0.0000],\n",
      "          [0.5727, 0.4273, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-1.8398e-01, -5.6608e-02,        -inf,        -inf,        -inf],\n",
      "          [-3.1934e-01, -4.8471e-01,        -inf,        -inf,        -inf],\n",
      "          [-4.4944e-01, -4.5807e-01,        -inf,        -inf,        -inf],\n",
      "          [-3.3072e-01, -3.5147e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.3048e-01,  3.5331e-01,        -inf,        -inf,        -inf],\n",
      "          [ 2.9306e-01, -6.4185e-03,        -inf,        -inf,        -inf],\n",
      "          [ 4.0721e-01, -1.1264e-01,        -inf,        -inf,        -inf],\n",
      "          [ 3.0236e-01, -1.5383e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 2.4313e-01,  9.9850e-01,        -inf,        -inf,        -inf],\n",
      "          [ 6.0767e-02,  7.3402e-01,        -inf,        -inf,        -inf],\n",
      "          [ 3.8248e-02,  7.9526e-01,        -inf,        -inf,        -inf],\n",
      "          [ 8.3000e-02,  7.2523e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-2.1108e-01,  4.1086e-01,        -inf,        -inf,        -inf],\n",
      "          [ 1.9487e-02, -6.9449e-02,        -inf,        -inf,        -inf],\n",
      "          [-2.1715e-01, -1.8549e-01,        -inf,        -inf,        -inf],\n",
      "          [-1.1473e-01, -2.2707e-02,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-2.4270e-01, -3.3171e-01,  5.9740e-01, -5.5668e-02,  6.2217e-02],\n",
      "          [-9.2378e-02, -2.8670e-01,  1.2206e-01,  6.1753e-02, -3.1536e-01],\n",
      "          [-9.8669e-02, -5.3082e-01,  1.4580e-01, -2.3073e-01, -3.1356e-01],\n",
      "          [-4.8266e-01,  2.6246e-01,  3.6863e-01, -4.1511e-02, -6.3307e-01]],\n",
      "\n",
      "         [[-1.6852e-01, -1.9644e-01, -8.6563e-02,  7.0322e-02,  7.7968e-02],\n",
      "          [-3.0440e-01, -3.3792e-01,  1.9678e-01, -2.5179e-01,  2.9547e-01],\n",
      "          [ 8.5902e-02,  1.5875e-02, -3.5432e-02,  2.5120e-01,  9.1305e-02],\n",
      "          [-2.8237e-02,  7.4663e-03, -2.4892e-01, -2.9748e-04,  3.2846e-01]],\n",
      "\n",
      "         [[-2.2600e-01,  1.9886e-01,  7.3415e-01,  5.0133e-01, -2.2681e-01],\n",
      "          [-2.0179e-01, -1.3054e-01,  1.6148e-01, -3.8031e-01,  3.3342e-01],\n",
      "          [-2.9310e-01,  3.2074e-01,  6.7073e-03,  3.6475e-01, -3.8430e-01],\n",
      "          [ 9.5312e-02, -2.6220e-01, -4.1723e-02, -5.2827e-01, -4.7218e-02]],\n",
      "\n",
      "         [[-4.9781e-01,  7.1599e-01, -1.5721e-01,  2.3422e-01,  2.1354e-02],\n",
      "          [ 3.9776e-01,  5.3977e-01, -1.0374e-01, -2.9623e-01, -1.6670e-02],\n",
      "          [-5.6116e-01, -2.3207e-01, -5.4903e-01, -1.4327e-01, -8.3830e-01],\n",
      "          [ 5.2335e-01,  5.0050e-01,  4.8340e-01,  2.5932e-01,  7.2920e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.2924e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.9858e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.7371e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.4407e-01,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-4.2680e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-2.1647e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-3.6368e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-1.8407e-01,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 4.6952e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.8849e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-6.5333e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.0475e-01,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 4.5521e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.0754e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 5.0660e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.2746e-01,        -inf,        -inf,        -inf,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4682, 0.5318, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5412, 0.4588, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5022, 0.4978, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5052, 0.4948, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4445, 0.5555, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5743, 0.4257, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6271, 0.3729, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6121, 0.3879, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3197, 0.6803, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3378, 0.6622, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3193, 0.6807, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3447, 0.6553, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3493, 0.6507, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5222, 0.4778, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4921, 0.5079, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4770, 0.5230, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1472, 0.1347, 0.3410, 0.1775, 0.1997],\n",
      "          [0.1988, 0.1637, 0.2464, 0.2320, 0.1591],\n",
      "          [0.2170, 0.1408, 0.2771, 0.1901, 0.1750],\n",
      "          [0.1272, 0.2679, 0.2979, 0.1977, 0.1094]],\n",
      "\n",
      "         [[0.1783, 0.1734, 0.1936, 0.2265, 0.2282],\n",
      "          [0.1540, 0.1489, 0.2542, 0.1623, 0.2806],\n",
      "          [0.1999, 0.1864, 0.1770, 0.2358, 0.2010],\n",
      "          [0.1888, 0.1957, 0.1515, 0.1942, 0.2698]],\n",
      "\n",
      "         [[0.1218, 0.1863, 0.3182, 0.2521, 0.1217],\n",
      "          [0.1651, 0.1773, 0.2374, 0.1381, 0.2820],\n",
      "          [0.1420, 0.2624, 0.1917, 0.2742, 0.1297],\n",
      "          [0.2516, 0.1760, 0.2194, 0.1349, 0.2182]],\n",
      "\n",
      "         [[0.1049, 0.3532, 0.1475, 0.2181, 0.1763],\n",
      "          [0.2552, 0.2941, 0.1546, 0.1275, 0.1686],\n",
      "          [0.1761, 0.2447, 0.1782, 0.2675, 0.1335],\n",
      "          [0.2302, 0.2250, 0.2212, 0.1768, 0.1467]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.5378,    -inf,    -inf,    -inf],\n",
      "          [-0.5286,    -inf,    -inf,    -inf],\n",
      "          [-0.5851,    -inf,    -inf,    -inf],\n",
      "          [-0.4753,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0318,    -inf,    -inf,    -inf],\n",
      "          [ 0.3953,    -inf,    -inf,    -inf],\n",
      "          [ 0.1941,    -inf,    -inf,    -inf],\n",
      "          [ 0.3646,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2309,    -inf,    -inf,    -inf],\n",
      "          [-0.1614,    -inf,    -inf,    -inf],\n",
      "          [ 0.1230,    -inf,    -inf,    -inf],\n",
      "          [-0.0547,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1519,    -inf,    -inf,    -inf],\n",
      "          [-0.0236,    -inf,    -inf,    -inf],\n",
      "          [-0.2204,    -inf,    -inf,    -inf],\n",
      "          [-0.2304,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1307,  0.3000,    -inf,    -inf],\n",
      "          [-0.1018, -0.3236,    -inf,    -inf],\n",
      "          [-0.8675, -0.5203,    -inf,    -inf],\n",
      "          [-0.4293, -0.0894,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0332,  0.0836,    -inf,    -inf],\n",
      "          [-0.7301, -0.2795,    -inf,    -inf],\n",
      "          [ 0.0948,  0.0981,    -inf,    -inf],\n",
      "          [-1.0352, -0.6072,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1884, -0.1774,    -inf,    -inf],\n",
      "          [ 0.2270,  0.1445,    -inf,    -inf],\n",
      "          [ 0.3972,  0.0201,    -inf,    -inf],\n",
      "          [-0.1420, -0.2852,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0046, -0.6659,    -inf,    -inf],\n",
      "          [-0.2092, -0.0235,    -inf,    -inf],\n",
      "          [-0.0657, -0.3739,    -inf,    -inf],\n",
      "          [-0.1233, -0.3910,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0744,    -inf,    -inf,    -inf],\n",
      "          [-0.4110,    -inf,    -inf,    -inf],\n",
      "          [-0.5540,    -inf,    -inf,    -inf],\n",
      "          [-0.4332,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0670,    -inf,    -inf,    -inf],\n",
      "          [-0.3291,    -inf,    -inf,    -inf],\n",
      "          [-0.2069,    -inf,    -inf,    -inf],\n",
      "          [-0.1764,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0768,    -inf,    -inf,    -inf],\n",
      "          [-0.1043,    -inf,    -inf,    -inf],\n",
      "          [-0.1792,    -inf,    -inf,    -inf],\n",
      "          [-0.2586,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2776,    -inf,    -inf,    -inf],\n",
      "          [-0.1206,    -inf,    -inf,    -inf],\n",
      "          [-0.1841,    -inf,    -inf,    -inf],\n",
      "          [-0.1452,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3940, 0.6060, 0.0000, 0.0000],\n",
      "          [0.5552, 0.4448, 0.0000, 0.0000],\n",
      "          [0.4140, 0.5860, 0.0000, 0.0000],\n",
      "          [0.4158, 0.5842, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4708, 0.5292, 0.0000, 0.0000],\n",
      "          [0.3892, 0.6108, 0.0000, 0.0000],\n",
      "          [0.4992, 0.5008, 0.0000, 0.0000],\n",
      "          [0.3946, 0.6054, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4972, 0.5028, 0.0000, 0.0000],\n",
      "          [0.5206, 0.4794, 0.0000, 0.0000],\n",
      "          [0.5932, 0.4068, 0.0000, 0.0000],\n",
      "          [0.5358, 0.4642, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.6616, 0.3384, 0.0000, 0.0000],\n",
      "          [0.4537, 0.5463, 0.0000, 0.0000],\n",
      "          [0.5764, 0.4236, 0.0000, 0.0000],\n",
      "          [0.5665, 0.4335, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-3.5046e-01, -4.2632e-01,        -inf,        -inf,        -inf],\n",
      "          [ 3.0566e-01, -3.0747e-01,        -inf,        -inf,        -inf],\n",
      "          [ 3.8075e-01, -3.8903e-01,        -inf,        -inf,        -inf],\n",
      "          [ 9.7710e-02, -4.8368e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-2.5665e-01, -2.3448e-02,        -inf,        -inf,        -inf],\n",
      "          [-5.7583e-02,  7.0593e-01,        -inf,        -inf,        -inf],\n",
      "          [-4.2607e-01,  3.2811e-01,        -inf,        -inf,        -inf],\n",
      "          [-5.2337e-01,  2.9511e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-6.0658e-01, -3.3380e-01,        -inf,        -inf,        -inf],\n",
      "          [-2.8359e-01,  5.5428e-02,        -inf,        -inf,        -inf],\n",
      "          [-2.4144e-01,  7.6894e-03,        -inf,        -inf,        -inf],\n",
      "          [-1.5775e-01,  1.7634e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.4296e-01,  2.7672e-01,        -inf,        -inf,        -inf],\n",
      "          [ 5.9602e-02, -8.8144e-02,        -inf,        -inf,        -inf],\n",
      "          [-1.1457e-01,  1.3085e-01,        -inf,        -inf,        -inf],\n",
      "          [-3.5329e-01,  1.8377e-02,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-5.9016e-02,  3.9052e-02, -2.8007e-02,  9.2453e-02,  2.3334e-01],\n",
      "          [ 9.6832e-03, -1.7511e-01, -2.0486e-02,  2.8086e-01,  5.9553e-02],\n",
      "          [-3.8074e-02, -4.9812e-01,  1.2665e-01, -2.4749e-01, -1.3323e-01],\n",
      "          [ 9.9988e-02,  1.8484e-02,  1.8915e-02,  1.9872e-02,  1.0416e-01]],\n",
      "\n",
      "         [[-2.4847e-02,  2.0592e-01, -2.6632e-02,  1.7750e-01,  5.4161e-01],\n",
      "          [ 4.5011e-01,  7.7839e-01,  3.2215e-01,  5.9555e-01,  7.3221e-01],\n",
      "          [ 1.7177e-01,  2.5432e-01, -3.7971e-01,  2.4206e-01,  6.7588e-01],\n",
      "          [ 1.9838e-01,  4.0318e-01,  2.7156e-03,  4.1084e-01,  4.2126e-01]],\n",
      "\n",
      "         [[ 1.2775e-01, -3.6808e-01, -3.8725e-01,  4.0153e-01,  5.7718e-02],\n",
      "          [ 1.6222e-01, -2.8023e-01, -6.0677e-01,  1.6677e-01, -4.2578e-01],\n",
      "          [ 8.3263e-02, -6.6049e-01, -4.5846e-01,  8.1623e-02, -4.5638e-02],\n",
      "          [-2.2006e-01, -2.5219e-01,  3.1538e-01,  5.9082e-02, -1.4668e-02]],\n",
      "\n",
      "         [[ 2.6030e-01,  1.4566e-01,  1.4344e-01,  1.2738e-01,  6.4599e-01],\n",
      "          [-2.4479e-01, -1.8389e-01,  2.9254e-01, -1.4307e-01, -3.4042e-02],\n",
      "          [ 1.3709e-01, -2.3985e-01,  4.5635e-01,  2.7970e-01,  5.7302e-02],\n",
      "          [-7.1107e-01, -3.1782e-01,  5.3384e-01, -5.5403e-01,  1.6066e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.3168e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.0518e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-5.0276e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 4.3265e-04,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-2.2129e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 7.6235e-03,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-3.1479e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-6.2010e-02,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.6072e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.4771e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 3.6895e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.5364e-01,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-5.4177e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.4793e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.8368e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.5590e-01,        -inf,        -inf,        -inf,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5190, 0.4810, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6487, 0.3513, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6835, 0.3165, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6414, 0.3586, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4420, 0.5580, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3179, 0.6821, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3199, 0.6801, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3061, 0.6939, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4322, 0.5678, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4160, 0.5840, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4380, 0.5620, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4172, 0.5828, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4666, 0.5334, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5369, 0.4631, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4390, 0.5610, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4081, 0.5919, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1774, 0.1957, 0.1830, 0.2064, 0.2376],\n",
      "          [0.1936, 0.1610, 0.1879, 0.2540, 0.2035],\n",
      "          [0.2207, 0.1393, 0.2602, 0.1790, 0.2007],\n",
      "          [0.2096, 0.1932, 0.1933, 0.1935, 0.2105]],\n",
      "\n",
      "         [[0.1602, 0.2017, 0.1599, 0.1961, 0.2822],\n",
      "          [0.1739, 0.2415, 0.1530, 0.2011, 0.2306],\n",
      "          [0.1855, 0.2015, 0.1069, 0.1990, 0.3071],\n",
      "          [0.1806, 0.2217, 0.1485, 0.2234, 0.2257]],\n",
      "\n",
      "         [[0.2245, 0.1368, 0.1342, 0.2952, 0.2093],\n",
      "          [0.2728, 0.1753, 0.1264, 0.2740, 0.1515],\n",
      "          [0.2542, 0.1208, 0.1479, 0.2537, 0.2234],\n",
      "          [0.1606, 0.1555, 0.2743, 0.2123, 0.1972]],\n",
      "\n",
      "         [[0.1950, 0.1739, 0.1735, 0.1708, 0.2868],\n",
      "          [0.1635, 0.1738, 0.2798, 0.1810, 0.2019],\n",
      "          [0.1946, 0.1335, 0.2678, 0.2244, 0.1797],\n",
      "          [0.1088, 0.1612, 0.3777, 0.1273, 0.2251]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "output shape: torch.Size([3, 4, 100277])\n",
      "softmaxed[0, 0, :10]: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "predicted: \n",
      " tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "predicted decoded: \n",
      " ['Bonjour<|endoftext|><|endoftext|><|endoftext|>', \"C'est une phrase\", 'START<|endoftext|><|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sents = [\"Hello World\", \"This is a simple sentence\", \"Me\"]\n",
    "encoded_sents = [encoding.encode(s) for s in sents]\n",
    "enc_x = pad_sequence([torch.tensor(es) for es in encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(enc_x)\n",
    "dec_sents = [\"Bonjour\", \"C'est une phrase\", \"START\"]\n",
    "dec_encoded_sents = [encoding.encode(s) for s in dec_sents]\n",
    "dec_x = pad_sequence([torch.tensor(es) for es in dec_encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(dec_x)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=100257)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=100257)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "print(f\"output shape: {output.shape}\")\n",
    "softmaxed = F.softmax(output, dim=-1)\n",
    "print(f\"softmaxed[0, 0, :10]: {softmaxed[0, 0, :10]}\")\n",
    "predicted = softmaxed.argmax(dim=-1)\n",
    "print(f\"predicted: \\n {predicted}\")\n",
    "\n",
    "predicted_list = predicted.tolist()\n",
    "predicted_decoded = [encoding.decode(l) for l in predicted_list]\n",
    "print(f\"predicted decoded: \\n {predicted_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "693b16e4-6684-4dbb-b21b-8c3650a10bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-6.3044e-02, -1.0383e-01, -3.5046e-01, -3.0917e-01, -5.6716e-01],\n",
      "          [ 3.8317e-01, -2.9907e-01,  5.7574e-01,  8.1185e-01,  1.2366e-01],\n",
      "          [ 5.0410e-01, -6.2694e-01,  7.3871e-04,  3.4817e-01, -4.0788e-01],\n",
      "          [-6.2386e-01,  5.7701e-01,  5.5564e-01,  1.2118e+00, -4.1845e-02],\n",
      "          [-2.2817e-01, -2.4818e-01,  1.0316e+00, -4.9224e-01,  1.1674e-01]],\n",
      "\n",
      "         [[ 5.4720e-01,  1.0197e+00,  7.9706e-02,  5.0480e-01, -6.9270e-02],\n",
      "          [ 2.1791e-01,  1.7174e-01, -2.5159e-01,  8.5140e-01,  1.9459e-01],\n",
      "          [-1.8700e-01, -3.0898e-01, -2.8735e-01,  1.1491e+00, -3.2226e-01],\n",
      "          [-3.7833e-02, -3.0201e-01,  3.6585e-01,  9.2050e-01,  3.6115e-01],\n",
      "          [ 2.9166e-01,  8.3893e-01,  1.0538e+00,  1.0583e+00,  5.2661e-01]],\n",
      "\n",
      "         [[ 8.9163e-02, -1.4486e-02,  6.3829e-01,  7.5214e-01,  6.0259e-01],\n",
      "          [ 9.1203e-01, -1.8334e-01, -5.5605e-01, -3.3620e-01, -2.5333e-01],\n",
      "          [ 3.2559e-01,  4.3031e-02,  4.8591e-01,  1.3703e+00, -4.8817e-02],\n",
      "          [ 2.3457e-01, -1.7860e-02, -8.8408e-02, -5.1389e-02,  6.3811e-02],\n",
      "          [ 2.1638e-01, -4.3446e-01,  1.1505e-01, -5.3130e-02,  9.5919e-02]],\n",
      "\n",
      "         [[ 3.6121e-01, -8.2268e-01, -6.3045e-01, -2.0163e+00, -4.9045e-01],\n",
      "          [ 8.0117e-02, -1.8328e-01, -5.4722e-01, -1.8457e-01, -7.7580e-01],\n",
      "          [-1.3049e+00, -1.7825e+00, -1.0555e+00, -9.3355e-01, -1.3517e+00],\n",
      "          [-2.3968e-01, -3.8129e-01, -9.8270e-01, -1.3284e-01, -1.0312e+00],\n",
      "          [-7.7382e-02, -1.8392e-01, -4.7495e-01, -2.4687e-01, -2.0307e-01]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2441, 0.2344, 0.1832, 0.1909, 0.1475],\n",
      "          [0.1990, 0.1006, 0.2413, 0.3055, 0.1535],\n",
      "          [0.3140, 0.1013, 0.1898, 0.2687, 0.1261],\n",
      "          [0.0640, 0.2125, 0.2080, 0.4010, 0.1145],\n",
      "          [0.1301, 0.1276, 0.4587, 0.0999, 0.1837]],\n",
      "\n",
      "         [[0.2115, 0.3392, 0.1325, 0.2027, 0.1142],\n",
      "          [0.1838, 0.1755, 0.1149, 0.3463, 0.1795],\n",
      "          [0.1339, 0.1185, 0.1211, 0.5094, 0.1170],\n",
      "          [0.1358, 0.1043, 0.2034, 0.3541, 0.2024],\n",
      "          [0.1206, 0.2085, 0.2585, 0.2597, 0.1526]],\n",
      "\n",
      "         [[0.1380, 0.1244, 0.2390, 0.2679, 0.2306],\n",
      "          [0.4622, 0.1546, 0.1065, 0.1327, 0.1441],\n",
      "          [0.1548, 0.1167, 0.1818, 0.4402, 0.1065],\n",
      "          [0.2442, 0.1897, 0.1768, 0.1835, 0.2059],\n",
      "          [0.2454, 0.1280, 0.2217, 0.1874, 0.2175]],\n",
      "\n",
      "         [[0.4553, 0.1393, 0.1689, 0.0422, 0.1943],\n",
      "          [0.2861, 0.2199, 0.1528, 0.2196, 0.1216],\n",
      "          [0.1884, 0.1169, 0.2418, 0.2731, 0.1798],\n",
      "          [0.2558, 0.2220, 0.1217, 0.2846, 0.1159],\n",
      "          [0.2327, 0.2092, 0.1564, 0.1964, 0.2052]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.3672, -0.1914, -0.1555, -0.0252, -0.0166],\n",
      "          [ 0.7819,  0.3045, -0.2356,  0.0146, -0.0437],\n",
      "          [-0.1012, -0.0922,  0.2281,  0.4556,  0.5162],\n",
      "          [ 0.3636,  0.1678, -0.2653,  0.0151,  0.0291],\n",
      "          [ 0.0533,  0.0850, -0.6745,  0.0015,  0.0799]],\n",
      "\n",
      "         [[ 0.3356, -0.1725,  0.1503, -0.2722,  0.2262],\n",
      "          [ 0.6190,  0.2334,  0.3203, -0.0958, -0.0200],\n",
      "          [ 0.0952, -0.2840, -0.2672,  0.0689, -0.2224],\n",
      "          [-0.0028, -0.0828, -0.0906, -0.1033,  0.0074],\n",
      "          [ 0.2439, -0.1616, -0.1291, -0.1496, -0.1620]],\n",
      "\n",
      "         [[-0.1553,  0.3228, -0.0677, -0.1484,  0.2436],\n",
      "          [-0.3164,  0.3163, -0.7243, -0.3588, -0.4401],\n",
      "          [ 0.0954,  1.0385,  0.0632,  0.3498, -0.6065],\n",
      "          [ 0.8806,  0.3253,  0.5053,  0.0077,  0.5008],\n",
      "          [ 0.0590,  0.5639,  0.2552, -0.3968, -0.0337]],\n",
      "\n",
      "         [[ 0.2200,  0.0833, -0.1726,  0.4705,  0.2653],\n",
      "          [ 0.3993, -0.1043, -0.0645,  0.1414, -0.1167],\n",
      "          [-0.2423, -0.1967,  0.1026,  0.1277,  0.0187],\n",
      "          [-0.5469,  0.1672,  0.2932,  0.5627,  0.1331],\n",
      "          [-0.1316,  0.2304,  0.0843, -0.3373, -0.1064]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1599, 0.1906, 0.1976, 0.2250, 0.2270],\n",
      "          [0.3467, 0.2151, 0.1253, 0.1610, 0.1519],\n",
      "          [0.1429, 0.1442, 0.1986, 0.2494, 0.2649],\n",
      "          [0.2648, 0.2177, 0.1412, 0.1869, 0.1895],\n",
      "          [0.2226, 0.2298, 0.1075, 0.2114, 0.2286]],\n",
      "\n",
      "         [[0.2582, 0.1553, 0.2145, 0.1406, 0.2314],\n",
      "          [0.2908, 0.1977, 0.2157, 0.1423, 0.1535],\n",
      "          [0.2450, 0.1676, 0.1705, 0.2386, 0.1783],\n",
      "          [0.2104, 0.1942, 0.1927, 0.1903, 0.2125],\n",
      "          [0.2705, 0.1804, 0.1863, 0.1825, 0.1803]],\n",
      "\n",
      "         [[0.1612, 0.2601, 0.1760, 0.1624, 0.2403],\n",
      "          [0.1855, 0.3493, 0.1234, 0.1778, 0.1639],\n",
      "          [0.1582, 0.4062, 0.1532, 0.2040, 0.0784],\n",
      "          [0.2974, 0.1707, 0.2043, 0.1242, 0.2034],\n",
      "          [0.1845, 0.3057, 0.2245, 0.1170, 0.1682]],\n",
      "\n",
      "         [[0.2050, 0.1788, 0.1384, 0.2633, 0.2145],\n",
      "          [0.2776, 0.1677, 0.1746, 0.2145, 0.1657],\n",
      "          [0.1612, 0.1687, 0.2276, 0.2333, 0.2092],\n",
      "          [0.0965, 0.1970, 0.2235, 0.2926, 0.1904],\n",
      "          [0.1813, 0.2603, 0.2249, 0.1476, 0.1859]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 3.2341e-01,  6.8685e-02,  2.0549e-01,  3.2047e-01,  5.1305e-01],\n",
      "          [-3.2656e-01, -2.5925e-01,  9.4348e-02, -2.2015e-01, -1.3280e-01],\n",
      "          [ 9.3083e-02,  3.7004e-01, -5.0175e-01, -4.3492e-02, -1.2870e-01],\n",
      "          [ 2.2760e-01,  1.0613e-01,  2.6420e-01,  3.4623e-02,  7.9242e-02],\n",
      "          [ 2.8009e-01,  2.4642e-01, -2.1696e-01, -8.0586e-02,  2.8363e-01]],\n",
      "\n",
      "         [[ 9.4885e-02,  2.4260e-02,  6.9095e-02, -9.4464e-02,  1.3649e-01],\n",
      "          [-8.8481e-02,  1.5093e-01, -3.0175e-01,  6.5123e-01, -1.1246e-02],\n",
      "          [-2.7882e-01, -3.8884e-01, -6.0368e-01, -3.2346e-01, -3.1758e-01],\n",
      "          [-2.0803e-03, -2.2114e-01, -2.9514e-01, -1.9248e-01, -6.0756e-01],\n",
      "          [-4.0608e-02, -4.0999e-01, -3.3611e-01,  1.5912e-01, -3.5053e-01]],\n",
      "\n",
      "         [[-4.0592e-01, -2.9289e-01, -1.7627e-01, -1.4844e-01, -3.7669e-02],\n",
      "          [-1.3756e-01,  2.4044e-01, -8.4428e-03,  5.8354e-04,  1.5272e-01],\n",
      "          [-4.3583e-02,  2.7833e-01,  3.1147e-03,  4.2494e-01,  3.1131e-01],\n",
      "          [-1.1874e-01,  1.1447e-01, -1.5699e-01,  3.0895e-01,  1.7781e-01],\n",
      "          [-2.5196e-01,  1.4730e-01,  1.2768e-01, -3.8465e-01,  1.0470e-01]],\n",
      "\n",
      "         [[ 1.5940e-01, -2.7183e-02, -3.8330e-01,  1.5635e-01, -6.9219e-01],\n",
      "          [ 6.9620e-02, -2.8293e-01, -7.7444e-01, -1.2417e-01, -3.9227e-01],\n",
      "          [-2.2228e-01,  3.6687e-02,  1.3341e-01,  1.3066e-01,  1.6087e-02],\n",
      "          [-3.1703e-01, -8.0188e-01, -4.6688e-01,  1.0432e-02, -6.3426e-01],\n",
      "          [ 1.0052e-01, -7.2977e-01, -3.4663e-01,  4.0010e-01, -5.6284e-01]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2053, 0.1592, 0.1825, 0.2047, 0.2482],\n",
      "          [0.1689, 0.1807, 0.2574, 0.1879, 0.2051],\n",
      "          [0.2201, 0.2903, 0.1214, 0.1920, 0.1763],\n",
      "          [0.2169, 0.1921, 0.2250, 0.1789, 0.1870],\n",
      "          [0.2339, 0.2261, 0.1423, 0.1631, 0.2347]],\n",
      "\n",
      "         [[0.2094, 0.1951, 0.2040, 0.1732, 0.2183],\n",
      "          [0.1599, 0.2031, 0.1292, 0.3350, 0.1727],\n",
      "          [0.2204, 0.1975, 0.1593, 0.2108, 0.2120],\n",
      "          [0.2550, 0.2048, 0.1902, 0.2108, 0.1392],\n",
      "          [0.2278, 0.1574, 0.1695, 0.2782, 0.1671]],\n",
      "\n",
      "         [[0.1635, 0.1831, 0.2057, 0.2115, 0.2363],\n",
      "          [0.1644, 0.2399, 0.1871, 0.1888, 0.2198],\n",
      "          [0.1550, 0.2139, 0.1624, 0.2476, 0.2210],\n",
      "          [0.1638, 0.2069, 0.1577, 0.2513, 0.2204],\n",
      "          [0.1598, 0.2383, 0.2336, 0.1400, 0.2283]],\n",
      "\n",
      "         [[0.2608, 0.2164, 0.1516, 0.2600, 0.1113],\n",
      "          [0.2788, 0.1960, 0.1199, 0.2297, 0.1757],\n",
      "          [0.1559, 0.2020, 0.2225, 0.2219, 0.1978],\n",
      "          [0.2178, 0.1341, 0.1875, 0.3021, 0.1586],\n",
      "          [0.2538, 0.1106, 0.1623, 0.3425, 0.1307]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 1, 1])\n",
      "scaled_prod: \n",
      " tensor([[[[0.2200]],\n",
      "\n",
      "         [[0.3676]],\n",
      "\n",
      "         [[0.4020]],\n",
      "\n",
      "         [[0.4481]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 1, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.2952,  0.6339,  0.2937,  0.1470,  0.1781]],\n",
      "\n",
      "         [[ 0.2721,  0.0765,  0.1068, -0.3607,  0.2056]],\n",
      "\n",
      "         [[-0.0471,  0.0844, -0.2946,  0.0496,  0.2160]],\n",
      "\n",
      "         [[-0.2178, -0.0815,  0.2777,  0.2100, -0.3728]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1941, 0.2723, 0.1938, 0.1673, 0.1726]],\n",
      "\n",
      "         [[0.2417, 0.1988, 0.2049, 0.1284, 0.2262]],\n",
      "\n",
      "         [[0.1878, 0.2142, 0.1467, 0.2069, 0.2444]],\n",
      "\n",
      "         [[0.1619, 0.1855, 0.2657, 0.2483, 0.1386]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 1, 1])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0121]],\n",
      "\n",
      "         [[-0.3741]],\n",
      "\n",
      "         [[-0.0552]],\n",
      "\n",
      "         [[ 0.1978]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 1, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-3.6041e-01,  1.0044e-01,  1.2307e-01, -5.7280e-01, -3.7765e-01]],\n",
      "\n",
      "         [[ 6.0719e-01,  3.5639e-04,  5.2769e-01,  6.5132e-01,  5.1862e-01]],\n",
      "\n",
      "         [[ 1.2804e-01, -4.4860e-01, -1.2755e-01, -8.1525e-02, -2.2099e-01]],\n",
      "\n",
      "         [[ 1.4401e-01,  2.0933e-01,  4.4923e-01,  2.7062e-01, -2.4443e-01]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1667, 0.2643, 0.2703, 0.1348, 0.1639]],\n",
      "\n",
      "         [[0.2258, 0.1231, 0.2085, 0.2360, 0.2066]],\n",
      "\n",
      "         [[0.2596, 0.1458, 0.2010, 0.2105, 0.1831]],\n",
      "\n",
      "         [[0.1909, 0.2038, 0.2591, 0.2167, 0.1295]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 1, 1])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2925]],\n",
      "\n",
      "         [[ 0.3005]],\n",
      "\n",
      "         [[-0.1286]],\n",
      "\n",
      "         [[ 0.0445]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]],\n",
      "\n",
      "         [[1.]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 1, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.2597, -0.7356,  0.6067, -0.0111,  0.0227]],\n",
      "\n",
      "         [[-0.1616, -0.3199, -0.4845, -0.2107, -0.2641]],\n",
      "\n",
      "         [[ 0.3304, -0.1925,  0.1084,  0.2711,  0.6459]],\n",
      "\n",
      "         [[ 0.1062, -0.0959, -0.0821,  0.1261,  0.2179]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2306, 0.0852, 0.3263, 0.1759, 0.1820]],\n",
      "\n",
      "         [[0.2256, 0.1926, 0.1634, 0.2148, 0.2036]],\n",
      "\n",
      "         [[0.2124, 0.1259, 0.1702, 0.2002, 0.2913]],\n",
      "\n",
      "         [[0.2090, 0.1708, 0.1732, 0.2132, 0.2338]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.3981, -0.0717,  0.1323, -0.3665, -0.0264],\n",
      "          [-0.1476, -0.2554,  0.3010,  0.5891,  0.2617],\n",
      "          [ 0.4361, -0.1405,  0.2184,  0.1007, -0.7079],\n",
      "          [ 0.0435,  0.9278,  0.3046,  0.4922, -0.0468],\n",
      "          [-0.2031, -0.1315,  0.4325,  0.0640, -0.1700]],\n",
      "\n",
      "         [[ 0.4715,  0.5918, -0.0509,  0.9832,  0.2236],\n",
      "          [ 0.0192,  0.2858, -0.5111,  0.6201,  0.3593],\n",
      "          [ 0.1415, -0.2938, -0.0773,  1.1787, -0.3294],\n",
      "          [ 0.1432,  0.3591,  0.7432,  1.1404, -0.0234],\n",
      "          [ 0.0075,  0.6830,  0.9555,  1.1871,  0.7176]],\n",
      "\n",
      "         [[ 0.1287,  0.2319,  0.1431,  0.3267,  0.6290],\n",
      "          [ 0.8886, -0.3687, -0.9074, -0.1343, -0.4823],\n",
      "          [ 0.5695,  0.5908,  0.3009,  0.9776, -0.0418],\n",
      "          [ 0.4294,  0.0079,  0.3116,  0.0516, -0.2165],\n",
      "          [ 0.2678, -0.5356, -0.4114,  0.4721, -0.1359]],\n",
      "\n",
      "         [[-0.4159, -0.2550, -0.7360, -1.4088, -0.4029],\n",
      "          [-0.0716, -0.0675, -0.3133,  0.3345, -0.6480],\n",
      "          [-1.4068, -2.0361, -1.1060, -0.7895, -1.1289],\n",
      "          [ 0.0268, -0.6012, -1.0893, -0.1662, -0.8485],\n",
      "          [-0.5000, -0.5585, -0.5618, -0.1856, -0.9242]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2848, 0.1780, 0.2183, 0.1326, 0.1863],\n",
      "          [0.1417, 0.1272, 0.2219, 0.2960, 0.2133],\n",
      "          [0.2941, 0.1652, 0.2366, 0.2103, 0.0937],\n",
      "          [0.1389, 0.3363, 0.1803, 0.2175, 0.1269],\n",
      "          [0.1587, 0.1705, 0.2996, 0.2073, 0.1640]],\n",
      "\n",
      "         [[0.1934, 0.2182, 0.1147, 0.3227, 0.1510],\n",
      "          [0.1633, 0.2132, 0.0961, 0.2979, 0.2295],\n",
      "          [0.1696, 0.1097, 0.1363, 0.4785, 0.1059],\n",
      "          [0.1312, 0.1628, 0.2391, 0.3557, 0.1111],\n",
      "          [0.0923, 0.1814, 0.2382, 0.3003, 0.1878]],\n",
      "\n",
      "         [[0.1669, 0.1850, 0.1693, 0.2035, 0.2753],\n",
      "          [0.4845, 0.1378, 0.0804, 0.1742, 0.1230],\n",
      "          [0.2069, 0.2114, 0.1582, 0.3112, 0.1123],\n",
      "          [0.2663, 0.1747, 0.2368, 0.1825, 0.1396],\n",
      "          [0.2598, 0.1163, 0.1317, 0.3187, 0.1735]],\n",
      "\n",
      "         [[0.2334, 0.2742, 0.1695, 0.0865, 0.2365],\n",
      "          [0.2061, 0.2069, 0.1618, 0.3093, 0.1158],\n",
      "          [0.1651, 0.0880, 0.2230, 0.3060, 0.2179],\n",
      "          [0.3223, 0.1720, 0.1056, 0.2658, 0.1343],\n",
      "          [0.2038, 0.1922, 0.1916, 0.2791, 0.1333]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-3.2430e-01, -3.8301e-01, -3.2087e-01,  7.6783e-04,  1.8551e-01],\n",
      "          [ 6.8761e-01, -1.5167e-02, -1.1819e-02,  3.0934e-01,  8.9849e-02],\n",
      "          [-3.9033e-01, -4.5420e-01,  1.5275e-02,  6.9730e-01,  4.7660e-01],\n",
      "          [ 1.9789e-01,  3.1662e-01, -2.4271e-01, -2.4735e-01, -1.5691e-01],\n",
      "          [ 3.6736e-02,  5.4676e-02, -1.3453e-01,  4.5498e-01,  4.0339e-01]],\n",
      "\n",
      "         [[ 7.5359e-01, -1.6781e-01,  2.7230e-01,  1.4589e-01,  2.0013e-01],\n",
      "          [ 7.4971e-01,  2.7136e-01,  9.3747e-02, -1.1298e-01,  2.5812e-01],\n",
      "          [ 1.9523e-01, -1.8823e-01,  1.5104e-01, -4.6194e-03, -2.8069e-01],\n",
      "          [ 7.5747e-03,  2.6931e-01,  7.6959e-02,  3.1364e-01, -2.6023e-02],\n",
      "          [ 6.1961e-01,  1.6929e-02,  1.1375e-01, -5.2946e-01, -2.4499e-01]],\n",
      "\n",
      "         [[-3.3443e-01,  9.9037e-02, -2.0473e-01,  1.4043e-02,  8.8003e-03],\n",
      "          [-5.5870e-01, -1.2013e-01, -7.7482e-01, -3.2105e-01, -2.6310e-01],\n",
      "          [ 4.0351e-01,  6.6909e-01,  9.7520e-02,  5.8656e-02, -2.6862e-01],\n",
      "          [ 6.0370e-01,  5.8769e-02,  2.8465e-01,  2.3998e-01,  2.4620e-01],\n",
      "          [ 4.3576e-01,  9.8144e-01,  1.8386e-01,  1.7585e-01,  3.6050e-01]],\n",
      "\n",
      "         [[ 2.7680e-01, -3.3079e-01,  1.5836e-01,  2.5459e-01, -1.9443e-02],\n",
      "          [ 2.2498e-01, -1.1000e-01,  1.9332e-01,  2.2817e-02,  4.5680e-02],\n",
      "          [-3.2453e-02,  6.4776e-02,  1.5396e-01, -1.2768e-01, -2.5129e-01],\n",
      "          [-4.1935e-01,  2.5444e-01,  1.7616e-01,  3.2744e-01,  2.7814e-01],\n",
      "          [-9.3504e-02,  4.3454e-01,  7.9219e-02, -1.7686e-01, -4.0706e-01]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1668, 0.1573, 0.1674, 0.2309, 0.2777],\n",
      "          [0.3099, 0.1534, 0.1540, 0.2123, 0.1704],\n",
      "          [0.1138, 0.1068, 0.1708, 0.3378, 0.2709],\n",
      "          [0.2432, 0.2739, 0.1565, 0.1558, 0.1706],\n",
      "          [0.1717, 0.1748, 0.1447, 0.2609, 0.2478]],\n",
      "\n",
      "         [[0.3189, 0.1269, 0.1971, 0.1737, 0.1834],\n",
      "          [0.3152, 0.1954, 0.1636, 0.1330, 0.1928],\n",
      "          [0.2452, 0.1671, 0.2346, 0.2008, 0.1523],\n",
      "          [0.1756, 0.2281, 0.1882, 0.2384, 0.1698],\n",
      "          [0.3462, 0.1895, 0.2088, 0.1097, 0.1458]],\n",
      "\n",
      "         [[0.1537, 0.2370, 0.1750, 0.2177, 0.2166],\n",
      "          [0.1676, 0.2598, 0.1350, 0.2125, 0.2252],\n",
      "          [0.2348, 0.3062, 0.1729, 0.1663, 0.1199],\n",
      "          [0.2702, 0.1567, 0.1964, 0.1878, 0.1890],\n",
      "          [0.1922, 0.3318, 0.1494, 0.1482, 0.1783]],\n",
      "\n",
      "         [[0.2407, 0.1311, 0.2138, 0.2354, 0.1790],\n",
      "          [0.2306, 0.1649, 0.2234, 0.1884, 0.1927],\n",
      "          [0.1992, 0.2196, 0.2400, 0.1811, 0.1601],\n",
      "          [0.1124, 0.2205, 0.2039, 0.2372, 0.2258],\n",
      "          [0.1807, 0.3063, 0.2147, 0.1662, 0.1320]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0952, -0.3119, -0.0642,  0.4048,  0.1334],\n",
      "          [-0.4902, -0.5489,  0.1133, -0.2846, -0.1366],\n",
      "          [ 0.2470,  0.3495, -0.3281, -0.1345, -0.1619],\n",
      "          [ 0.0803,  0.0375,  0.1493,  0.0252,  0.3009],\n",
      "          [ 0.4436,  0.0297, -0.4964, -0.4036, -0.2285]],\n",
      "\n",
      "         [[-0.3012,  0.0330, -0.0439,  0.2471,  0.1701],\n",
      "          [-0.2651, -0.0277, -0.3931,  0.2428, -0.3300],\n",
      "          [-0.6099, -0.2339, -0.6019, -0.4911, -0.5255],\n",
      "          [ 0.0784, -0.1882, -0.4192, -0.4869, -0.7194],\n",
      "          [-0.2297, -0.3427, -0.4673, -0.1037, -0.5947]],\n",
      "\n",
      "         [[-0.5704, -0.2190, -0.2613, -0.2410, -0.1511],\n",
      "          [ 0.0777,  0.0614, -0.3449,  0.3434, -0.0431],\n",
      "          [-0.1796,  0.3868,  0.2570,  0.2022,  0.1991],\n",
      "          [-0.0651, -0.2357, -0.0537,  0.3470,  0.0640],\n",
      "          [-0.0624,  0.0808,  0.2095,  0.2959,  0.2771]],\n",
      "\n",
      "         [[ 0.5747,  0.0271, -0.4334, -0.4740, -0.3765],\n",
      "          [ 0.5327, -0.1715, -0.1604,  0.3163,  0.0285],\n",
      "          [-0.0723, -0.1819,  0.2387, -0.0387, -0.0767],\n",
      "          [-0.1830, -0.4231, -0.5513,  0.2758, -0.7060],\n",
      "          [ 0.2725, -0.3019, -0.5403,  0.1529, -0.3607]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2032, 0.1353, 0.1733, 0.2770, 0.2112],\n",
      "          [0.1557, 0.1468, 0.2846, 0.1912, 0.2217],\n",
      "          [0.2489, 0.2758, 0.1400, 0.1700, 0.1654],\n",
      "          [0.1915, 0.1834, 0.2051, 0.1812, 0.2387],\n",
      "          [0.3343, 0.2210, 0.1306, 0.1433, 0.1707]],\n",
      "\n",
      "         [[0.1424, 0.1989, 0.1842, 0.2464, 0.2281],\n",
      "          [0.1740, 0.2206, 0.1531, 0.2892, 0.1631],\n",
      "          [0.1761, 0.2565, 0.1775, 0.1983, 0.1916],\n",
      "          [0.2948, 0.2258, 0.1792, 0.1675, 0.1327],\n",
      "          [0.2217, 0.1980, 0.1748, 0.2515, 0.1539]],\n",
      "\n",
      "         [[0.1494, 0.2123, 0.2035, 0.2077, 0.2272],\n",
      "          [0.2070, 0.2037, 0.1357, 0.2701, 0.1835],\n",
      "          [0.1382, 0.2435, 0.2139, 0.2025, 0.2019],\n",
      "          [0.1817, 0.1532, 0.1838, 0.2744, 0.2068],\n",
      "          [0.1587, 0.1831, 0.2083, 0.2271, 0.2228]],\n",
      "\n",
      "         [[0.3731, 0.2158, 0.1362, 0.1307, 0.1441],\n",
      "          [0.2938, 0.1453, 0.1469, 0.2366, 0.1774],\n",
      "          [0.1890, 0.1694, 0.2580, 0.1955, 0.1882],\n",
      "          [0.2149, 0.1690, 0.1487, 0.3400, 0.1274],\n",
      "          [0.2920, 0.1644, 0.1295, 0.2591, 0.1550]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 2, 2])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0835, -0.0339],\n",
      "          [-0.1439, -0.2599]],\n",
      "\n",
      "         [[-0.1005, -0.2667],\n",
      "          [ 0.2388,  0.1429]],\n",
      "\n",
      "         [[ 0.2260, -0.0814],\n",
      "          [ 0.1880,  0.0574]],\n",
      "\n",
      "         [[ 0.7960,  0.5752],\n",
      "          [ 0.4023,  0.2997]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5293, 0.4707],\n",
      "          [0.5290, 0.4710]],\n",
      "\n",
      "         [[0.5415, 0.4585],\n",
      "          [0.5240, 0.4760]],\n",
      "\n",
      "         [[0.5762, 0.4238],\n",
      "          [0.5326, 0.4674]],\n",
      "\n",
      "         [[0.5550, 0.4450],\n",
      "          [0.5256, 0.4744]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 2, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2631,  0.5384,  0.6251,  0.0580,  0.3884],\n",
      "          [-0.2058,  0.3974,  0.4309,  0.1016,  0.4543]],\n",
      "\n",
      "         [[ 0.3782,  0.2442,  0.2052, -0.3718,  0.3570],\n",
      "          [-0.0230,  0.1594,  0.0186, -0.2548,  0.2299]],\n",
      "\n",
      "         [[-0.1531, -0.1647,  0.1363,  0.5207,  0.2140],\n",
      "          [-0.1433,  0.0851,  0.3363,  0.2845,  0.0275]],\n",
      "\n",
      "         [[-0.6794, -0.2623, -0.3397, -0.0541, -0.6474],\n",
      "          [-0.6101, -0.3195, -0.2979, -0.0177, -0.5206]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1117, 0.2489, 0.2714, 0.1539, 0.2142],\n",
      "          [0.1248, 0.2281, 0.2359, 0.1697, 0.2415]],\n",
      "\n",
      "         [[0.2400, 0.2099, 0.2019, 0.1134, 0.2349],\n",
      "          [0.1879, 0.2254, 0.1958, 0.1490, 0.2419]],\n",
      "\n",
      "         [[0.1486, 0.1469, 0.1985, 0.2915, 0.2145],\n",
      "          [0.1517, 0.1906, 0.2450, 0.2327, 0.1799]],\n",
      "\n",
      "         [[0.1466, 0.2224, 0.2058, 0.2739, 0.1513],\n",
      "          [0.1514, 0.2024, 0.2069, 0.2738, 0.1656]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 2, 2])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1479,  0.1433],\n",
      "          [ 0.2323,  0.2617]],\n",
      "\n",
      "         [[-0.4212, -0.1442],\n",
      "          [-0.5771, -0.2761]],\n",
      "\n",
      "         [[-0.1097, -0.0760],\n",
      "          [-0.3042, -0.3040]],\n",
      "\n",
      "         [[-0.0187,  0.0422],\n",
      "          [ 0.1225, -0.0644]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5011, 0.4989],\n",
      "          [0.4927, 0.5073]],\n",
      "\n",
      "         [[0.4312, 0.5688],\n",
      "          [0.4253, 0.5747]],\n",
      "\n",
      "         [[0.4916, 0.5084],\n",
      "          [0.5000, 0.5000]],\n",
      "\n",
      "         [[0.4848, 0.5152],\n",
      "          [0.5466, 0.4534]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 2, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0968,  0.1125,  0.0223, -0.2095, -0.1896],\n",
      "          [-0.4298, -0.1559, -0.0569, -0.5173, -0.1312]],\n",
      "\n",
      "         [[ 0.6093,  0.1367,  0.6895,  1.1011,  0.5707],\n",
      "          [ 0.6034,  0.3745,  0.8324,  0.8897,  0.8384]],\n",
      "\n",
      "         [[-0.0593, -0.5022,  0.1611,  0.1577, -0.0887],\n",
      "          [ 0.2022, -0.2135,  0.1061, -0.1119, -0.2275]],\n",
      "\n",
      "         [[ 0.0666,  0.0077,  0.6650, -0.0370,  0.0783],\n",
      "          [ 0.2803,  0.1367,  0.8018,  0.3686,  0.6513]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1936, 0.2387, 0.2181, 0.1730, 0.1765],\n",
      "          [0.1658, 0.2180, 0.2407, 0.1519, 0.2235]],\n",
      "\n",
      "         [[0.1885, 0.1175, 0.2043, 0.3083, 0.1814],\n",
      "          [0.1770, 0.1408, 0.2226, 0.2357, 0.2239]],\n",
      "\n",
      "         [[0.1960, 0.1259, 0.2443, 0.2435, 0.1903],\n",
      "          [0.2532, 0.1671, 0.2300, 0.1849, 0.1648]],\n",
      "\n",
      "         [[0.1762, 0.1661, 0.3205, 0.1589, 0.1783],\n",
      "          [0.1641, 0.1422, 0.2765, 0.1793, 0.2379]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 2, 2])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.3938, -0.2154],\n",
      "          [-0.5718, -0.2794]],\n",
      "\n",
      "         [[ 0.4582,  0.2752],\n",
      "          [ 0.5044,  0.2822]],\n",
      "\n",
      "         [[ 0.2814,  0.2803],\n",
      "          [ 0.3394,  0.3181]],\n",
      "\n",
      "         [[ 0.0425,  0.3628],\n",
      "          [ 0.1114,  0.4402]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4555, 0.5445],\n",
      "          [0.4274, 0.5726]],\n",
      "\n",
      "         [[0.5456, 0.4544],\n",
      "          [0.5553, 0.4447]],\n",
      "\n",
      "         [[0.5003, 0.4997],\n",
      "          [0.5053, 0.4947]],\n",
      "\n",
      "         [[0.4206, 0.5794],\n",
      "          [0.4185, 0.5815]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 2, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.4631, -0.2975,  0.7220,  0.1502,  0.0094],\n",
      "          [ 0.3963, -0.4135,  0.8233,  0.0753, -0.0234]],\n",
      "\n",
      "         [[ 0.0323, -0.5663, -0.5314, -0.2793, -0.0047],\n",
      "          [-0.0403, -0.6695, -0.7266, -0.4894, -0.0864]],\n",
      "\n",
      "         [[-0.0166,  0.0027,  0.3905, -0.1870,  0.0482],\n",
      "          [-0.2017, -0.0821,  0.4436, -0.0946,  0.0342]],\n",
      "\n",
      "         [[ 0.0758, -0.2306, -0.2013, -0.1080, -0.0024],\n",
      "          [ 0.0796, -0.2999, -0.1828, -0.2997,  0.0462]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2422, 0.1132, 0.3137, 0.1771, 0.1538],\n",
      "          [0.2293, 0.1020, 0.3515, 0.1664, 0.1507]],\n",
      "\n",
      "         [[0.2622, 0.1441, 0.1492, 0.1920, 0.2526],\n",
      "          [0.2755, 0.1468, 0.1387, 0.1758, 0.2631]],\n",
      "\n",
      "         [[0.1841, 0.1877, 0.2766, 0.1553, 0.1964],\n",
      "          [0.1559, 0.1758, 0.2973, 0.1736, 0.1974]],\n",
      "\n",
      "         [[0.2352, 0.1732, 0.1783, 0.1957, 0.2175],\n",
      "          [0.2436, 0.1667, 0.1874, 0.1667, 0.2356]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1317, -0.2061,  0.0739, -0.5270, -0.2601],\n",
      "          [ 0.3988,  0.0687,  0.6207,  0.7019,  0.6455],\n",
      "          [ 0.2564, -0.4492, -0.0841,  0.0299, -0.5582],\n",
      "          [-0.1724,  0.5743,  0.2094,  1.1049,  0.2976],\n",
      "          [-0.0273, -0.1392,  0.8159,  0.2963,  0.1721]],\n",
      "\n",
      "         [[ 0.4385,  1.1028,  0.4459,  0.1204, -0.3613],\n",
      "          [-0.0176,  0.2536, -0.2354,  0.4587,  0.1451],\n",
      "          [ 0.6323, -0.3496,  0.1500,  0.3079, -0.2106],\n",
      "          [-0.1041,  0.2377,  0.5801,  0.7351, -0.0735],\n",
      "          [ 0.0499,  0.3882,  0.6670,  0.8309,  0.0862]],\n",
      "\n",
      "         [[ 0.6082,  0.3925,  0.1870,  0.4059,  0.5149],\n",
      "          [ 0.5330, -0.3026, -0.9638, -0.0892, -0.2921],\n",
      "          [ 0.3225, -0.0670,  0.1036,  0.7641,  0.2302],\n",
      "          [-0.5977,  0.3316,  0.9108,  0.3601, -0.2686],\n",
      "          [ 0.4499,  0.1976, -0.0306,  0.2386,  0.1796]],\n",
      "\n",
      "         [[ 0.2222, -0.1163, -0.2620, -1.0565, -0.2892],\n",
      "          [-0.0867, -0.4356, -0.3169, -0.2066, -0.8414],\n",
      "          [-1.3500, -1.8715, -1.5101, -1.1686, -1.1780],\n",
      "          [-0.1240, -0.7172, -0.7187,  0.2672, -1.0246],\n",
      "          [-0.4119, -0.5978, -0.4870, -0.3623, -0.7531]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2597, 0.1853, 0.2451, 0.1344, 0.1755],\n",
      "          [0.1785, 0.1283, 0.2229, 0.2417, 0.2285],\n",
      "          [0.2902, 0.1433, 0.2065, 0.2314, 0.1285],\n",
      "          [0.1024, 0.2162, 0.1501, 0.3674, 0.1639],\n",
      "          [0.1466, 0.1311, 0.3407, 0.2026, 0.1790]],\n",
      "\n",
      "         [[0.1950, 0.3790, 0.1965, 0.1419, 0.0877],\n",
      "          [0.1694, 0.2222, 0.1363, 0.2728, 0.1994],\n",
      "          [0.3179, 0.1191, 0.1963, 0.2298, 0.1369],\n",
      "          [0.1293, 0.1819, 0.2562, 0.2992, 0.1333],\n",
      "          [0.1337, 0.1876, 0.2479, 0.2921, 0.1387]],\n",
      "\n",
      "         [[0.2387, 0.1924, 0.1566, 0.1950, 0.2174],\n",
      "          [0.3799, 0.1647, 0.0850, 0.2039, 0.1665],\n",
      "          [0.2021, 0.1369, 0.1624, 0.3143, 0.1843],\n",
      "          [0.0830, 0.2102, 0.3751, 0.2163, 0.1153],\n",
      "          [0.2520, 0.1958, 0.1559, 0.2040, 0.1923]],\n",
      "\n",
      "         [[0.3118, 0.2223, 0.1921, 0.0868, 0.1870],\n",
      "          [0.2593, 0.1829, 0.2059, 0.2300, 0.1219],\n",
      "          [0.2069, 0.1228, 0.1763, 0.2481, 0.2458],\n",
      "          [0.2507, 0.1385, 0.1383, 0.3707, 0.1018],\n",
      "          [0.2212, 0.1837, 0.2053, 0.2325, 0.1573]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-5.2250e-04, -3.4038e-01,  1.9606e-01,  3.1710e-02, -3.2984e-02],\n",
      "          [ 7.4151e-01, -6.1643e-02, -5.4645e-01,  8.6858e-02, -6.5896e-02],\n",
      "          [-3.1681e-01, -5.0635e-02,  3.3133e-01,  3.6711e-01,  4.7187e-01],\n",
      "          [ 3.2869e-01,  3.8478e-02, -4.0063e-01,  6.1564e-02, -1.5882e-02],\n",
      "          [ 2.0730e-01,  3.7284e-01, -2.1274e-01,  3.1455e-01,  4.4475e-01]],\n",
      "\n",
      "         [[ 2.5957e-01, -6.7090e-02,  4.3853e-01,  1.4712e-01,  3.1100e-02],\n",
      "          [ 5.3793e-01,  3.0752e-01,  1.7881e-01,  1.8472e-01,  1.5987e-02],\n",
      "          [-3.1595e-02, -3.5354e-01, -1.3729e-01,  1.1187e-01, -2.9930e-01],\n",
      "          [-1.3466e-01,  1.8526e-01, -5.1138e-02,  4.1536e-01, -2.4171e-01],\n",
      "          [ 6.3078e-01, -2.0812e-01,  6.0053e-02, -7.4107e-02, -3.0021e-01]],\n",
      "\n",
      "         [[-1.3614e-01,  1.1237e-01, -2.3150e-01, -2.8423e-01,  2.7922e-01],\n",
      "          [-5.2634e-01,  3.6230e-01, -7.3396e-01, -3.0162e-01, -1.3257e-01],\n",
      "          [ 1.6724e-01,  4.4397e-01,  1.5696e-02,  4.2186e-02, -5.9102e-02],\n",
      "          [ 9.0797e-01,  5.4416e-01,  3.3723e-01, -4.2034e-01,  4.5978e-01],\n",
      "          [ 7.6676e-03,  6.6865e-01,  4.0317e-01, -8.1592e-03, -3.9783e-02]],\n",
      "\n",
      "         [[ 2.5156e-01, -1.7508e-01, -2.4254e-01,  3.9073e-01,  1.7152e-01],\n",
      "          [ 5.2618e-01, -3.6376e-03, -1.6710e-01,  4.7912e-01, -3.4974e-01],\n",
      "          [-1.6691e-01, -1.9727e-01,  4.9962e-02,  4.2368e-01, -2.4794e-01],\n",
      "          [-5.9650e-01, -1.1739e-01,  3.4284e-01,  4.5516e-01,  4.6924e-02],\n",
      "          [-1.9714e-01,  1.4216e-01,  2.1043e-02,  1.7223e-01, -5.8993e-01]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2028, 0.1444, 0.2469, 0.2095, 0.1964],\n",
      "          [0.3718, 0.1665, 0.1026, 0.1932, 0.1658],\n",
      "          [0.1191, 0.1554, 0.2276, 0.2359, 0.2620],\n",
      "          [0.2699, 0.2019, 0.1302, 0.2067, 0.1913],\n",
      "          [0.1916, 0.2261, 0.1259, 0.2133, 0.2430]],\n",
      "\n",
      "         [[0.2171, 0.1566, 0.2596, 0.1940, 0.1727],\n",
      "          [0.2640, 0.2096, 0.1843, 0.1854, 0.1566],\n",
      "          [0.2201, 0.1595, 0.1980, 0.2540, 0.1684],\n",
      "          [0.1640, 0.2259, 0.1783, 0.2843, 0.1474],\n",
      "          [0.3465, 0.1498, 0.1958, 0.1713, 0.1366]],\n",
      "\n",
      "         [[0.1796, 0.2302, 0.1632, 0.1549, 0.2721],\n",
      "          [0.1433, 0.3485, 0.1164, 0.1794, 0.2124],\n",
      "          [0.2058, 0.2715, 0.1769, 0.1816, 0.1641],\n",
      "          [0.3161, 0.2197, 0.1786, 0.0837, 0.2019],\n",
      "          [0.1572, 0.3045, 0.2335, 0.1548, 0.1500]],\n",
      "\n",
      "         [[0.2307, 0.1506, 0.1407, 0.2651, 0.2129],\n",
      "          [0.2891, 0.1702, 0.1445, 0.2758, 0.1204],\n",
      "          [0.1684, 0.1633, 0.2091, 0.3039, 0.1553],\n",
      "          [0.1006, 0.1625, 0.2574, 0.2880, 0.1915],\n",
      "          [0.1733, 0.2433, 0.2156, 0.2508, 0.1170]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-4.2441e-02,  3.9161e-02,  4.5027e-02,  1.7607e-01,  4.4166e-01],\n",
      "          [-1.2500e-01,  9.2328e-05, -4.3896e-03, -1.7410e-01, -6.6638e-02],\n",
      "          [ 6.8270e-02,  1.8272e-01, -2.9512e-01, -9.8248e-02, -7.7594e-02],\n",
      "          [ 6.4206e-02,  4.5735e-01,  5.0428e-01,  3.3272e-01,  1.6994e-01],\n",
      "          [ 5.9729e-01,  2.0959e-01, -3.6562e-01, -1.0410e-01, -6.2499e-03]],\n",
      "\n",
      "         [[-9.8145e-03,  1.4516e-01,  1.6222e-01,  9.9793e-02,  1.5962e-01],\n",
      "          [ 4.8957e-02, -1.7694e-01, -5.1150e-01,  5.6206e-01, -3.2806e-01],\n",
      "          [-3.6468e-01, -1.1548e-01, -7.2438e-01, -2.4862e-01, -4.2851e-01],\n",
      "          [ 4.0391e-02, -3.9951e-03, -2.0749e-01, -2.1291e-01, -1.9798e-01],\n",
      "          [-3.8870e-01, -2.9118e-01, -3.3283e-01,  1.4132e-01, -2.6508e-01]],\n",
      "\n",
      "         [[-4.5208e-01, -2.3575e-01, -3.6326e-01,  2.9609e-01, -2.9657e-01],\n",
      "          [-2.1543e-01, -2.3018e-01,  3.6894e-02, -2.9640e-02,  9.9743e-02],\n",
      "          [-5.0756e-02,  4.2218e-01,  5.5792e-02,  5.3951e-01,  3.2789e-01],\n",
      "          [-2.1567e-01, -3.8193e-02, -1.1108e-01,  2.1731e-01,  2.2236e-01],\n",
      "          [-1.8581e-01,  1.8207e-01,  3.8575e-01,  7.2952e-02,  3.1003e-01]],\n",
      "\n",
      "         [[ 3.3848e-01,  1.2359e-01, -5.8257e-01, -1.1710e-01, -9.5140e-01],\n",
      "          [ 2.7249e-01, -9.3661e-02, -4.3728e-01, -1.8452e-02, -3.4681e-01],\n",
      "          [ 7.2095e-02, -4.6422e-02,  3.2869e-01,  4.7397e-02,  2.1203e-03],\n",
      "          [-2.9336e-01, -6.5835e-01, -4.4360e-01, -1.9097e-01, -7.6378e-01],\n",
      "          [ 5.7437e-01, -7.6218e-01, -8.0042e-01,  1.2575e-01, -8.1905e-01]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1655, 0.1795, 0.1806, 0.2059, 0.2685],\n",
      "          [0.1896, 0.2149, 0.2139, 0.1805, 0.2010],\n",
      "          [0.2209, 0.2477, 0.1536, 0.1870, 0.1909],\n",
      "          [0.1549, 0.2296, 0.2406, 0.2027, 0.1722],\n",
      "          [0.3223, 0.2187, 0.1230, 0.1598, 0.1762]],\n",
      "\n",
      "         [[0.1768, 0.2064, 0.2100, 0.1973, 0.2095],\n",
      "          [0.2116, 0.1688, 0.1208, 0.3535, 0.1452],\n",
      "          [0.1983, 0.2545, 0.1384, 0.2227, 0.1861],\n",
      "          [0.2325, 0.2224, 0.1815, 0.1805, 0.1832],\n",
      "          [0.1669, 0.1840, 0.1765, 0.2836, 0.1889]],\n",
      "\n",
      "         [[0.1512, 0.1877, 0.1652, 0.3194, 0.1766],\n",
      "          [0.1710, 0.1685, 0.2201, 0.2059, 0.2344],\n",
      "          [0.1432, 0.2298, 0.1593, 0.2584, 0.2092],\n",
      "          [0.1563, 0.1867, 0.1736, 0.2411, 0.2423],\n",
      "          [0.1398, 0.2020, 0.2476, 0.1811, 0.2295]],\n",
      "\n",
      "         [[0.3211, 0.2590, 0.1278, 0.2036, 0.0884],\n",
      "          [0.2881, 0.1998, 0.1417, 0.2154, 0.1551],\n",
      "          [0.1965, 0.1745, 0.2540, 0.1917, 0.1832],\n",
      "          [0.2332, 0.1619, 0.2007, 0.2584, 0.1457],\n",
      "          [0.4163, 0.1094, 0.1053, 0.2658, 0.1033]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-2.0763e-01, -5.0008e-01, -5.6383e-01, -4.5365e-01],\n",
      "          [-2.9546e-02, -1.4052e-01, -3.7220e-01, -9.7320e-03],\n",
      "          [-2.2042e-01, -4.7324e-01, -4.2011e-01, -2.7383e-01],\n",
      "          [-6.3582e-01, -7.2786e-01, -9.7678e-01, -6.9612e-01]],\n",
      "\n",
      "         [[-3.6541e-04,  5.6057e-01,  6.0325e-02,  1.4748e-01],\n",
      "          [ 4.0984e-01,  5.8828e-01,  6.9276e-02,  3.0304e-01],\n",
      "          [-1.3536e-02,  8.3968e-02, -5.8860e-01, -4.7324e-02],\n",
      "          [ 2.3802e-01,  2.8126e-01, -2.8030e-01,  1.3923e-01]],\n",
      "\n",
      "         [[-2.0211e-01, -4.1269e-01, -2.0032e-02, -7.6403e-01],\n",
      "          [ 1.3954e-01, -2.2207e-01,  1.1191e-01, -6.2145e-01],\n",
      "          [ 3.4692e-01,  2.4367e-01,  4.8051e-01, -3.6517e-01],\n",
      "          [ 3.7867e-01,  3.5553e-01,  4.7649e-01, -7.2652e-02]],\n",
      "\n",
      "         [[ 6.3121e-01,  6.6115e-01,  6.0074e-01,  3.6944e-01],\n",
      "          [ 2.8259e-01,  3.7174e-01,  2.1686e-01,  8.0422e-03],\n",
      "          [ 4.5755e-01,  6.4678e-01,  3.8813e-01,  2.5676e-01],\n",
      "          [ 4.7520e-01,  4.6805e-01,  1.6648e-01,  4.0040e-02]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3097, 0.2312, 0.2169, 0.2422],\n",
      "          [0.2759, 0.2469, 0.1958, 0.2814],\n",
      "          [0.2822, 0.2192, 0.2311, 0.2675],\n",
      "          [0.2805, 0.2559, 0.1995, 0.2641]],\n",
      "\n",
      "         [[0.2010, 0.3523, 0.2136, 0.2331],\n",
      "          [0.2628, 0.3141, 0.1869, 0.2362],\n",
      "          [0.2753, 0.3035, 0.1549, 0.2662],\n",
      "          [0.2820, 0.2945, 0.1680, 0.2555]],\n",
      "\n",
      "         [[0.2793, 0.2263, 0.3351, 0.1593],\n",
      "          [0.3188, 0.2221, 0.3101, 0.1490],\n",
      "          [0.2828, 0.2551, 0.3233, 0.1388],\n",
      "          [0.2691, 0.2629, 0.2967, 0.1713]],\n",
      "\n",
      "         [[0.2652, 0.2733, 0.2573, 0.2042],\n",
      "          [0.2639, 0.2885, 0.2471, 0.2005],\n",
      "          [0.2526, 0.3052, 0.2356, 0.2066],\n",
      "          [0.2963, 0.2942, 0.2176, 0.1918]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 4, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0164,  0.3422,  0.2642, -0.2999,  0.5021],\n",
      "          [-0.1354,  0.1233,  0.1980, -0.1440,  0.3867],\n",
      "          [-0.0928,  0.1975,  0.1432, -0.1880,  0.2515],\n",
      "          [ 0.0844,  0.2012,  0.2806, -0.1995,  0.5349]],\n",
      "\n",
      "         [[ 0.4646,  0.2699,  0.2069, -0.0567,  0.7686],\n",
      "          [ 0.4843,  0.0243,  0.0749, -0.2626,  0.4502],\n",
      "          [ 0.1609, -0.0319, -0.0978, -0.2290,  0.4154],\n",
      "          [ 0.4466,  0.0651,  0.2689,  0.0208,  0.6441]],\n",
      "\n",
      "         [[-0.0677, -0.1893, -0.1232,  0.1843,  0.0364],\n",
      "          [ 0.0085, -0.1966,  0.3190,  0.4742,  0.1977],\n",
      "          [ 0.1307, -0.1221,  0.3236,  0.4907,  0.1246],\n",
      "          [ 0.2079, -0.0431,  0.4503,  0.4758,  0.3107]],\n",
      "\n",
      "         [[-0.6514, -0.2454, -0.0812,  0.2015, -0.4863],\n",
      "          [-0.7303, -0.1880,  0.1605,  0.2941, -0.5862],\n",
      "          [-0.9862, -0.3705,  0.0766,  0.0252, -0.7149],\n",
      "          [-0.7926, -0.3592, -0.0688,  0.0265, -0.6891]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1661, 0.2301, 0.2128, 0.1211, 0.2700],\n",
      "          [0.1570, 0.2034, 0.2192, 0.1557, 0.2647],\n",
      "          [0.1688, 0.2257, 0.2138, 0.1535, 0.2382],\n",
      "          [0.1766, 0.1985, 0.2149, 0.1330, 0.2771]],\n",
      "\n",
      "         [[0.2200, 0.1811, 0.1700, 0.1306, 0.2982],\n",
      "          [0.2677, 0.1690, 0.1778, 0.1268, 0.2587],\n",
      "          [0.2191, 0.1807, 0.1692, 0.1484, 0.2826],\n",
      "          [0.2277, 0.1555, 0.1906, 0.1488, 0.2774]],\n",
      "\n",
      "         [[0.1913, 0.1694, 0.1809, 0.2461, 0.2123],\n",
      "          [0.1672, 0.1362, 0.2281, 0.2664, 0.2020],\n",
      "          [0.1846, 0.1434, 0.2239, 0.2646, 0.1835],\n",
      "          [0.1829, 0.1423, 0.2331, 0.2391, 0.2027]],\n",
      "\n",
      "         [[0.1283, 0.1925, 0.2269, 0.3010, 0.1513],\n",
      "          [0.1099, 0.1891, 0.2679, 0.3062, 0.1270],\n",
      "          [0.1020, 0.1887, 0.2952, 0.2804, 0.1338],\n",
      "          [0.1253, 0.1932, 0.2584, 0.2842, 0.1389]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.3664, -0.1327,  0.0893,  0.0485],\n",
      "          [ 0.4809,  0.0370,  0.2164,  0.2563],\n",
      "          [ 0.4016, -0.0829,  0.1351,  0.0608],\n",
      "          [ 0.1801, -0.1042, -0.0700,  0.0603]],\n",
      "\n",
      "         [[-0.2660, -0.3013, -0.3845, -0.3168],\n",
      "          [-0.3053, -0.4189, -0.4815, -0.2742],\n",
      "          [-0.3395, -0.3576, -0.4731, -0.5063],\n",
      "          [-0.1416, -0.2382, -0.3463, -0.2452]],\n",
      "\n",
      "         [[-0.0040,  0.0845,  0.0138,  0.0581],\n",
      "          [-0.2589, -0.1862, -0.2071, -0.1297],\n",
      "          [-0.1724, -0.0907,  0.0010, -0.0538],\n",
      "          [-0.1422, -0.1483, -0.0749, -0.0102]],\n",
      "\n",
      "         [[-0.0413,  0.0756, -0.0630,  0.1447],\n",
      "          [ 0.0993,  0.2207,  0.0711,  0.2889],\n",
      "          [ 0.1755,  0.2469,  0.0535,  0.3250],\n",
      "          [ 0.2793,  0.2650,  0.0578,  0.3988]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3233, 0.1963, 0.2451, 0.2353],\n",
      "          [0.3117, 0.2000, 0.2393, 0.2490],\n",
      "          [0.3233, 0.1992, 0.2477, 0.2299],\n",
      "          [0.2925, 0.2201, 0.2278, 0.2595]],\n",
      "\n",
      "         [[0.2629, 0.2538, 0.2335, 0.2499],\n",
      "          [0.2658, 0.2372, 0.2228, 0.2742],\n",
      "          [0.2700, 0.2652, 0.2362, 0.2285],\n",
      "          [0.2759, 0.2505, 0.2248, 0.2487]],\n",
      "\n",
      "         [[0.2395, 0.2617, 0.2439, 0.2549],\n",
      "          [0.2344, 0.2521, 0.2468, 0.2667],\n",
      "          [0.2272, 0.2466, 0.2703, 0.2559],\n",
      "          [0.2378, 0.2364, 0.2544, 0.2714]],\n",
      "\n",
      "         [[0.2322, 0.2610, 0.2272, 0.2796],\n",
      "          [0.2320, 0.2620, 0.2256, 0.2804],\n",
      "          [0.2427, 0.2607, 0.2148, 0.2818],\n",
      "          [0.2555, 0.2519, 0.2047, 0.2879]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 4, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.3348, -0.0264, -0.1102, -0.5144, -0.2852],\n",
      "          [-0.2897, -0.0027,  0.2795, -0.6808, -0.2749],\n",
      "          [-0.3480,  0.0370,  0.2350, -0.5262, -0.0885],\n",
      "          [-0.5260, -0.0400, -0.0136, -0.6156, -0.3615]],\n",
      "\n",
      "         [[ 0.6880, -0.0431,  0.4885,  1.0031,  0.2060],\n",
      "          [ 0.5013,  0.1401,  0.3716,  1.1284,  0.3346],\n",
      "          [ 0.5731,  0.3817,  0.5215,  0.8989,  0.3808],\n",
      "          [ 0.5665,  0.1454,  0.4267,  0.9285,  0.4153]],\n",
      "\n",
      "         [[-0.0735, -0.2975, -0.1367, -0.3160, -0.7349],\n",
      "          [ 0.1085, -0.0472,  0.1407, -0.0986, -0.3242],\n",
      "          [ 0.2030, -0.0752, -0.0013, -0.0558, -0.1327],\n",
      "          [ 0.2053, -0.0099,  0.1056, -0.0631, -0.2491]],\n",
      "\n",
      "         [[ 0.0668, -0.0529,  0.6410, -0.0516,  0.2707],\n",
      "          [ 0.1353,  0.0353,  0.7850,  0.3963,  0.3741],\n",
      "          [ 0.1597, -0.0618,  0.7405,  0.4894,  0.4089],\n",
      "          [ 0.1008,  0.0078,  0.6811,  0.2084,  0.5461]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1818, 0.2475, 0.2276, 0.1519, 0.1911],\n",
      "          [0.1727, 0.2301, 0.3051, 0.1168, 0.1753],\n",
      "          [0.1564, 0.2298, 0.2802, 0.1309, 0.2027],\n",
      "          [0.1565, 0.2545, 0.2613, 0.1431, 0.1845]],\n",
      "\n",
      "         [[0.2332, 0.1122, 0.1910, 0.3196, 0.1440],\n",
      "          [0.1889, 0.1316, 0.1659, 0.3536, 0.1599],\n",
      "          [0.2006, 0.1656, 0.1905, 0.2778, 0.1655],\n",
      "          [0.2074, 0.1361, 0.1803, 0.2979, 0.1783]],\n",
      "\n",
      "         [[0.2476, 0.1979, 0.2324, 0.1943, 0.1278],\n",
      "          [0.2299, 0.1967, 0.2374, 0.1869, 0.1491],\n",
      "          [0.2464, 0.1865, 0.2008, 0.1902, 0.1761],\n",
      "          [0.2433, 0.1961, 0.2202, 0.1860, 0.1544]],\n",
      "\n",
      "         [[0.1731, 0.1536, 0.3074, 0.1538, 0.2122],\n",
      "          [0.1566, 0.1417, 0.2998, 0.2032, 0.1988],\n",
      "          [0.1597, 0.1280, 0.2854, 0.2221, 0.2049],\n",
      "          [0.1569, 0.1430, 0.2804, 0.1748, 0.2450]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2799, -0.4956, -0.6991, -0.5773],\n",
      "          [-0.2961, -0.5976, -0.6961, -0.7058],\n",
      "          [-0.1640, -0.2669, -0.5211, -0.4523],\n",
      "          [-0.1488, -0.2622, -0.4546, -0.2337]],\n",
      "\n",
      "         [[ 0.3722,  0.4459,  0.3676,  0.5731],\n",
      "          [ 0.4301,  0.5462,  0.3528,  0.5219],\n",
      "          [ 0.2302,  0.3924,  0.2877,  0.3636],\n",
      "          [ 0.4769,  0.6009,  0.5218,  0.5276]],\n",
      "\n",
      "         [[ 0.0807,  0.2285,  0.3310,  0.3192],\n",
      "          [ 0.0503,  0.0124,  0.0768,  0.0850],\n",
      "          [-0.0503, -0.2029, -0.0593,  0.0842],\n",
      "          [-0.0836, -0.1463,  0.0741,  0.0376]],\n",
      "\n",
      "         [[ 0.3822,  0.2991,  0.3427,  0.3236],\n",
      "          [ 0.5953,  0.4855,  0.4415,  0.5741],\n",
      "          [ 0.4427,  0.3930,  0.3656,  0.4232],\n",
      "          [ 0.3661,  0.2905,  0.2503,  0.2936]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3119, 0.2514, 0.2051, 0.2316],\n",
      "          [0.3253, 0.2407, 0.2181, 0.2160],\n",
      "          [0.2984, 0.2692, 0.2088, 0.2237],\n",
      "          [0.2818, 0.2517, 0.2076, 0.2589]],\n",
      "\n",
      "         [[0.2329, 0.2507, 0.2318, 0.2847],\n",
      "          [0.2413, 0.2710, 0.2233, 0.2645],\n",
      "          [0.2284, 0.2686, 0.2419, 0.2610],\n",
      "          [0.2364, 0.2676, 0.2473, 0.2487]],\n",
      "\n",
      "         [[0.2122, 0.2460, 0.2725, 0.2693],\n",
      "          [0.2485, 0.2392, 0.2551, 0.2572],\n",
      "          [0.2504, 0.2150, 0.2482, 0.2865],\n",
      "          [0.2359, 0.2216, 0.2762, 0.2663]],\n",
      "\n",
      "         [[0.2615, 0.2406, 0.2513, 0.2466],\n",
      "          [0.2679, 0.2401, 0.2297, 0.2623],\n",
      "          [0.2592, 0.2466, 0.2400, 0.2542],\n",
      "          [0.2668, 0.2474, 0.2376, 0.2482]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 4, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1541, -0.5848,  0.2683, -0.3035, -0.2517],\n",
      "          [ 0.1388, -0.5582,  0.2996, -0.1100, -0.0593],\n",
      "          [ 0.1612, -0.4709,  0.7540,  0.0747,  0.0305],\n",
      "          [ 0.0016, -0.6124,  0.4792,  0.0536, -0.2689]],\n",
      "\n",
      "         [[-0.4107, -0.8633, -0.3950, -0.3775, -0.2562],\n",
      "          [-0.1843, -0.7061, -0.5921, -0.0838, -0.1209],\n",
      "          [-0.0220, -0.8032, -0.4757, -0.1713, -0.1232],\n",
      "          [-0.0890, -0.7545, -0.4503,  0.0514,  0.0107]],\n",
      "\n",
      "         [[ 0.0605,  0.0799,  0.0732, -0.2581,  0.3247],\n",
      "          [-0.0435, -0.0484,  0.0713, -0.3807,  0.2472],\n",
      "          [-0.2102, -0.3106, -0.1140, -0.4275, -0.0159],\n",
      "          [ 0.0630, -0.0327,  0.0161, -0.5461,  0.0656]],\n",
      "\n",
      "         [[-0.2434, -0.1967, -0.3714, -0.2672,  0.0363],\n",
      "          [-0.3731, -0.0839, -0.5213, -0.1444, -0.0358],\n",
      "          [ 0.1325,  0.0818, -0.4601, -0.1162, -0.0507],\n",
      "          [ 0.0037, -0.1149, -0.4227, -0.1320,  0.1754]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2566, 0.1225, 0.2876, 0.1623, 0.1710],\n",
      "          [0.2341, 0.1166, 0.2749, 0.1825, 0.1920],\n",
      "          [0.1947, 0.1035, 0.3523, 0.1786, 0.1709],\n",
      "          [0.2012, 0.1089, 0.3244, 0.2120, 0.1535]],\n",
      "\n",
      "         [[0.2061, 0.1311, 0.2093, 0.2130, 0.2405],\n",
      "          [0.2258, 0.1340, 0.1501, 0.2496, 0.2405],\n",
      "          [0.2592, 0.1187, 0.1647, 0.2232, 0.2342],\n",
      "          [0.2239, 0.1151, 0.1560, 0.2576, 0.2474]],\n",
      "\n",
      "         [[0.1976, 0.2014, 0.2001, 0.1437, 0.2573],\n",
      "          [0.1935, 0.1926, 0.2170, 0.1381, 0.2588],\n",
      "          [0.1990, 0.1800, 0.2191, 0.1601, 0.2417],\n",
      "          [0.2268, 0.2061, 0.2164, 0.1233, 0.2274]],\n",
      "\n",
      "         [[0.1913, 0.2005, 0.1683, 0.1868, 0.2531],\n",
      "          [0.1708, 0.2280, 0.1472, 0.2147, 0.2393],\n",
      "          [0.2430, 0.2310, 0.1343, 0.1895, 0.2023],\n",
      "          [0.2173, 0.1930, 0.1419, 0.1897, 0.2580]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0915, -0.0066, -0.2452, -0.1914, -0.0568],\n",
      "          [ 0.0189, -0.0140,  0.3464,  1.0454,  0.2595],\n",
      "          [-0.4529, -0.6335, -0.1177,  0.2130, -0.6858],\n",
      "          [-0.2356,  0.8250,  0.2038,  1.3375,  0.1286],\n",
      "          [-0.0838, -0.5503,  0.8487,  0.2419, -0.0810]],\n",
      "\n",
      "         [[ 0.8073,  0.9674, -0.0360,  0.7809,  0.3918],\n",
      "          [ 0.8738,  0.7981, -0.0981,  0.8027,  0.1291],\n",
      "          [ 0.2960,  0.1703, -0.0699,  0.9557,  0.0969],\n",
      "          [ 0.1584,  0.1226,  0.5311,  0.7341,  0.5806],\n",
      "          [ 0.3638,  0.8306,  0.5024,  1.0825,  0.4361]],\n",
      "\n",
      "         [[ 0.1628,  0.1661, -0.0398,  0.8543,  0.1966],\n",
      "          [ 0.7419, -0.3950, -0.7681, -0.0729, -0.2005],\n",
      "          [ 0.2024,  0.1035,  0.2469,  1.0570,  0.1443],\n",
      "          [ 0.1824,  0.2301, -0.0354,  0.1576,  0.0316],\n",
      "          [ 0.2941,  0.1779,  0.1018,  0.3587,  0.2368]],\n",
      "\n",
      "         [[-0.1830, -0.2376, -0.6197, -1.0461, -0.0902],\n",
      "          [-0.2556,  0.1323, -0.4594, -0.2677, -0.4586],\n",
      "          [-1.2076, -1.9608, -1.5565, -1.1410, -1.1859],\n",
      "          [-0.0861, -0.7520, -1.2935, -0.0244, -1.2020],\n",
      "          [-0.4563, -1.0767, -0.9912, -0.4607, -0.7460]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2360, 0.2140, 0.1686, 0.1779, 0.2035],\n",
      "          [0.1348, 0.1304, 0.1870, 0.3763, 0.1715],\n",
      "          [0.1675, 0.1398, 0.2342, 0.3259, 0.1327],\n",
      "          [0.0855, 0.2468, 0.1326, 0.4121, 0.1230],\n",
      "          [0.1525, 0.0957, 0.3876, 0.2113, 0.1530]],\n",
      "\n",
      "         [[0.2360, 0.2769, 0.1015, 0.2298, 0.1557],\n",
      "          [0.2694, 0.2498, 0.1019, 0.2509, 0.1279],\n",
      "          [0.1876, 0.1655, 0.1301, 0.3629, 0.1538],\n",
      "          [0.1488, 0.1436, 0.2160, 0.2646, 0.2270],\n",
      "          [0.1456, 0.2321, 0.1672, 0.2986, 0.1565]],\n",
      "\n",
      "         [[0.1709, 0.1715, 0.1396, 0.3413, 0.1768],\n",
      "          [0.4212, 0.1351, 0.0930, 0.1865, 0.1641],\n",
      "          [0.1601, 0.1450, 0.1674, 0.3763, 0.1511],\n",
      "          [0.2133, 0.2237, 0.1715, 0.2081, 0.1834],\n",
      "          [0.2116, 0.1884, 0.1746, 0.2257, 0.1998]],\n",
      "\n",
      "         [[0.2432, 0.2303, 0.1571, 0.1026, 0.2668],\n",
      "          [0.1963, 0.2893, 0.1601, 0.1940, 0.1602],\n",
      "          [0.2344, 0.1104, 0.1653, 0.2505, 0.2395],\n",
      "          [0.3121, 0.1604, 0.0933, 0.3320, 0.1023],\n",
      "          [0.2586, 0.1390, 0.1514, 0.2574, 0.1935]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2454, -0.3985, -0.0502, -0.0448,  0.0628],\n",
      "          [ 0.6432, -0.2321, -0.3276,  0.3257,  0.1441],\n",
      "          [-0.2443, -0.2705,  0.0511,  0.4086,  0.2608],\n",
      "          [ 0.3999,  0.0451, -0.6418,  0.0584,  0.1046],\n",
      "          [ 0.1992,  0.0931, -0.3023,  0.1974,  0.2679]],\n",
      "\n",
      "         [[ 0.5786, -0.1176,  0.1747, -0.0391,  0.1061],\n",
      "          [ 0.5124,  0.2061, -0.5070,  0.1599,  0.0890],\n",
      "          [-0.0735, -0.0921,  0.0143, -0.2627, -0.2131],\n",
      "          [ 0.2083,  0.0570, -0.1717,  0.3084,  0.1007],\n",
      "          [ 0.4108, -0.2989, -0.1631, -0.4351, -0.1482]],\n",
      "\n",
      "         [[ 0.0025,  0.3299, -0.2688, -0.2851, -0.0491],\n",
      "          [-0.0511,  0.2182, -0.4276, -0.4404, -0.5434],\n",
      "          [ 0.0764,  0.3993, -0.1897,  0.0475, -0.6460],\n",
      "          [ 0.8524,  0.3246,  0.6832, -0.2732,  0.3215],\n",
      "          [ 0.1707,  0.5709,  0.2070, -0.5010, -0.3946]],\n",
      "\n",
      "         [[ 0.4291, -0.0080,  0.2331,  0.3381,  0.3741],\n",
      "          [ 0.5014,  0.0252, -0.1741,  0.1995,  0.1479],\n",
      "          [-0.0794, -0.0976, -0.1507,  0.1340, -0.3887],\n",
      "          [-0.6100, -0.0341, -0.1228,  0.6526,  0.1159],\n",
      "          [ 0.0091,  0.3734, -0.0402, -0.0525, -0.2677]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1768, 0.1517, 0.2149, 0.2161, 0.2406],\n",
      "          [0.3194, 0.1331, 0.1210, 0.2325, 0.1939],\n",
      "          [0.1450, 0.1413, 0.1948, 0.2786, 0.2403],\n",
      "          [0.2850, 0.1999, 0.1006, 0.2025, 0.2121],\n",
      "          [0.2186, 0.1966, 0.1324, 0.2182, 0.2342]],\n",
      "\n",
      "         [[0.3004, 0.1497, 0.2006, 0.1620, 0.1873],\n",
      "          [0.2895, 0.2131, 0.1044, 0.2035, 0.1895],\n",
      "          [0.2096, 0.2058, 0.2288, 0.1735, 0.1823],\n",
      "          [0.2199, 0.1891, 0.1504, 0.2431, 0.1975],\n",
      "          [0.3272, 0.1609, 0.1843, 0.1404, 0.1871]],\n",
      "\n",
      "         [[0.2062, 0.2861, 0.1572, 0.1547, 0.1958],\n",
      "          [0.2334, 0.3056, 0.1602, 0.1582, 0.1427],\n",
      "          [0.2172, 0.2999, 0.1664, 0.2110, 0.1055],\n",
      "          [0.2988, 0.1763, 0.2523, 0.0969, 0.1757],\n",
      "          [0.2170, 0.3238, 0.2250, 0.1109, 0.1233]],\n",
      "\n",
      "         [[0.2311, 0.1493, 0.1900, 0.2110, 0.2187],\n",
      "          [0.2800, 0.1739, 0.1425, 0.2070, 0.1966],\n",
      "          [0.2047, 0.2010, 0.1906, 0.2534, 0.1502],\n",
      "          [0.0999, 0.1777, 0.1626, 0.3532, 0.2065],\n",
      "          [0.1965, 0.2828, 0.1870, 0.1847, 0.1490]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-2.1930e-01, -7.8158e-02,  1.5164e-01,  4.2441e-01,  2.0569e-01],\n",
      "          [-3.7623e-01, -1.2206e-01,  2.2410e-01, -1.1875e-01, -2.5110e-03],\n",
      "          [ 6.1458e-01,  2.0885e-01, -4.6175e-01, -2.9699e-02, -3.3576e-01],\n",
      "          [-2.7182e-01, -2.5229e-02,  3.3043e-01,  7.6016e-02,  1.6064e-01],\n",
      "          [ 4.5545e-01,  1.3897e-01, -2.8918e-01, -1.6772e-01,  3.4264e-01]],\n",
      "\n",
      "         [[-1.6931e-01,  8.7788e-02,  1.3372e-01, -1.8151e-01,  6.5159e-02],\n",
      "          [-1.4780e-01, -2.0410e-01, -5.9320e-01,  2.7575e-01, -6.3067e-01],\n",
      "          [-7.1771e-01, -8.0352e-01, -5.1129e-01, -4.9638e-01, -6.5043e-01],\n",
      "          [-2.5331e-01, -3.1840e-01, -3.9111e-01, -4.8015e-01, -6.0470e-01],\n",
      "          [-1.9848e-01, -5.8727e-01, -9.3126e-02,  1.3849e-01, -2.6704e-01]],\n",
      "\n",
      "         [[-1.7854e-01, -1.1739e-01, -1.8902e-01, -1.3297e-01, -3.2412e-01],\n",
      "          [ 4.0609e-02,  1.1040e-01,  1.2685e-01,  4.4632e-02,  1.3754e-01],\n",
      "          [-6.1492e-02,  5.1731e-01,  7.9244e-02,  5.0241e-01, -1.9675e-01],\n",
      "          [-6.6361e-02,  5.2638e-04, -1.6164e-01,  1.2031e-01, -1.0626e-01],\n",
      "          [-3.3690e-02,  1.7420e-01,  1.4461e-01, -7.7633e-02,  1.8287e-01]],\n",
      "\n",
      "         [[ 2.9796e-01,  1.8966e-01, -5.9742e-01, -2.7727e-01, -6.7103e-01],\n",
      "          [ 1.3407e-01,  3.4317e-02, -3.0027e-01,  4.2918e-02, -2.2862e-01],\n",
      "          [-3.2134e-02,  4.1573e-01,  4.7073e-02, -2.0228e-01,  1.2453e-01],\n",
      "          [-1.3815e-01, -5.0157e-01, -6.8119e-01, -1.9527e-01, -7.4555e-01],\n",
      "          [ 2.4462e-01, -2.9285e-01, -8.9485e-01, -9.1976e-02, -5.8169e-01]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1422, 0.1637, 0.2060, 0.2706, 0.2175],\n",
      "          [0.1458, 0.1880, 0.2657, 0.1886, 0.2119],\n",
      "          [0.3426, 0.2283, 0.1168, 0.1799, 0.1324],\n",
      "          [0.1416, 0.1812, 0.2586, 0.2005, 0.2182],\n",
      "          [0.2752, 0.2006, 0.1307, 0.1476, 0.2459]],\n",
      "\n",
      "         [[0.1695, 0.2192, 0.2295, 0.1675, 0.2143],\n",
      "          [0.2114, 0.1998, 0.1354, 0.3229, 0.1304],\n",
      "          [0.1830, 0.1680, 0.2250, 0.2283, 0.1957],\n",
      "          [0.2321, 0.2174, 0.2022, 0.1850, 0.1633],\n",
      "          [0.1952, 0.1323, 0.2169, 0.2734, 0.1822]],\n",
      "\n",
      "         [[0.2015, 0.2142, 0.1994, 0.2109, 0.1742],\n",
      "          [0.1898, 0.2035, 0.2069, 0.1906, 0.2091],\n",
      "          [0.1523, 0.2717, 0.1753, 0.2677, 0.1330],\n",
      "          [0.1944, 0.2078, 0.1767, 0.2343, 0.1868],\n",
      "          [0.1778, 0.2189, 0.2125, 0.1701, 0.2208]],\n",
      "\n",
      "         [[0.3079, 0.2763, 0.1258, 0.1732, 0.1168],\n",
      "          [0.2403, 0.2175, 0.1556, 0.2194, 0.1672],\n",
      "          [0.1767, 0.2765, 0.1912, 0.1490, 0.2066],\n",
      "          [0.2656, 0.1846, 0.1543, 0.2508, 0.1447],\n",
      "          [0.3272, 0.1912, 0.1047, 0.2337, 0.1432]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 8, 8])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.5699, -0.1706,  0.0549, -0.0782, -0.4235, -0.5121, -0.5967,\n",
      "           -0.8955],\n",
      "          [-0.3769, -0.1200, -0.0209, -0.0865, -0.2835, -0.4105, -0.4497,\n",
      "           -0.8079],\n",
      "          [-0.7066, -0.2669, -0.1569, -0.2790, -0.4802, -0.6146, -0.6888,\n",
      "           -0.8757],\n",
      "          [-0.4796, -0.0221,  0.0624,  0.2756, -0.2467, -0.0962, -0.1081,\n",
      "           -0.3082],\n",
      "          [-0.4214, -0.0831, -0.0091,  0.1985, -0.2443, -0.1200, -0.3074,\n",
      "           -0.4330],\n",
      "          [-0.1919,  0.1397,  0.1938,  0.2613, -0.3946, -0.0604, -0.0143,\n",
      "           -0.2513],\n",
      "          [ 0.0842,  0.3855,  0.3659,  0.3940, -0.1402,  0.0857,  0.1178,\n",
      "           -0.1078],\n",
      "          [ 0.0763,  0.2034,  0.2980,  0.2295, -0.4053,  0.0803,  0.0090,\n",
      "           -0.1724]],\n",
      "\n",
      "         [[ 0.1214,  0.7867,  0.8036,  0.9906,  0.3048,  0.5931,  0.7261,\n",
      "            0.5573],\n",
      "          [ 0.1866,  0.6836,  0.7180,  0.8290,  0.2206,  0.5027,  0.7559,\n",
      "            0.2875],\n",
      "          [ 0.3870,  0.7265,  0.7235,  0.9654,  0.2112,  0.5751,  0.7580,\n",
      "            0.5136],\n",
      "          [ 0.0850,  0.4945,  0.4858,  0.6821,  0.0520,  0.3658,  0.4676,\n",
      "            0.1258],\n",
      "          [-0.0161,  0.4820,  0.2740,  0.7383,  0.0970,  0.4218,  0.6365,\n",
      "            0.2799],\n",
      "          [ 0.2420,  0.7143,  0.6607,  0.7774,  0.2660,  0.6507,  0.7299,\n",
      "            0.4194],\n",
      "          [ 0.3775,  0.7130,  0.6638,  0.7610,  0.2090,  0.7050,  0.7383,\n",
      "            0.4037],\n",
      "          [ 0.5102,  0.9844,  0.8966,  0.8490,  0.3016,  0.8698,  0.9623,\n",
      "            0.6600]],\n",
      "\n",
      "         [[-0.0100,  0.0135, -0.3646, -0.2536, -0.0164,  0.1019, -0.2806,\n",
      "            0.0681],\n",
      "          [ 0.1402,  0.1630, -0.1256,  0.0719, -0.0797,  0.1148, -0.3618,\n",
      "            0.0231],\n",
      "          [ 0.0307,  0.2427,  0.0670,  0.1168, -0.1104,  0.1482, -0.3487,\n",
      "           -0.0330],\n",
      "          [ 0.5318,  0.6688,  0.4176,  0.4241,  0.2956,  0.5461,  0.0245,\n",
      "            0.4008],\n",
      "          [ 0.1545,  0.3502,  0.2672,  0.2895,  0.1926,  0.3949,  0.0081,\n",
      "            0.2939],\n",
      "          [ 0.4738,  0.6895,  0.6122,  0.5319,  0.6175,  0.7932,  0.2923,\n",
      "            0.6896],\n",
      "          [ 0.1291,  0.2865,  0.0783,  0.1108,  0.1222,  0.2463, -0.1873,\n",
      "            0.0611],\n",
      "          [-0.0713, -0.0640, -0.1203, -0.0950, -0.0743, -0.0049, -0.3487,\n",
      "            0.0173]],\n",
      "\n",
      "         [[ 1.1209,  0.9850,  1.0693,  1.1426,  0.9359,  1.0160,  0.9053,\n",
      "            0.5125],\n",
      "          [ 0.7577,  0.6643,  0.6440,  0.8034,  0.5123,  0.5133,  0.3920,\n",
      "            0.2638],\n",
      "          [ 0.8001,  0.6941,  0.6599,  0.7465,  0.4917,  0.6320,  0.5205,\n",
      "            0.3311],\n",
      "          [ 0.3086,  0.3424,  0.4754,  0.4921,  0.1693,  0.2051,  0.1691,\n",
      "           -0.2255],\n",
      "          [-0.0298,  0.1499,  0.1731,  0.3468,  0.0536,  0.1618, -0.0020,\n",
      "           -0.2215],\n",
      "          [ 0.5553,  0.7487,  0.7313,  0.8203,  0.5125,  0.6920,  0.5448,\n",
      "            0.2577],\n",
      "          [ 0.4084,  0.5526,  0.7173,  0.7075,  0.2978,  0.5514,  0.4382,\n",
      "            0.1370],\n",
      "          [ 0.3159,  0.2157,  0.2528,  0.2112,  0.1081,  0.0327, -0.0035,\n",
      "           -0.1385]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1009, 0.1505, 0.1885, 0.1651, 0.1169, 0.1069, 0.0983, 0.0729],\n",
      "          [0.1149, 0.1486, 0.1641, 0.1536, 0.1262, 0.1111, 0.1068, 0.0747],\n",
      "          [0.0997, 0.1547, 0.1727, 0.1529, 0.1250, 0.1093, 0.1015, 0.0842],\n",
      "          [0.0848, 0.1340, 0.1458, 0.1804, 0.1070, 0.1244, 0.1229, 0.1006],\n",
      "          [0.0959, 0.1345, 0.1448, 0.1783, 0.1145, 0.1296, 0.1075, 0.0948],\n",
      "          [0.1049, 0.1462, 0.1543, 0.1651, 0.0857, 0.1197, 0.1253, 0.0989],\n",
      "          [0.1149, 0.1554, 0.1523, 0.1567, 0.0918, 0.1151, 0.1189, 0.0949],\n",
      "          [0.1268, 0.1440, 0.1583, 0.1478, 0.0783, 0.1273, 0.1186, 0.0989]],\n",
      "\n",
      "         [[0.0741, 0.1442, 0.1467, 0.1768, 0.0891, 0.1188, 0.1357, 0.1146],\n",
      "          [0.0868, 0.1426, 0.1476, 0.1649, 0.0898, 0.1190, 0.1533, 0.0960],\n",
      "          [0.0979, 0.1374, 0.1370, 0.1745, 0.0821, 0.1181, 0.1418, 0.1111],\n",
      "          [0.0942, 0.1419, 0.1406, 0.1712, 0.0911, 0.1247, 0.1381, 0.0981],\n",
      "          [0.0830, 0.1366, 0.1110, 0.1766, 0.0930, 0.1287, 0.1595, 0.1116],\n",
      "          [0.0894, 0.1434, 0.1359, 0.1527, 0.0916, 0.1346, 0.1456, 0.1068],\n",
      "          [0.1011, 0.1414, 0.1346, 0.1484, 0.0854, 0.1403, 0.1450, 0.1038],\n",
      "          [0.0956, 0.1536, 0.1407, 0.1342, 0.0776, 0.1370, 0.1503, 0.1111]],\n",
      "\n",
      "         [[0.1340, 0.1371, 0.0940, 0.1050, 0.1331, 0.1498, 0.1022, 0.1448],\n",
      "          [0.1429, 0.1462, 0.1096, 0.1335, 0.1147, 0.1394, 0.0865, 0.1271],\n",
      "          [0.1253, 0.1549, 0.1300, 0.1366, 0.1088, 0.1410, 0.0858, 0.1176],\n",
      "          [0.1385, 0.1588, 0.1235, 0.1244, 0.1094, 0.1405, 0.0834, 0.1215],\n",
      "          [0.1136, 0.1381, 0.1271, 0.1300, 0.1180, 0.1445, 0.0981, 0.1306],\n",
      "          [0.1104, 0.1370, 0.1268, 0.1170, 0.1275, 0.1520, 0.0921, 0.1370],\n",
      "          [0.1269, 0.1485, 0.1206, 0.1245, 0.1260, 0.1426, 0.0924, 0.1185],\n",
      "          [0.1274, 0.1283, 0.1213, 0.1244, 0.1270, 0.1361, 0.0965, 0.1392]],\n",
      "\n",
      "         [[0.1444, 0.1260, 0.1371, 0.1475, 0.1200, 0.1300, 0.1164, 0.0786],\n",
      "          [0.1488, 0.1355, 0.1328, 0.1558, 0.1164, 0.1166, 0.1032, 0.0908],\n",
      "          [0.1498, 0.1347, 0.1302, 0.1419, 0.1100, 0.1266, 0.1132, 0.0937],\n",
      "          [0.1308, 0.1353, 0.1546, 0.1572, 0.1138, 0.1179, 0.1138, 0.0767],\n",
      "          [0.1107, 0.1325, 0.1356, 0.1614, 0.1204, 0.1341, 0.1139, 0.0914],\n",
      "          [0.1170, 0.1420, 0.1395, 0.1525, 0.1121, 0.1342, 0.1158, 0.0869],\n",
      "          [0.1148, 0.1327, 0.1564, 0.1549, 0.1028, 0.1325, 0.1183, 0.0875],\n",
      "          [0.1499, 0.1356, 0.1407, 0.1350, 0.1218, 0.1129, 0.1089, 0.0952]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 8, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 1.5450e-01,  5.8037e-01,  2.1877e-01, -1.3845e-01,  3.2467e-01],\n",
      "          [-6.1309e-02,  4.3084e-01, -1.2039e-02, -6.0539e-02,  2.4170e-01],\n",
      "          [-1.1892e-01,  1.7900e-01,  1.9800e-01, -3.2330e-01,  1.6528e-01],\n",
      "          [-8.0727e-02,  4.2254e-01,  1.1791e-02,  5.9657e-02,  6.7385e-02],\n",
      "          [ 6.8827e-02,  1.9785e-01, -1.6193e-01, -1.4835e-01, -9.6622e-03],\n",
      "          [-1.5730e-01,  1.6947e-01,  1.3010e-01, -8.4242e-02,  2.4002e-01],\n",
      "          [-1.7314e-01,  3.2573e-01, -2.3894e-02,  5.5875e-02,  2.9129e-01],\n",
      "          [-1.4379e-01,  1.8840e-01, -7.7748e-02, -6.2020e-02,  1.7010e-01]],\n",
      "\n",
      "         [[ 6.3937e-02,  1.5538e-01,  1.9371e-01, -2.7879e-01,  6.6061e-01],\n",
      "          [ 2.0308e-01,  8.2269e-02,  1.0495e-01, -2.8553e-01,  5.2569e-01],\n",
      "          [ 2.6978e-01,  4.7660e-02,  3.3892e-01, -4.2499e-01,  6.6702e-01],\n",
      "          [ 2.6067e-01,  3.8399e-02,  2.8954e-01, -4.0881e-01,  5.2322e-01],\n",
      "          [ 1.0444e-01,  7.4506e-02,  4.2970e-02, -3.7718e-01,  5.0491e-01],\n",
      "          [ 2.4287e-01,  1.3168e-01,  1.6627e-01, -1.6334e-01,  5.2968e-01],\n",
      "          [ 1.1143e-01,  1.0327e-01,  8.5571e-02, -3.5959e-02,  5.6596e-01],\n",
      "          [ 1.3452e-01,  2.0152e-01, -1.7069e-01, -1.4409e-01,  2.3169e-01]],\n",
      "\n",
      "         [[-1.1728e-01,  5.9779e-02,  1.1328e-01,  7.7485e-02,  3.1034e-01],\n",
      "          [-2.0423e-01, -2.1323e-01,  2.1478e-01,  2.0211e-01,  4.7163e-01],\n",
      "          [-2.9523e-01, -9.7336e-02,  1.5092e-01,  2.2426e-01,  3.4955e-01],\n",
      "          [ 5.7354e-03,  6.3745e-02,  1.4090e-01,  2.6596e-01,  4.2361e-01],\n",
      "          [ 1.0820e-01,  9.6894e-02, -2.7524e-02,  3.6970e-01,  3.9914e-01],\n",
      "          [ 2.4803e-01,  1.9401e-01,  1.5268e-01,  4.2929e-01,  4.5164e-01],\n",
      "          [ 1.0252e-01, -8.8445e-02,  2.4973e-01,  1.6049e-01,  5.4649e-01],\n",
      "          [ 3.9551e-02, -1.3710e-01,  2.3327e-02,  2.4025e-01,  2.3684e-01]],\n",
      "\n",
      "         [[-6.5839e-01, -2.3926e-01,  2.2917e-01,  6.3542e-02, -4.4019e-01],\n",
      "          [-5.7275e-01, -4.2726e-01,  1.6393e-01,  5.3237e-02, -4.4973e-01],\n",
      "          [-6.9420e-01, -5.2115e-01,  1.3495e-01, -4.0323e-04, -6.3683e-01],\n",
      "          [-9.6310e-01, -6.1053e-01, -1.8842e-01,  4.0177e-02, -5.8783e-01],\n",
      "          [-8.2029e-01, -4.6739e-01, -1.5718e-01, -1.3266e-01, -5.3807e-01],\n",
      "          [-9.9717e-01, -3.0101e-01, -2.2101e-01,  5.3898e-02, -5.1615e-01],\n",
      "          [-8.8613e-01, -4.2957e-01, -1.0413e-01, -1.4182e-02, -5.3012e-01],\n",
      "          [-8.6909e-01, -3.6351e-01, -2.2896e-01, -1.5608e-02, -6.6900e-01]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1809, 0.2769, 0.1929, 0.1349, 0.2144],\n",
      "          [0.1655, 0.2708, 0.1739, 0.1657, 0.2241],\n",
      "          [0.1705, 0.2297, 0.2341, 0.1390, 0.2266],\n",
      "          [0.1650, 0.2729, 0.1810, 0.1898, 0.1913],\n",
      "          [0.2145, 0.2441, 0.1703, 0.1727, 0.1984],\n",
      "          [0.1592, 0.2207, 0.2122, 0.1712, 0.2368],\n",
      "          [0.1502, 0.2474, 0.1744, 0.1889, 0.2390],\n",
      "          [0.1690, 0.2356, 0.1806, 0.1834, 0.2314]],\n",
      "\n",
      "         [[0.1736, 0.1902, 0.1977, 0.1232, 0.3153],\n",
      "          [0.2089, 0.1851, 0.1894, 0.1282, 0.2884],\n",
      "          [0.2058, 0.1648, 0.2205, 0.1027, 0.3062],\n",
      "          [0.2154, 0.1725, 0.2217, 0.1103, 0.2801],\n",
      "          [0.1992, 0.1933, 0.1873, 0.1230, 0.2972],\n",
      "          [0.2075, 0.1857, 0.1922, 0.1382, 0.2764],\n",
      "          [0.1850, 0.1835, 0.1803, 0.1597, 0.2915],\n",
      "          [0.2144, 0.2292, 0.1580, 0.1622, 0.2362]],\n",
      "\n",
      "         [[0.1612, 0.1925, 0.2031, 0.1959, 0.2473],\n",
      "          [0.1433, 0.1420, 0.2179, 0.2151, 0.2817],\n",
      "          [0.1357, 0.1654, 0.2120, 0.2282, 0.2586],\n",
      "          [0.1661, 0.1760, 0.1901, 0.2155, 0.2523],\n",
      "          [0.1819, 0.1798, 0.1588, 0.2362, 0.2433],\n",
      "          [0.1894, 0.1794, 0.1721, 0.2270, 0.2321],\n",
      "          [0.1785, 0.1474, 0.2068, 0.1891, 0.2782],\n",
      "          [0.1900, 0.1593, 0.1870, 0.2323, 0.2315]],\n",
      "\n",
      "         [[0.1212, 0.1843, 0.2944, 0.2494, 0.1507],\n",
      "          [0.1380, 0.1596, 0.2883, 0.2581, 0.1561],\n",
      "          [0.1326, 0.1577, 0.3039, 0.2654, 0.1404],\n",
      "          [0.1140, 0.1621, 0.2473, 0.3108, 0.1659],\n",
      "          [0.1302, 0.1853, 0.2527, 0.2590, 0.1727],\n",
      "          [0.1035, 0.2077, 0.2250, 0.2962, 0.1675],\n",
      "          [0.1165, 0.1839, 0.2547, 0.2786, 0.1663],\n",
      "          [0.1231, 0.2041, 0.2335, 0.2890, 0.1504]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 8, 8])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0472,  0.0879,  0.2978,  0.2386,  0.1248,  0.1915,  0.2561,\n",
      "            0.0054],\n",
      "          [ 0.1045,  0.0680,  0.4094,  0.3350,  0.3683,  0.2622,  0.3887,\n",
      "            0.1088],\n",
      "          [ 0.0312,  0.0456,  0.3465,  0.2296,  0.2971,  0.1873,  0.4172,\n",
      "            0.1886],\n",
      "          [ 0.0589,  0.0716,  0.3312,  0.2210,  0.2280,  0.2030,  0.4405,\n",
      "            0.1993],\n",
      "          [-0.0433, -0.1895,  0.0271, -0.0117,  0.0349,  0.0042,  0.2886,\n",
      "            0.1751],\n",
      "          [ 0.0006, -0.0522,  0.1956,  0.1828,  0.2536,  0.1157,  0.3631,\n",
      "            0.1631],\n",
      "          [ 0.1913,  0.0980,  0.4047,  0.3811,  0.4267,  0.2827,  0.5435,\n",
      "            0.3536],\n",
      "          [ 0.0049, -0.0641,  0.2779,  0.1581,  0.1793,  0.0866,  0.2751,\n",
      "            0.1169]],\n",
      "\n",
      "         [[-0.2038, -0.3203, -0.4043, -0.3652, -0.4229, -0.1450, -0.1790,\n",
      "           -0.2777],\n",
      "          [-0.3791, -0.2727, -0.5135, -0.5375, -0.5189, -0.2452, -0.3469,\n",
      "           -0.2675],\n",
      "          [-0.2960, -0.2784, -0.5102, -0.4768, -0.5058, -0.1357, -0.1648,\n",
      "           -0.1303],\n",
      "          [-0.4490, -0.3802, -0.5467, -0.4965, -0.4844, -0.1826, -0.3141,\n",
      "           -0.2694],\n",
      "          [-0.1396, -0.1420, -0.2779, -0.2200, -0.1974,  0.0331, -0.0011,\n",
      "           -0.0863],\n",
      "          [-0.2451, -0.2023, -0.4178, -0.4166, -0.3912, -0.0837, -0.1934,\n",
      "           -0.1447],\n",
      "          [-0.2785, -0.3508, -0.4351, -0.3887, -0.4555, -0.1564, -0.1856,\n",
      "           -0.3140],\n",
      "          [-0.1070, -0.1070, -0.3466, -0.3433, -0.4881, -0.0483, -0.0690,\n",
      "           -0.1479]],\n",
      "\n",
      "         [[-0.1401, -0.2089, -0.1894, -0.4651, -0.3546, -0.2886, -0.2807,\n",
      "           -0.2235],\n",
      "          [ 0.0513, -0.0397, -0.0068, -0.3492, -0.3441,  0.0028, -0.0369,\n",
      "            0.1802],\n",
      "          [-0.0689, -0.1826, -0.1029, -0.3850, -0.3671, -0.1349, -0.0766,\n",
      "            0.1303],\n",
      "          [-0.3028, -0.4159, -0.3241, -0.6000, -0.6032, -0.3399, -0.3714,\n",
      "           -0.0878],\n",
      "          [-0.0782, -0.2803, -0.1886, -0.4934, -0.3996, -0.2402, -0.1169,\n",
      "            0.0828],\n",
      "          [-0.0660, -0.1688, -0.0455, -0.3962, -0.3664, -0.1314, -0.0972,\n",
      "            0.0269],\n",
      "          [-0.0775, -0.0846, -0.0314, -0.3432, -0.3590, -0.1349, -0.1642,\n",
      "            0.0360],\n",
      "          [-0.1818, -0.2044, -0.1627, -0.5259, -0.5129, -0.2428, -0.2320,\n",
      "           -0.0342]],\n",
      "\n",
      "         [[-0.0978,  0.0816,  0.0872,  0.1109,  0.2894, -0.0668,  0.2430,\n",
      "            0.1021],\n",
      "          [ 0.0884,  0.2630,  0.2126,  0.3563,  0.4291,  0.1541,  0.4116,\n",
      "            0.2473],\n",
      "          [-0.0240, -0.0103,  0.0871,  0.2621,  0.2092, -0.0082,  0.3021,\n",
      "            0.1139],\n",
      "          [ 0.0623,  0.1662,  0.1258,  0.2464,  0.2679,  0.1075,  0.4032,\n",
      "            0.2795],\n",
      "          [ 0.1088,  0.2178,  0.1627,  0.2622,  0.3066, -0.0028,  0.2230,\n",
      "            0.0636],\n",
      "          [-0.0219,  0.1363,  0.1279,  0.1945,  0.2320,  0.0052,  0.1742,\n",
      "            0.0058],\n",
      "          [ 0.0545,  0.3291,  0.2716,  0.3081,  0.3346,  0.1749,  0.3869,\n",
      "            0.3069],\n",
      "          [-0.0169,  0.0764,  0.0667,  0.1099,  0.2250,  0.0651,  0.1986,\n",
      "            0.0716]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1115, 0.1162, 0.1433, 0.1351, 0.1206, 0.1289, 0.1375, 0.1070],\n",
      "          [0.1065, 0.1027, 0.1445, 0.1342, 0.1387, 0.1247, 0.1416, 0.1070],\n",
      "          [0.1029, 0.1044, 0.1410, 0.1255, 0.1342, 0.1203, 0.1514, 0.1204],\n",
      "          [0.1058, 0.1071, 0.1388, 0.1244, 0.1252, 0.1221, 0.1549, 0.1217],\n",
      "          [0.1145, 0.0989, 0.1228, 0.1181, 0.1238, 0.1200, 0.1595, 0.1424],\n",
      "          [0.1065, 0.1010, 0.1295, 0.1278, 0.1372, 0.1195, 0.1531, 0.1253],\n",
      "          [0.1073, 0.0978, 0.1329, 0.1298, 0.1358, 0.1176, 0.1526, 0.1262],\n",
      "          [0.1097, 0.1024, 0.1441, 0.1278, 0.1306, 0.1190, 0.1437, 0.1227]],\n",
      "\n",
      "         [[0.1356, 0.1206, 0.1109, 0.1154, 0.1089, 0.1438, 0.1390, 0.1259],\n",
      "          [0.1249, 0.1390, 0.1092, 0.1066, 0.1086, 0.1428, 0.1290, 0.1397],\n",
      "          [0.1256, 0.1278, 0.1013, 0.1048, 0.1018, 0.1474, 0.1432, 0.1482],\n",
      "          [0.1171, 0.1254, 0.1061, 0.1116, 0.1130, 0.1528, 0.1339, 0.1401],\n",
      "          [0.1230, 0.1228, 0.1072, 0.1135, 0.1161, 0.1462, 0.1413, 0.1298],\n",
      "          [0.1262, 0.1317, 0.1062, 0.1063, 0.1090, 0.1483, 0.1329, 0.1395],\n",
      "          [0.1297, 0.1206, 0.1109, 0.1161, 0.1087, 0.1465, 0.1423, 0.1252],\n",
      "          [0.1366, 0.1366, 0.1075, 0.1079, 0.0933, 0.1449, 0.1419, 0.1312]],\n",
      "\n",
      "         [[0.1415, 0.1321, 0.1347, 0.1023, 0.1142, 0.1220, 0.1230, 0.1302],\n",
      "          [0.1388, 0.1267, 0.1309, 0.0930, 0.0935, 0.1322, 0.1271, 0.1579],\n",
      "          [0.1337, 0.1193, 0.1292, 0.0975, 0.0992, 0.1252, 0.1327, 0.1632],\n",
      "          [0.1335, 0.1192, 0.1306, 0.0992, 0.0988, 0.1286, 0.1246, 0.1655],\n",
      "          [0.1411, 0.1153, 0.1264, 0.0932, 0.1023, 0.1200, 0.1358, 0.1658],\n",
      "          [0.1354, 0.1222, 0.1382, 0.0973, 0.1003, 0.1268, 0.1312, 0.1486],\n",
      "          [0.1326, 0.1316, 0.1388, 0.1016, 0.1001, 0.1252, 0.1216, 0.1485],\n",
      "          [0.1338, 0.1308, 0.1364, 0.0948, 0.0961, 0.1259, 0.1272, 0.1551]],\n",
      "\n",
      "         [[0.1024, 0.1225, 0.1232, 0.1262, 0.1509, 0.1056, 0.1440, 0.1251],\n",
      "          [0.1035, 0.1233, 0.1172, 0.1354, 0.1456, 0.1106, 0.1431, 0.1214],\n",
      "          [0.1078, 0.1093, 0.1205, 0.1435, 0.1361, 0.1095, 0.1494, 0.1238],\n",
      "          [0.1075, 0.1193, 0.1146, 0.1292, 0.1321, 0.1125, 0.1512, 0.1336],\n",
      "          [0.1173, 0.1308, 0.1238, 0.1367, 0.1429, 0.1049, 0.1315, 0.1121],\n",
      "          [0.1095, 0.1282, 0.1271, 0.1359, 0.1411, 0.1125, 0.1332, 0.1125],\n",
      "          [0.1002, 0.1319, 0.1245, 0.1291, 0.1326, 0.1130, 0.1397, 0.1290],\n",
      "          [0.1110, 0.1218, 0.1206, 0.1260, 0.1413, 0.1204, 0.1376, 0.1212]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 8, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.3307,  0.2155, -0.0146, -0.2539, -0.0081],\n",
      "          [-0.4289, -0.0680, -0.1093, -0.6340, -0.2179],\n",
      "          [-0.1419, -0.0552,  0.0260, -0.5957, -0.0959],\n",
      "          [-0.3504, -0.0177,  0.1118, -0.6786, -0.2153],\n",
      "          [-0.4281, -0.1295,  0.2015, -0.7512, -0.3979],\n",
      "          [-0.4233,  0.0148,  0.0344, -0.6891, -0.3069],\n",
      "          [-0.6321,  0.0215, -0.0155, -0.7987, -0.2768],\n",
      "          [-0.6137, -0.1093, -0.1492, -0.7069, -0.3325]],\n",
      "\n",
      "         [[ 0.4600,  0.3125, -0.0667,  0.6121,  0.3190],\n",
      "          [ 0.5564,  0.3003,  0.2764,  0.7300,  0.5190],\n",
      "          [ 0.6340,  0.4417,  0.1714,  0.7495,  0.5692],\n",
      "          [ 0.3772,  0.3060,  0.3064,  0.5488,  0.5529],\n",
      "          [ 0.4406,  0.0500,  0.0174,  0.3449,  0.3408],\n",
      "          [ 0.4279,  0.2249, -0.0579,  0.5792,  0.4190],\n",
      "          [ 0.3431,  0.2115, -0.2007,  0.4964,  0.2652],\n",
      "          [ 0.3414,  0.3264, -0.0845,  0.4890,  0.2977]],\n",
      "\n",
      "         [[-0.4453, -0.2233,  0.0412, -0.1704, -0.5501],\n",
      "          [-0.3859, -0.1806,  0.2349,  0.0133, -0.3566],\n",
      "          [-0.1319, -0.0488,  0.2307,  0.0825, -0.2776],\n",
      "          [-0.2135, -0.1379,  0.3365,  0.2086, -0.0988],\n",
      "          [-0.0411, -0.0885,  0.2943,  0.2819, -0.1926],\n",
      "          [-0.2923, -0.3554,  0.2423,  0.2346, -0.2412],\n",
      "          [-0.4073, -0.4115,  0.1201,  0.0040, -0.4932],\n",
      "          [-0.4161, -0.2384,  0.1624, -0.0713, -0.3593]],\n",
      "\n",
      "         [[-0.0561,  0.0967,  0.4912,  0.2258,  0.3624],\n",
      "          [-0.1481,  0.0470,  0.6200,  0.5928,  0.2194],\n",
      "          [-0.0993, -0.0774,  0.5653,  0.5862,  0.1920],\n",
      "          [-0.0728,  0.1358,  0.6785,  0.7100,  0.3084],\n",
      "          [ 0.2063,  0.2957,  0.6790,  0.5443,  0.6354],\n",
      "          [-0.1183,  0.0728,  0.5247,  0.4921,  0.2857],\n",
      "          [ 0.0262,  0.1927,  0.5152,  0.7391,  0.3965],\n",
      "          [ 0.2636,  0.3364,  0.5019,  0.5510,  0.5122]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1525, 0.2632, 0.2091, 0.1646, 0.2105],\n",
      "          [0.1706, 0.2448, 0.2349, 0.1390, 0.2107],\n",
      "          [0.2018, 0.2201, 0.2387, 0.1282, 0.2113],\n",
      "          [0.1710, 0.2385, 0.2715, 0.1232, 0.1958],\n",
      "          [0.1672, 0.2254, 0.3139, 0.1211, 0.1724],\n",
      "          [0.1661, 0.2574, 0.2625, 0.1273, 0.1866],\n",
      "          [0.1419, 0.2728, 0.2628, 0.1201, 0.2024],\n",
      "          [0.1543, 0.2554, 0.2454, 0.1405, 0.2043]],\n",
      "\n",
      "         [[0.2229, 0.1923, 0.1316, 0.2595, 0.1936],\n",
      "          [0.2136, 0.1653, 0.1614, 0.2540, 0.2057],\n",
      "          [0.2215, 0.1828, 0.1395, 0.2486, 0.2076],\n",
      "          [0.1908, 0.1776, 0.1777, 0.2265, 0.2274],\n",
      "          [0.2412, 0.1632, 0.1580, 0.2192, 0.2183],\n",
      "          [0.2181, 0.1780, 0.1341, 0.2537, 0.2161],\n",
      "          [0.2199, 0.1928, 0.1276, 0.2563, 0.2034],\n",
      "          [0.2103, 0.2072, 0.1374, 0.2438, 0.2013]],\n",
      "\n",
      "         [[0.1641, 0.2049, 0.2670, 0.2161, 0.1478],\n",
      "          [0.1513, 0.1858, 0.2815, 0.2256, 0.1558],\n",
      "          [0.1777, 0.1931, 0.2554, 0.2202, 0.1536],\n",
      "          [0.1548, 0.1670, 0.2684, 0.2362, 0.1737],\n",
      "          [0.1788, 0.1705, 0.2501, 0.2470, 0.1537],\n",
      "          [0.1564, 0.1469, 0.2670, 0.2650, 0.1647],\n",
      "          [0.1635, 0.1628, 0.2770, 0.2467, 0.1500],\n",
      "          [0.1551, 0.1852, 0.2766, 0.2189, 0.1641]],\n",
      "\n",
      "         [[0.1484, 0.1729, 0.2565, 0.1967, 0.2255],\n",
      "          [0.1264, 0.1536, 0.2724, 0.2651, 0.1825],\n",
      "          [0.1372, 0.1402, 0.2667, 0.2723, 0.1836],\n",
      "          [0.1249, 0.1539, 0.2649, 0.2733, 0.1829],\n",
      "          [0.1507, 0.1648, 0.2418, 0.2113, 0.2314],\n",
      "          [0.1342, 0.1625, 0.2553, 0.2471, 0.2010],\n",
      "          [0.1370, 0.1618, 0.2234, 0.2794, 0.1984],\n",
      "          [0.1678, 0.1805, 0.2129, 0.2237, 0.2152]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 8, 8])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.3414, -0.4940, -0.3267, -0.4164, -0.5228, -0.2414, -0.2458,\n",
      "           -0.4544],\n",
      "          [-0.2495, -0.3998, -0.3095, -0.3494, -0.4517, -0.2147, -0.2118,\n",
      "           -0.3863],\n",
      "          [-0.4591, -0.5301, -0.4447, -0.4618, -0.5478, -0.2754, -0.3433,\n",
      "           -0.4170],\n",
      "          [-0.4365, -0.5371, -0.4610, -0.4086, -0.5275, -0.2282, -0.2631,\n",
      "           -0.3383],\n",
      "          [-0.1014, -0.2306, -0.0807, -0.1465, -0.2561,  0.0095, -0.0517,\n",
      "           -0.0762],\n",
      "          [-0.4360, -0.5106, -0.3735, -0.4143, -0.5320, -0.2566, -0.2743,\n",
      "           -0.3313],\n",
      "          [-0.4814, -0.5873, -0.3963, -0.4590, -0.5527, -0.2576, -0.3275,\n",
      "           -0.4378],\n",
      "          [-0.3076, -0.3413, -0.1096, -0.2622, -0.3701, -0.1046, -0.0443,\n",
      "           -0.2298]],\n",
      "\n",
      "         [[ 0.5682,  0.5673,  0.8571,  0.7633,  0.6823,  0.6411,  0.5384,\n",
      "            0.6856],\n",
      "          [ 0.4304,  0.4118,  0.6452,  0.5275,  0.4497,  0.4375,  0.3070,\n",
      "            0.4698],\n",
      "          [ 0.2383,  0.2923,  0.5573,  0.4655,  0.3977,  0.3389,  0.1810,\n",
      "            0.3706],\n",
      "          [ 0.2877,  0.3775,  0.4733,  0.3787,  0.3473,  0.3654,  0.1539,\n",
      "            0.2951],\n",
      "          [ 0.2686,  0.4291,  0.4994,  0.4255,  0.4101,  0.3995,  0.2032,\n",
      "            0.2382],\n",
      "          [ 0.3751,  0.4423,  0.5918,  0.5059,  0.3697,  0.4602,  0.2286,\n",
      "            0.3420],\n",
      "          [ 0.5268,  0.4953,  0.6645,  0.5495,  0.5056,  0.5833,  0.4119,\n",
      "            0.4479],\n",
      "          [ 0.3662,  0.3288,  0.4373,  0.2934,  0.2361,  0.2119,  0.0574,\n",
      "            0.1738]],\n",
      "\n",
      "         [[ 0.2740,  0.3427,  0.3239,  0.2784,  0.2819,  0.3747,  0.3760,\n",
      "            0.2646],\n",
      "          [ 0.1800,  0.2058,  0.1587,  0.1497,  0.1555,  0.3483,  0.2572,\n",
      "            0.1591],\n",
      "          [ 0.0974,  0.0931, -0.0020,  0.0105,  0.0833,  0.2385,  0.1082,\n",
      "           -0.0993],\n",
      "          [ 0.1964,  0.1616,  0.2473,  0.1071,  0.1747,  0.2542,  0.2174,\n",
      "            0.1207],\n",
      "          [ 0.3587,  0.4074,  0.3215,  0.3987,  0.3118,  0.4339,  0.5075,\n",
      "            0.2275],\n",
      "          [ 0.4005,  0.3463,  0.2764,  0.2136,  0.3972,  0.4653,  0.4152,\n",
      "            0.2878],\n",
      "          [ 0.4318,  0.3647,  0.3836,  0.2761,  0.3473,  0.3445,  0.3351,\n",
      "            0.1703],\n",
      "          [ 0.3911,  0.3872,  0.3048,  0.3476,  0.3634,  0.4550,  0.4593,\n",
      "            0.3027]],\n",
      "\n",
      "         [[ 0.1950,  0.2940,  0.2632,  0.2776,  0.2095,  0.1282,  0.1781,\n",
      "            0.2737],\n",
      "          [ 0.2027,  0.3986,  0.3134,  0.2936,  0.2370,  0.0897,  0.2647,\n",
      "            0.2674],\n",
      "          [ 0.2554,  0.4018,  0.2982,  0.2847,  0.2701,  0.1919,  0.3699,\n",
      "            0.3202],\n",
      "          [ 0.1817,  0.3531,  0.2166,  0.2466,  0.1191,  0.0308,  0.1757,\n",
      "            0.2064],\n",
      "          [ 0.2570,  0.4236,  0.3428,  0.3562,  0.2817,  0.2940,  0.4040,\n",
      "            0.4741],\n",
      "          [ 0.2319,  0.3425,  0.2409,  0.2902,  0.1730,  0.0654,  0.2758,\n",
      "            0.2357],\n",
      "          [ 0.1919,  0.3095,  0.1264,  0.1569,  0.0175,  0.0516,  0.1600,\n",
      "            0.2368],\n",
      "          [ 0.2377,  0.3361,  0.2939,  0.4119,  0.1781,  0.2263,  0.2926,\n",
      "            0.2903]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1293, 0.1110, 0.1312, 0.1200, 0.1078, 0.1429, 0.1423, 0.1155],\n",
      "          [0.1339, 0.1152, 0.1261, 0.1211, 0.1094, 0.1386, 0.1390, 0.1167],\n",
      "          [0.1216, 0.1132, 0.1233, 0.1212, 0.1112, 0.1461, 0.1365, 0.1268],\n",
      "          [0.1198, 0.1084, 0.1169, 0.1232, 0.1094, 0.1476, 0.1425, 0.1322],\n",
      "          [0.1265, 0.1112, 0.1291, 0.1209, 0.1084, 0.1413, 0.1329, 0.1297],\n",
      "          [0.1190, 0.1104, 0.1266, 0.1216, 0.1081, 0.1424, 0.1398, 0.1321],\n",
      "          [0.1190, 0.1070, 0.1296, 0.1217, 0.1108, 0.1488, 0.1388, 0.1243],\n",
      "          [0.1139, 0.1101, 0.1389, 0.1192, 0.1070, 0.1396, 0.1482, 0.1231]],\n",
      "\n",
      "         [[0.1131, 0.1130, 0.1510, 0.1375, 0.1268, 0.1217, 0.1098, 0.1272],\n",
      "          [0.1209, 0.1186, 0.1498, 0.1332, 0.1232, 0.1217, 0.1068, 0.1257],\n",
      "          [0.1105, 0.1166, 0.1520, 0.1387, 0.1296, 0.1222, 0.1043, 0.1261],\n",
      "          [0.1188, 0.1300, 0.1430, 0.1301, 0.1261, 0.1284, 0.1039, 0.1197],\n",
      "          [0.1136, 0.1334, 0.1431, 0.1329, 0.1309, 0.1295, 0.1064, 0.1102],\n",
      "          [0.1195, 0.1278, 0.1485, 0.1362, 0.1189, 0.1302, 0.1032, 0.1156],\n",
      "          [0.1251, 0.1212, 0.1436, 0.1280, 0.1225, 0.1324, 0.1115, 0.1156],\n",
      "          [0.1377, 0.1327, 0.1479, 0.1281, 0.1209, 0.1180, 0.1011, 0.1136]],\n",
      "\n",
      "         [[0.1199, 0.1285, 0.1261, 0.1205, 0.1209, 0.1326, 0.1328, 0.1188],\n",
      "          [0.1220, 0.1252, 0.1195, 0.1184, 0.1191, 0.1444, 0.1318, 0.1195],\n",
      "          [0.1284, 0.1279, 0.1163, 0.1177, 0.1266, 0.1479, 0.1298, 0.1055],\n",
      "          [0.1263, 0.1220, 0.1329, 0.1155, 0.1236, 0.1338, 0.1290, 0.1171],\n",
      "          [0.1231, 0.1292, 0.1186, 0.1281, 0.1175, 0.1327, 0.1428, 0.1080],\n",
      "          [0.1310, 0.1241, 0.1157, 0.1087, 0.1306, 0.1398, 0.1330, 0.1171],\n",
      "          [0.1378, 0.1289, 0.1313, 0.1179, 0.1266, 0.1263, 0.1251, 0.1061],\n",
      "          [0.1267, 0.1262, 0.1162, 0.1213, 0.1232, 0.1350, 0.1356, 0.1159]],\n",
      "\n",
      "         [[0.1208, 0.1334, 0.1294, 0.1312, 0.1226, 0.1130, 0.1188, 0.1307],\n",
      "          [0.1178, 0.1433, 0.1316, 0.1290, 0.1219, 0.1052, 0.1254, 0.1257],\n",
      "          [0.1194, 0.1383, 0.1247, 0.1230, 0.1212, 0.1121, 0.1339, 0.1274],\n",
      "          [0.1233, 0.1464, 0.1277, 0.1316, 0.1158, 0.1061, 0.1226, 0.1264],\n",
      "          [0.1131, 0.1337, 0.1233, 0.1249, 0.1160, 0.1174, 0.1311, 0.1406],\n",
      "          [0.1246, 0.1392, 0.1257, 0.1321, 0.1175, 0.1055, 0.1302, 0.1251],\n",
      "          [0.1290, 0.1451, 0.1208, 0.1246, 0.1084, 0.1121, 0.1250, 0.1350],\n",
      "          [0.1192, 0.1315, 0.1260, 0.1418, 0.1123, 0.1178, 0.1259, 0.1256]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 8, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 3.4416e-02, -5.1394e-01,  4.7925e-01, -1.8597e-01, -7.9549e-03],\n",
      "          [ 4.7104e-02, -4.1745e-01,  5.0724e-01,  1.5570e-01,  2.1338e-01],\n",
      "          [-1.6469e-01, -4.5388e-01,  2.9737e-01,  1.7602e-01,  6.2699e-04],\n",
      "          [-8.8892e-02, -4.6374e-01,  3.4337e-01,  2.0068e-02,  1.1583e-01],\n",
      "          [-1.5275e-01, -3.8735e-01,  3.6139e-01, -6.6418e-02, -3.5008e-02],\n",
      "          [ 5.5429e-02, -5.3345e-01,  5.8496e-01,  1.1188e-01,  8.9563e-02],\n",
      "          [-1.0329e-02, -3.8974e-01,  5.3744e-01,  8.8686e-02,  1.2824e-01],\n",
      "          [ 2.3154e-01, -2.5520e-01,  6.3366e-01, -7.0266e-02,  2.9803e-01]],\n",
      "\n",
      "         [[ 5.5532e-02, -2.9351e-01, -6.2054e-01, -3.8651e-01, -3.0640e-01],\n",
      "          [ 9.7693e-02, -6.5540e-01, -6.3236e-01, -5.7749e-01, -1.3694e-01],\n",
      "          [ 3.5847e-02, -6.1913e-01, -6.0587e-01, -3.0911e-01, -2.0199e-01],\n",
      "          [ 8.4381e-02, -3.7847e-01, -5.7414e-01, -3.1905e-01, -5.9053e-02],\n",
      "          [ 1.1970e-01, -3.4987e-01, -4.2026e-01, -1.8453e-01,  8.4350e-03],\n",
      "          [ 1.5063e-01, -4.6152e-01, -6.3900e-01, -4.6603e-01, -1.0685e-01],\n",
      "          [-6.0672e-02, -5.6141e-01, -6.5662e-01, -5.4991e-01, -1.0188e-01],\n",
      "          [ 3.4504e-02, -4.5296e-01, -6.1152e-01, -5.8775e-01, -1.1055e-02]],\n",
      "\n",
      "         [[-2.0457e-01, -8.3415e-02,  1.1722e-01, -3.1498e-01,  8.8550e-02],\n",
      "          [-2.5437e-01, -3.2943e-01, -2.3373e-02, -3.4330e-01, -1.4160e-01],\n",
      "          [-2.0150e-01, -5.6732e-02,  1.9524e-01, -2.5601e-01,  1.3082e-01],\n",
      "          [-2.4355e-01, -1.2310e-01,  2.1264e-01, -3.2612e-01,  2.9566e-02],\n",
      "          [-2.3046e-01, -7.2693e-02,  2.9401e-01, -3.8817e-01, -8.1319e-02],\n",
      "          [-2.2319e-01, -1.5485e-01,  2.0879e-01, -5.1229e-01,  8.8437e-02],\n",
      "          [-2.5789e-01, -2.5860e-01,  1.8222e-01, -4.4721e-01, -5.8979e-02],\n",
      "          [-1.8950e-01, -3.8314e-01, -8.7653e-02, -5.2166e-01, -1.4617e-01]],\n",
      "\n",
      "         [[-1.9410e-02,  1.7936e-01, -7.8127e-02,  1.7603e-01,  1.3431e-01],\n",
      "          [-8.2152e-02, -2.0285e-01, -3.3784e-01, -1.3423e-01, -3.8584e-02],\n",
      "          [ 5.8586e-02,  9.2147e-02, -4.0506e-01,  3.2619e-02,  3.6055e-02],\n",
      "          [ 4.8340e-02, -1.1573e-01, -4.4642e-01,  9.8245e-02, -3.0522e-02],\n",
      "          [-1.0714e-01, -1.2027e-01, -2.6510e-01,  1.8887e-01,  4.9562e-02],\n",
      "          [ 1.2974e-01, -4.6710e-03, -1.0660e-01,  4.6800e-02,  7.1707e-02],\n",
      "          [ 8.5940e-02, -3.0514e-02, -2.2954e-01,  9.2792e-02,  1.5688e-01],\n",
      "          [-2.8258e-02, -1.7232e-01, -1.1869e-01,  6.3595e-02, -1.8082e-02]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2041, 0.1180, 0.3185, 0.1638, 0.1957],\n",
      "          [0.1815, 0.1141, 0.2876, 0.2024, 0.2144],\n",
      "          [0.1689, 0.1265, 0.2680, 0.2374, 0.1992],\n",
      "          [0.1795, 0.1234, 0.2766, 0.2002, 0.2203],\n",
      "          [0.1761, 0.1393, 0.2945, 0.1920, 0.1981],\n",
      "          [0.1871, 0.1038, 0.3177, 0.1979, 0.1936],\n",
      "          [0.1765, 0.1208, 0.3052, 0.1948, 0.2027],\n",
      "          [0.2033, 0.1250, 0.3040, 0.1504, 0.2173]],\n",
      "\n",
      "         [[0.2815, 0.1985, 0.1431, 0.1809, 0.1960],\n",
      "          [0.3074, 0.1448, 0.1481, 0.1565, 0.2431],\n",
      "          [0.2823, 0.1466, 0.1486, 0.1999, 0.2225],\n",
      "          [0.2716, 0.1710, 0.1406, 0.1815, 0.2353],\n",
      "          [0.2604, 0.1628, 0.1517, 0.1921, 0.2330],\n",
      "          [0.3022, 0.1639, 0.1372, 0.1631, 0.2336],\n",
      "          [0.2681, 0.1625, 0.1477, 0.1644, 0.2573],\n",
      "          [0.2754, 0.1692, 0.1444, 0.1478, 0.2632]],\n",
      "\n",
      "         [[0.1741, 0.1965, 0.2402, 0.1559, 0.2334],\n",
      "          [0.1915, 0.1777, 0.2413, 0.1752, 0.2144],\n",
      "          [0.1671, 0.1931, 0.2485, 0.1582, 0.2330],\n",
      "          [0.1683, 0.1899, 0.2656, 0.1550, 0.2212],\n",
      "          [0.1702, 0.1993, 0.2876, 0.1454, 0.1976],\n",
      "          [0.1747, 0.1870, 0.2690, 0.1308, 0.2385],\n",
      "          [0.1786, 0.1785, 0.2773, 0.1478, 0.2179],\n",
      "          [0.2131, 0.1756, 0.2359, 0.1529, 0.2225]],\n",
      "\n",
      "         [[0.1803, 0.2200, 0.1701, 0.2193, 0.2103],\n",
      "          [0.2148, 0.1904, 0.1664, 0.2039, 0.2244],\n",
      "          [0.2167, 0.2241, 0.1363, 0.2111, 0.2118],\n",
      "          [0.2255, 0.1914, 0.1375, 0.2371, 0.2084],\n",
      "          [0.1867, 0.1843, 0.1595, 0.2511, 0.2184],\n",
      "          [0.2209, 0.1931, 0.1744, 0.2033, 0.2084],\n",
      "          [0.2128, 0.1894, 0.1552, 0.2142, 0.2284],\n",
      "          [0.2047, 0.1772, 0.1870, 0.2244, 0.2068]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1571,  0.0563, -0.0080, -0.4970, -0.4950],\n",
      "          [ 0.1945,  0.0083,  0.6080,  0.9470,  0.4510],\n",
      "          [ 0.3743, -0.6010, -0.0247,  0.1531, -0.4621],\n",
      "          [-0.1899,  1.0667,  0.6453,  1.0561,  0.3081],\n",
      "          [-0.0550, -0.2778,  1.0004,  0.1930,  0.2523]],\n",
      "\n",
      "         [[ 0.1564,  1.0077,  0.2923,  0.3599,  0.0765],\n",
      "          [-0.1620,  0.4576, -0.3878,  0.4414, -0.0807],\n",
      "          [ 0.2359, -0.0707,  0.2827,  0.4405, -0.4669],\n",
      "          [ 0.0139,  0.4975,  0.4907,  0.9356,  0.4522],\n",
      "          [ 0.1065,  0.9500,  0.9562,  1.0010,  0.3872]],\n",
      "\n",
      "         [[ 0.2850,  0.4325, -0.0581,  0.5683,  0.3361],\n",
      "          [ 0.9697, -0.1926, -0.9441, -0.0612,  0.0784],\n",
      "          [ 0.2087,  0.0717,  0.3321,  1.0168,  0.1431],\n",
      "          [ 0.0335, -0.1217,  0.2570,  0.5451,  0.0048],\n",
      "          [ 0.2480,  0.0241,  0.4090,  0.2645,  0.4204]],\n",
      "\n",
      "         [[ 0.1466, -0.2276, -0.6137, -1.3748, -0.8677],\n",
      "          [ 0.3496,  0.0651, -0.2894, -0.2262, -0.6113],\n",
      "          [-1.0896, -1.6045, -0.9507, -1.0571, -0.9920],\n",
      "          [-0.0803, -0.6954, -1.0408, -0.0200, -0.9940],\n",
      "          [-0.4838, -0.5583, -0.3845, -0.5571, -0.6770]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2637, 0.2384, 0.2235, 0.1371, 0.1374],\n",
      "          [0.1480, 0.1229, 0.2238, 0.3141, 0.1913],\n",
      "          [0.3046, 0.1149, 0.2044, 0.2442, 0.1320],\n",
      "          [0.0838, 0.2942, 0.1931, 0.2911, 0.1378],\n",
      "          [0.1367, 0.1094, 0.3928, 0.1752, 0.1859]],\n",
      "\n",
      "         [[0.1507, 0.3530, 0.1726, 0.1847, 0.1391],\n",
      "          [0.1522, 0.2829, 0.1215, 0.2783, 0.1651],\n",
      "          [0.2219, 0.1633, 0.2326, 0.2723, 0.1099],\n",
      "          [0.1205, 0.1955, 0.1942, 0.3030, 0.1868],\n",
      "          [0.1060, 0.2464, 0.2479, 0.2593, 0.1404]],\n",
      "\n",
      "         [[0.1905, 0.2208, 0.1352, 0.2529, 0.2005],\n",
      "          [0.4490, 0.1404, 0.0662, 0.1602, 0.1842],\n",
      "          [0.1617, 0.1410, 0.1830, 0.3629, 0.1514],\n",
      "          [0.1740, 0.1490, 0.2176, 0.2903, 0.1691],\n",
      "          [0.1931, 0.1544, 0.2268, 0.1963, 0.2294]],\n",
      "\n",
      "         [[0.3654, 0.2514, 0.1708, 0.0798, 0.1325],\n",
      "          [0.3101, 0.2333, 0.1637, 0.1743, 0.1186],\n",
      "          [0.2048, 0.1224, 0.2353, 0.2116, 0.2258],\n",
      "          [0.2953, 0.1596, 0.1130, 0.3136, 0.1184],\n",
      "          [0.2089, 0.1939, 0.2307, 0.1942, 0.1722]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0773, -0.2902, -0.1825,  0.0140,  0.0430],\n",
      "          [ 0.5002,  0.0920, -0.5720, -0.0644,  0.0727],\n",
      "          [-0.0806, -0.0468,  0.3618,  0.4324,  0.4531],\n",
      "          [ 0.2633,  0.1665, -0.6885,  0.0320,  0.0284],\n",
      "          [ 0.1094,  0.1155, -0.0302,  0.0696,  0.2207]],\n",
      "\n",
      "         [[ 0.4773, -0.3923,  0.5982,  0.0718,  0.1122],\n",
      "          [ 0.9683,  0.0388,  0.2810,  0.2973,  0.2391],\n",
      "          [ 0.1827, -0.0615,  0.0303,  0.1935, -0.0798],\n",
      "          [-0.3070,  0.0963, -0.3138,  0.3143, -0.1880],\n",
      "          [ 0.6168, -0.4548,  0.0715, -0.2642, -0.4356]],\n",
      "\n",
      "         [[-0.1333,  0.2595, -0.0857, -0.4038,  0.1670],\n",
      "          [-0.2693,  0.1771, -0.7054, -0.5193, -0.1186],\n",
      "          [ 0.4937,  0.8406, -0.0991,  0.1612, -0.1924],\n",
      "          [ 1.0035,  0.2581,  0.5613,  0.0065,  0.3698],\n",
      "          [ 0.5807,  0.8714,  0.3242,  0.1471,  0.2736]],\n",
      "\n",
      "         [[ 0.1060, -0.3661,  0.1278,  0.1428,  0.3248],\n",
      "          [ 0.2121,  0.2212, -0.0875,  0.1410,  0.2848],\n",
      "          [-0.1144, -0.0821,  0.0142,  0.0402, -0.3827],\n",
      "          [-0.7463, -0.0079,  0.1649,  0.3276,  0.1022],\n",
      "          [-0.0249,  0.2456,  0.0130,  0.1253, -0.4687]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2028, 0.1639, 0.1825, 0.2222, 0.2287],\n",
      "          [0.3098, 0.2060, 0.1060, 0.1762, 0.2020],\n",
      "          [0.1436, 0.1485, 0.2234, 0.2398, 0.2448],\n",
      "          [0.2579, 0.2341, 0.0995, 0.2046, 0.2039],\n",
      "          [0.2018, 0.2031, 0.1755, 0.1940, 0.2256]],\n",
      "\n",
      "         [[0.2559, 0.1072, 0.2887, 0.1706, 0.1776],\n",
      "          [0.3459, 0.1365, 0.1740, 0.1768, 0.1668],\n",
      "          [0.2261, 0.1772, 0.1942, 0.2286, 0.1739],\n",
      "          [0.1544, 0.2311, 0.1533, 0.2874, 0.1739],\n",
      "          [0.3724, 0.1275, 0.2158, 0.1543, 0.1300]],\n",
      "\n",
      "         [[0.1772, 0.2625, 0.1858, 0.1352, 0.2393],\n",
      "          [0.1941, 0.3034, 0.1255, 0.1512, 0.2257],\n",
      "          [0.2388, 0.3378, 0.1320, 0.1712, 0.1202],\n",
      "          [0.3315, 0.1573, 0.2130, 0.1223, 0.1759],\n",
      "          [0.2225, 0.2975, 0.1721, 0.1442, 0.1637]],\n",
      "\n",
      "         [[0.2029, 0.1266, 0.2074, 0.2105, 0.2526],\n",
      "          [0.2102, 0.2121, 0.1558, 0.1958, 0.2261],\n",
      "          [0.1960, 0.2024, 0.2229, 0.2288, 0.1499],\n",
      "          [0.0922, 0.1930, 0.2294, 0.2699, 0.2155],\n",
      "          [0.1941, 0.2543, 0.2016, 0.2255, 0.1245]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1497,  0.0532, -0.2729, -0.0261,  0.3919],\n",
      "          [-0.3960, -0.0452,  0.1526, -0.2068,  0.0883],\n",
      "          [ 0.4125,  0.3968, -0.2099, -0.0090, -0.1632],\n",
      "          [ 0.0202,  0.1814,  0.5127,  0.0632,  0.2478],\n",
      "          [ 0.3292,  0.1996, -0.4308, -0.0787,  0.1052]],\n",
      "\n",
      "         [[-0.0651,  0.1546,  0.0147, -0.2536,  0.2848],\n",
      "          [-0.2380, -0.2980, -0.3187,  0.3778, -0.0850],\n",
      "          [-0.2635, -0.1228, -0.5132, -0.3818, -0.1859],\n",
      "          [-0.2236, -0.1355, -0.4232, -0.5038, -0.2760],\n",
      "          [-0.3630, -0.2503, -0.3154,  0.2234, -0.3709]],\n",
      "\n",
      "         [[-0.3474, -0.1791, -0.0900, -0.0590, -0.2466],\n",
      "          [-0.0859,  0.1523,  0.0759,  0.1000,  0.0212],\n",
      "          [-0.0335,  0.5791,  0.1715,  0.3236,  0.1872],\n",
      "          [-0.1923,  0.0289, -0.0198, -0.1190, -0.0192],\n",
      "          [-0.1621,  0.3367,  0.1822, -0.2249,  0.1790]],\n",
      "\n",
      "         [[ 0.6150, -0.0751, -0.3776,  0.0628, -0.4543],\n",
      "          [ 0.5209, -0.1568, -0.3600,  0.1127, -0.2658],\n",
      "          [ 0.1838, -0.2343,  0.2302,  0.1409,  0.0348],\n",
      "          [-0.1517, -0.4872, -0.2798,  0.2261, -0.5148],\n",
      "          [ 0.6211, -0.5733, -0.7677,  0.1265, -0.4719]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1678, 0.2056, 0.1483, 0.1899, 0.2884],\n",
      "          [0.1432, 0.2034, 0.2479, 0.1731, 0.2324],\n",
      "          [0.2674, 0.2632, 0.1435, 0.1754, 0.1504],\n",
      "          [0.1636, 0.1923, 0.2678, 0.1708, 0.2055],\n",
      "          [0.2624, 0.2305, 0.1227, 0.1745, 0.2098]],\n",
      "\n",
      "         [[0.1793, 0.2234, 0.1942, 0.1485, 0.2545],\n",
      "          [0.1701, 0.1601, 0.1569, 0.3148, 0.1982],\n",
      "          [0.2041, 0.2350, 0.1590, 0.1813, 0.2206],\n",
      "          [0.2166, 0.2366, 0.1774, 0.1637, 0.2056],\n",
      "          [0.1679, 0.1879, 0.1760, 0.3017, 0.1665]],\n",
      "\n",
      "         [[0.1690, 0.2000, 0.2186, 0.2255, 0.1869],\n",
      "          [0.1735, 0.2202, 0.2040, 0.2090, 0.1932],\n",
      "          [0.1482, 0.2734, 0.1819, 0.2118, 0.1848],\n",
      "          [0.1754, 0.2188, 0.2084, 0.1888, 0.2086],\n",
      "          [0.1562, 0.2572, 0.2204, 0.1467, 0.2197]],\n",
      "\n",
      "         [[0.3583, 0.1797, 0.1328, 0.2063, 0.1230],\n",
      "          [0.3287, 0.1669, 0.1362, 0.2185, 0.1497],\n",
      "          [0.2210, 0.1455, 0.2315, 0.2117, 0.1904],\n",
      "          [0.2106, 0.1505, 0.1852, 0.3072, 0.1464],\n",
      "          [0.4004, 0.1213, 0.0999, 0.2442, 0.1342]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 16, 16])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0830,  0.0409, -0.0599,  ..., -0.2607, -0.3699, -0.5025],\n",
      "          [ 0.2205,  0.0553,  0.0221,  ..., -0.2131, -0.2166, -0.3046],\n",
      "          [-0.2561, -0.3334, -0.2023,  ..., -0.6271, -0.5368, -0.5701],\n",
      "          ...,\n",
      "          [ 0.4947,  0.4107,  0.3241,  ...,  0.2535,  0.2708,  0.1482],\n",
      "          [ 0.2926,  0.2563,  0.1669,  ...,  0.0866,  0.0759, -0.0657],\n",
      "          [ 0.0722,  0.0375, -0.1187,  ..., -0.2975, -0.3566, -0.5728]],\n",
      "\n",
      "         [[ 0.3200,  0.6248,  0.4050,  ...,  0.6676,  0.6559,  0.7821],\n",
      "          [ 0.1036,  0.5554,  0.3143,  ...,  0.7066,  0.5260,  0.5967],\n",
      "          [ 0.4683,  0.8612,  0.7677,  ...,  1.0485,  0.9780,  0.9719],\n",
      "          ...,\n",
      "          [-0.2100, -0.1282, -0.3124,  ...,  0.3499,  0.2421,  0.3515],\n",
      "          [-0.0513,  0.1259,  0.0737,  ...,  0.2162,  0.1110,  0.2743],\n",
      "          [ 0.0961,  0.3443,  0.2769,  ...,  0.6463,  0.4969,  0.5569]],\n",
      "\n",
      "         [[ 0.2488,  0.5948,  0.2567,  ...,  1.2151,  0.2799,  0.1036],\n",
      "          [ 0.3796,  0.6129,  0.2845,  ...,  1.1222,  0.3587, -0.0072],\n",
      "          [ 0.4701,  0.7543,  0.4210,  ...,  1.1835,  0.4356,  0.1121],\n",
      "          ...,\n",
      "          [ 0.7143,  0.8885,  0.7917,  ...,  1.4706,  0.7334,  0.5423],\n",
      "          [ 0.5918,  0.9904,  0.7328,  ...,  1.5143,  0.7070,  0.4940],\n",
      "          [ 0.6893,  0.9401,  0.7683,  ...,  1.3653,  0.7080,  0.4529]],\n",
      "\n",
      "         [[ 0.8460,  0.6207,  0.5496,  ...,  0.4553,  0.5407,  0.3929],\n",
      "          [ 0.5028,  0.2927,  0.2554,  ...,  0.4681,  0.3505,  0.3703],\n",
      "          [ 0.7720,  0.5062,  0.4967,  ...,  0.5902,  0.5144,  0.5233],\n",
      "          ...,\n",
      "          [ 0.4942,  0.5251,  0.0506,  ...,  0.6943,  0.3955,  0.2139],\n",
      "          [ 0.7522,  0.9336,  0.5444,  ...,  0.9720,  0.6756,  0.5743],\n",
      "          [ 0.5342,  0.7213,  0.1969,  ...,  0.5831,  0.4303,  0.2819]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.0665, 0.0752, 0.0680,  ..., 0.0556, 0.0499, 0.0437],\n",
      "          [0.0821, 0.0696, 0.0673,  ..., 0.0532, 0.0530, 0.0486],\n",
      "          [0.0668, 0.0619, 0.0705,  ..., 0.0461, 0.0505, 0.0488],\n",
      "          ...,\n",
      "          [0.0744, 0.0684, 0.0628,  ..., 0.0585, 0.0595, 0.0526],\n",
      "          [0.0730, 0.0704, 0.0644,  ..., 0.0594, 0.0588, 0.0510],\n",
      "          [0.0814, 0.0786, 0.0673,  ..., 0.0563, 0.0530, 0.0427]],\n",
      "\n",
      "         [[0.0495, 0.0671, 0.0538,  ..., 0.0700, 0.0692, 0.0785],\n",
      "          [0.0461, 0.0725, 0.0569,  ..., 0.0843, 0.0704, 0.0755],\n",
      "          [0.0448, 0.0664, 0.0605,  ..., 0.0801, 0.0746, 0.0741],\n",
      "          ...,\n",
      "          [0.0506, 0.0549, 0.0457,  ..., 0.0886, 0.0795, 0.0887],\n",
      "          [0.0510, 0.0609, 0.0578,  ..., 0.0667, 0.0600, 0.0707],\n",
      "          [0.0485, 0.0621, 0.0581,  ..., 0.0840, 0.0724, 0.0768]],\n",
      "\n",
      "         [[0.0519, 0.0733, 0.0523,  ..., 0.1363, 0.0535, 0.0449],\n",
      "          [0.0597, 0.0754, 0.0543,  ..., 0.1254, 0.0584, 0.0405],\n",
      "          [0.0598, 0.0794, 0.0569,  ..., 0.1220, 0.0577, 0.0418],\n",
      "          ...,\n",
      "          [0.0588, 0.0700, 0.0635,  ..., 0.1252, 0.0599, 0.0495],\n",
      "          [0.0509, 0.0759, 0.0586,  ..., 0.1281, 0.0571, 0.0462],\n",
      "          [0.0549, 0.0706, 0.0594,  ..., 0.1080, 0.0559, 0.0433]],\n",
      "\n",
      "         [[0.0787, 0.0629, 0.0585,  ..., 0.0533, 0.0580, 0.0500],\n",
      "          [0.0728, 0.0590, 0.0568,  ..., 0.0703, 0.0625, 0.0637],\n",
      "          [0.0764, 0.0586, 0.0580,  ..., 0.0637, 0.0591, 0.0596],\n",
      "          ...,\n",
      "          [0.0711, 0.0733, 0.0456,  ..., 0.0868, 0.0644, 0.0537],\n",
      "          [0.0638, 0.0765, 0.0519,  ..., 0.0795, 0.0591, 0.0534],\n",
      "          [0.0676, 0.0815, 0.0483,  ..., 0.0710, 0.0610, 0.0525]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 16, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2036,  0.5206, -0.0206, -0.0968,  0.3594],\n",
      "          [-0.0281,  0.5442,  0.0173,  0.1498,  0.4482],\n",
      "          [-0.0620,  0.4494, -0.0514, -0.1366,  0.3838],\n",
      "          [-0.1678,  0.0879, -0.0234, -0.1038,  0.2823],\n",
      "          [-0.2815,  0.1932, -0.2302, -0.0786,  0.0544],\n",
      "          [-0.2214,  0.2059, -0.2695, -0.1428,  0.4490],\n",
      "          [-0.0553,  0.3487, -0.2028, -0.0218,  0.3879],\n",
      "          [-0.0753,  0.4362, -0.0865, -0.3521,  0.4758],\n",
      "          [ 0.0164,  0.4923, -0.1658, -0.2271,  0.4731],\n",
      "          [-0.0416,  0.5616, -0.1852, -0.0431,  0.4078],\n",
      "          [ 0.0405,  0.3487, -0.2825, -0.1781,  0.2569],\n",
      "          [-0.0244,  0.2180, -0.4239, -0.2124,  0.1606],\n",
      "          [ 0.0665,  0.1447, -0.3887, -0.0781,  0.2345],\n",
      "          [-0.3276,  0.1054, -0.5142, -0.5292,  0.1766],\n",
      "          [-0.2435,  0.1136, -0.4328, -0.2055,  0.1285],\n",
      "          [-0.2034,  0.0639, -0.2395, -0.2059,  0.2002]],\n",
      "\n",
      "         [[ 0.1093,  0.1286,  0.3596, -0.3726,  0.4985],\n",
      "          [ 0.2158,  0.0624,  0.3939, -0.5412,  0.4091],\n",
      "          [ 0.1938,  0.0529,  0.4244, -0.4117,  0.3861],\n",
      "          [ 0.2419,  0.0741,  0.3411, -0.3382,  0.3884],\n",
      "          [ 0.3075,  0.0415,  0.4033, -0.2017,  0.5580],\n",
      "          [ 0.2200, -0.0225,  0.2202, -0.2526,  0.1423],\n",
      "          [ 0.1740, -0.1115,  0.2816, -0.4923,  0.0802],\n",
      "          [ 0.3507,  0.1335,  0.4429, -0.1337,  0.3696],\n",
      "          [ 0.2014,  0.1127,  0.4588, -0.4119,  0.2506],\n",
      "          [ 0.1102,  0.0484,  0.2124, -0.2904,  0.0983],\n",
      "          [-0.0906, -0.0707,  0.2796, -0.2410,  0.1972],\n",
      "          [ 0.1381, -0.0774,  0.2334, -0.5575,  0.1022],\n",
      "          [ 0.1115,  0.1032,  0.3531, -0.4199,  0.4952],\n",
      "          [ 0.1719,  0.1952,  0.1456, -0.1925,  0.3945],\n",
      "          [ 0.2311,  0.0909,  0.2385, -0.1921,  0.4297],\n",
      "          [ 0.0675, -0.0023,  0.1979, -0.0599,  0.1055]],\n",
      "\n",
      "         [[ 0.0027, -0.1788,  0.0104,  0.0233, -0.1101],\n",
      "          [ 0.1197, -0.0667,  0.1368,  0.2911,  0.0678],\n",
      "          [ 0.2600, -0.0571,  0.1703,  0.4478,  0.1145],\n",
      "          [ 0.3660,  0.0769,  0.0776,  0.2813,  0.0137],\n",
      "          [ 0.2697, -0.0772,  0.1711,  0.2633, -0.0323],\n",
      "          [ 0.3311,  0.0589,  0.1566,  0.3181, -0.0671],\n",
      "          [ 0.2436, -0.0912,  0.1028,  0.1614, -0.0909],\n",
      "          [ 0.3180, -0.1209,  0.0973,  0.2653,  0.0760],\n",
      "          [ 0.2831,  0.0542,  0.3772,  0.3795,  0.0507],\n",
      "          [ 0.3351, -0.0401,  0.2144,  0.2697,  0.1004],\n",
      "          [ 0.3439,  0.0671,  0.1259,  0.1683,  0.0276],\n",
      "          [ 0.2087, -0.0337, -0.0952,  0.2037,  0.1192],\n",
      "          [ 0.3016, -0.0424,  0.0233,  0.3706,  0.1797],\n",
      "          [ 0.0575, -0.1909,  0.0237,  0.2783, -0.0032],\n",
      "          [ 0.1930, -0.1723,  0.1610,  0.2146,  0.1607],\n",
      "          [ 0.2552, -0.0458,  0.1987,  0.5425,  0.2202]],\n",
      "\n",
      "         [[-0.5766, -0.3473, -0.0384,  0.2948, -0.4764],\n",
      "          [-0.5109, -0.2649, -0.0024,  0.3587, -0.3229],\n",
      "          [-0.7094, -0.2127,  0.0989,  0.3460, -0.2111],\n",
      "          [-0.8185, -0.2566, -0.1180,  0.2363, -0.4974],\n",
      "          [-0.7822, -0.2603, -0.2977,  0.0447, -0.4080],\n",
      "          [-0.8908, -0.1047, -0.2637,  0.2923, -0.4400],\n",
      "          [-0.7896, -0.1747, -0.1358,  0.0662, -0.4461],\n",
      "          [-0.8651, -0.1816, -0.2450,  0.3205, -0.5964],\n",
      "          [-0.8692, -0.2033, -0.1089,  0.2519, -0.4260],\n",
      "          [-0.6275, -0.1929, -0.2776,  0.2934, -0.6481],\n",
      "          [-0.6004, -0.0121, -0.2433,  0.3226, -0.4728],\n",
      "          [-0.6894, -0.1296, -0.1185,  0.3822, -0.4301],\n",
      "          [-0.3890, -0.0415, -0.0678,  0.2349, -0.2322],\n",
      "          [-0.6621, -0.1205, -0.2134,  0.3170, -0.2228],\n",
      "          [-0.6552, -0.0737, -0.2007,  0.1969, -0.2571],\n",
      "          [-0.5988, -0.0838, -0.1041,  0.2082, -0.2786]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1402, 0.2893, 0.1684, 0.1560, 0.2462],\n",
      "          [0.1510, 0.2676, 0.1580, 0.1804, 0.2431],\n",
      "          [0.1621, 0.2704, 0.1639, 0.1505, 0.2532],\n",
      "          [0.1644, 0.2123, 0.1900, 0.1753, 0.2579],\n",
      "          [0.1591, 0.2558, 0.1675, 0.1949, 0.2227],\n",
      "          [0.1533, 0.2350, 0.1461, 0.1658, 0.2997],\n",
      "          [0.1680, 0.2516, 0.1450, 0.1737, 0.2617],\n",
      "          [0.1626, 0.2712, 0.1608, 0.1233, 0.2821],\n",
      "          [0.1723, 0.2772, 0.1436, 0.1350, 0.2720],\n",
      "          [0.1597, 0.2920, 0.1384, 0.1595, 0.2504],\n",
      "          [0.1949, 0.2653, 0.1411, 0.1567, 0.2420],\n",
      "          [0.2009, 0.2560, 0.1348, 0.1665, 0.2418],\n",
      "          [0.2099, 0.2270, 0.1332, 0.1817, 0.2483],\n",
      "          [0.1711, 0.2638, 0.1420, 0.1399, 0.2833],\n",
      "          [0.1740, 0.2487, 0.1440, 0.1808, 0.2525],\n",
      "          [0.1734, 0.2266, 0.1673, 0.1730, 0.2597]],\n",
      "\n",
      "         [[0.1853, 0.1889, 0.2380, 0.1144, 0.2734],\n",
      "          [0.2112, 0.1812, 0.2524, 0.0991, 0.2562],\n",
      "          [0.2047, 0.1778, 0.2578, 0.1117, 0.2481],\n",
      "          [0.2143, 0.1811, 0.2366, 0.1199, 0.2481],\n",
      "          [0.2104, 0.1613, 0.2316, 0.1264, 0.2703],\n",
      "          [0.2308, 0.1811, 0.2308, 0.1439, 0.2135],\n",
      "          [0.2331, 0.1752, 0.2596, 0.1197, 0.2123],\n",
      "          [0.2204, 0.1774, 0.2417, 0.1358, 0.2246],\n",
      "          [0.2083, 0.1906, 0.2695, 0.1128, 0.2188],\n",
      "          [0.2125, 0.1998, 0.2354, 0.1424, 0.2100],\n",
      "          [0.1766, 0.1802, 0.2557, 0.1519, 0.2355],\n",
      "          [0.2288, 0.1845, 0.2517, 0.1141, 0.2208],\n",
      "          [0.1880, 0.1864, 0.2393, 0.1105, 0.2758],\n",
      "          [0.2024, 0.2071, 0.1971, 0.1406, 0.2528],\n",
      "          [0.2105, 0.1829, 0.2120, 0.1378, 0.2567],\n",
      "          [0.2004, 0.1869, 0.2283, 0.1764, 0.2081]],\n",
      "\n",
      "         [[0.2103, 0.1754, 0.2119, 0.2146, 0.1878],\n",
      "          [0.2007, 0.1665, 0.2041, 0.2382, 0.1905],\n",
      "          [0.2122, 0.1545, 0.1939, 0.2560, 0.1834],\n",
      "          [0.2427, 0.1818, 0.1819, 0.2230, 0.1706],\n",
      "          [0.2301, 0.1627, 0.2085, 0.2286, 0.1701],\n",
      "          [0.2347, 0.1788, 0.1971, 0.2317, 0.1576],\n",
      "          [0.2369, 0.1695, 0.2058, 0.2182, 0.1696],\n",
      "          [0.2392, 0.1542, 0.1918, 0.2269, 0.1878],\n",
      "          [0.2089, 0.1661, 0.2295, 0.2300, 0.1656],\n",
      "          [0.2325, 0.1598, 0.2061, 0.2178, 0.1839],\n",
      "          [0.2421, 0.1836, 0.1947, 0.2031, 0.1765],\n",
      "          [0.2256, 0.1771, 0.1665, 0.2245, 0.2063],\n",
      "          [0.2261, 0.1603, 0.1712, 0.2423, 0.2002],\n",
      "          [0.2026, 0.1581, 0.1959, 0.2527, 0.1907],\n",
      "          [0.2149, 0.1492, 0.2082, 0.2196, 0.2081],\n",
      "          [0.2007, 0.1485, 0.1896, 0.2674, 0.1937]],\n",
      "\n",
      "         [[0.1339, 0.1684, 0.2294, 0.3202, 0.1480],\n",
      "          [0.1327, 0.1697, 0.2207, 0.3167, 0.1602],\n",
      "          [0.1063, 0.1747, 0.2386, 0.3054, 0.1750],\n",
      "          [0.1109, 0.1945, 0.2234, 0.3184, 0.1529],\n",
      "          [0.1242, 0.2094, 0.2017, 0.2841, 0.1806],\n",
      "          [0.1010, 0.2217, 0.1891, 0.3297, 0.1585],\n",
      "          [0.1172, 0.2167, 0.2253, 0.2757, 0.1652],\n",
      "          [0.1061, 0.2103, 0.1973, 0.3474, 0.1389],\n",
      "          [0.1030, 0.2004, 0.2203, 0.3159, 0.1604],\n",
      "          [0.1341, 0.2072, 0.1904, 0.3369, 0.1314],\n",
      "          [0.1269, 0.2284, 0.1813, 0.3193, 0.1441],\n",
      "          [0.1145, 0.2004, 0.2026, 0.3342, 0.1483],\n",
      "          [0.1464, 0.2072, 0.2019, 0.2732, 0.1713],\n",
      "          [0.1177, 0.2022, 0.1843, 0.3132, 0.1826],\n",
      "          [0.1220, 0.2182, 0.1922, 0.2860, 0.1816],\n",
      "          [0.1261, 0.2110, 0.2067, 0.2826, 0.1736]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 16, 16])\n",
      "scaled_prod: \n",
      " tensor([[[[ 2.1784e-01,  2.6032e-01,  3.9666e-01,  ...,  5.5863e-01,\n",
      "            4.0010e-01,  6.5767e-01],\n",
      "          [ 2.5603e-01,  3.4185e-01,  4.0970e-01,  ...,  5.9345e-01,\n",
      "            4.2074e-01,  6.0633e-01],\n",
      "          [ 1.0492e-01,  1.3707e-01,  2.4138e-01,  ...,  3.4511e-01,\n",
      "            1.7644e-01,  4.3773e-01],\n",
      "          ...,\n",
      "          [-2.8274e-02,  1.5283e-01,  2.8968e-01,  ...,  1.6378e-01,\n",
      "            3.9053e-02,  2.3487e-01],\n",
      "          [ 1.2510e-01,  3.2069e-01,  4.1099e-01,  ...,  3.7849e-01,\n",
      "            2.1480e-01,  4.6345e-01],\n",
      "          [ 1.4257e-01,  2.3301e-01,  2.6353e-01,  ...,  2.3428e-01,\n",
      "            1.2374e-01,  3.8958e-01]],\n",
      "\n",
      "         [[-2.8896e-01, -3.4437e-01, -1.7753e-01,  ..., -3.9162e-01,\n",
      "           -1.1959e-01, -2.0477e-01],\n",
      "          [-3.8534e-01, -4.3848e-01, -3.2777e-01,  ..., -4.1055e-01,\n",
      "           -1.7814e-01, -3.0926e-01],\n",
      "          [-3.2281e-01, -4.0489e-01, -2.4071e-01,  ..., -4.2106e-01,\n",
      "           -1.4590e-01, -3.5639e-01],\n",
      "          ...,\n",
      "          [-1.3829e-01, -2.3293e-01, -6.1405e-02,  ..., -1.7186e-01,\n",
      "            4.1433e-02, -9.4647e-02],\n",
      "          [ 5.6523e-03, -1.0929e-01,  2.7898e-02,  ..., -1.1366e-01,\n",
      "            9.7759e-02, -2.2133e-02],\n",
      "          [-1.3855e-01, -1.7631e-01, -8.2382e-03,  ..., -2.5408e-01,\n",
      "           -5.1148e-02, -8.9557e-02]],\n",
      "\n",
      "         [[ 5.1066e-02, -2.5125e-01, -2.0176e-01,  ..., -2.0811e-01,\n",
      "           -4.5288e-01, -3.2596e-01],\n",
      "          [ 1.9774e-01, -6.0887e-02,  5.5126e-02,  ...,  1.8227e-02,\n",
      "           -2.6208e-01, -1.3774e-01],\n",
      "          [ 3.8994e-02, -2.1951e-01, -1.4559e-01,  ..., -1.0812e-01,\n",
      "           -3.8832e-01, -2.9152e-01],\n",
      "          ...,\n",
      "          [ 4.6906e-02, -1.9701e-01, -1.3264e-01,  ..., -1.8439e-01,\n",
      "           -3.4983e-01, -3.1064e-01],\n",
      "          [-4.1254e-02, -2.5321e-01, -1.9339e-01,  ..., -2.3631e-01,\n",
      "           -4.2556e-01, -3.1569e-01],\n",
      "          [ 3.2424e-02, -1.8229e-01, -1.4635e-01,  ..., -1.4139e-01,\n",
      "           -3.6054e-01, -2.0427e-01]],\n",
      "\n",
      "         [[ 1.5050e-01,  3.5532e-01,  6.0303e-02,  ...,  3.4239e-02,\n",
      "           -1.2540e-02,  2.2853e-01],\n",
      "          [ 1.0768e-01,  2.8244e-01,  5.1471e-02,  ..., -3.5343e-02,\n",
      "           -1.2374e-01,  1.5776e-01],\n",
      "          [ 1.4041e-01,  2.6115e-01,  3.9594e-02,  ..., -1.0862e-01,\n",
      "           -1.9534e-01,  3.6986e-02],\n",
      "          ...,\n",
      "          [-1.3449e-01, -4.9740e-04, -1.5086e-01,  ..., -2.5487e-01,\n",
      "           -2.3993e-01, -4.8003e-02],\n",
      "          [-1.4893e-01, -5.9062e-03, -1.8034e-01,  ..., -2.4376e-01,\n",
      "           -3.2066e-01, -8.5483e-02],\n",
      "          [ 5.7557e-04,  9.9623e-02, -3.8060e-02,  ..., -1.7833e-01,\n",
      "           -1.7541e-01,  2.8970e-02]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.0528, 0.0550, 0.0631,  ..., 0.0742, 0.0633, 0.0819],\n",
      "          [0.0525, 0.0572, 0.0613,  ..., 0.0736, 0.0619, 0.0746],\n",
      "          [0.0556, 0.0575, 0.0638,  ..., 0.0707, 0.0598, 0.0776],\n",
      "          ...,\n",
      "          [0.0522, 0.0626, 0.0718,  ..., 0.0633, 0.0559, 0.0680],\n",
      "          [0.0537, 0.0653, 0.0715,  ..., 0.0692, 0.0587, 0.0753],\n",
      "          [0.0597, 0.0653, 0.0673,  ..., 0.0654, 0.0586, 0.0764]],\n",
      "\n",
      "         [[0.0618, 0.0585, 0.0691,  ..., 0.0558, 0.0732, 0.0672],\n",
      "          [0.0621, 0.0589, 0.0658,  ..., 0.0606, 0.0765, 0.0671],\n",
      "          [0.0630, 0.0581, 0.0684,  ..., 0.0571, 0.0752, 0.0609],\n",
      "          ...,\n",
      "          [0.0593, 0.0539, 0.0640,  ..., 0.0573, 0.0709, 0.0619],\n",
      "          [0.0656, 0.0585, 0.0671,  ..., 0.0582, 0.0719, 0.0638],\n",
      "          [0.0636, 0.0612, 0.0724,  ..., 0.0566, 0.0694, 0.0668]],\n",
      "\n",
      "         [[0.0811, 0.0600, 0.0630,  ..., 0.0626, 0.0490, 0.0556],\n",
      "          [0.0763, 0.0589, 0.0662,  ..., 0.0638, 0.0482, 0.0546],\n",
      "          [0.0771, 0.0596, 0.0641,  ..., 0.0666, 0.0503, 0.0554],\n",
      "          ...,\n",
      "          [0.0782, 0.0612, 0.0653,  ..., 0.0620, 0.0526, 0.0547],\n",
      "          [0.0747, 0.0604, 0.0641,  ..., 0.0614, 0.0508, 0.0567],\n",
      "          [0.0750, 0.0605, 0.0627,  ..., 0.0630, 0.0506, 0.0592]],\n",
      "\n",
      "         [[0.0619, 0.0760, 0.0566,  ..., 0.0551, 0.0526, 0.0669],\n",
      "          [0.0602, 0.0717, 0.0569,  ..., 0.0522, 0.0478, 0.0633],\n",
      "          [0.0668, 0.0754, 0.0604,  ..., 0.0521, 0.0478, 0.0603],\n",
      "          ...,\n",
      "          [0.0622, 0.0711, 0.0612,  ..., 0.0551, 0.0560, 0.0678],\n",
      "          [0.0629, 0.0725, 0.0609,  ..., 0.0572, 0.0529, 0.0670],\n",
      "          [0.0645, 0.0712, 0.0621,  ..., 0.0540, 0.0541, 0.0664]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 16, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-5.8452e-01, -3.1897e-02,  1.2846e-01, -4.7723e-01, -5.1991e-02],\n",
      "          [-6.1916e-01, -1.9357e-01,  2.3444e-01, -7.1569e-01, -1.3119e-01],\n",
      "          [-6.5416e-01, -1.4283e-01,  1.7406e-01, -4.7775e-01,  6.4459e-02],\n",
      "          [-4.6867e-01,  2.1465e-01,  2.6653e-01, -2.6352e-01,  4.1175e-02],\n",
      "          [-4.9749e-01, -8.8692e-02,  2.3374e-01, -2.8197e-01, -2.8749e-02],\n",
      "          [-7.6211e-01, -1.7231e-01,  1.8183e-01, -4.7278e-01, -2.7162e-01],\n",
      "          [-5.6810e-01,  7.7385e-02,  1.2278e-01, -5.8370e-01, -2.3337e-01],\n",
      "          [-5.3568e-01, -4.7602e-02,  1.3447e-01, -4.2000e-01, -4.5755e-02],\n",
      "          [-8.8774e-01, -3.0610e-01,  2.4127e-01, -7.9856e-01, -2.5029e-01],\n",
      "          [-5.0505e-01, -6.2462e-02,  2.3365e-01, -3.0556e-01,  4.2121e-02],\n",
      "          [-7.9359e-01, -1.0337e-01,  3.6705e-01, -3.9915e-01, -1.0435e-01],\n",
      "          [-4.3523e-01,  3.0020e-02,  1.4870e-01, -2.8701e-01,  4.2604e-02],\n",
      "          [-7.6583e-01, -5.4588e-02,  1.4459e-01, -6.1013e-01, -1.7746e-01],\n",
      "          [-6.1427e-01, -1.3256e-01, -1.0050e-01, -5.3616e-01, -1.8014e-01],\n",
      "          [-6.2348e-01, -1.2616e-01,  2.8810e-02, -6.6300e-01, -2.0870e-01],\n",
      "          [-4.3805e-01,  9.8157e-02,  2.0485e-01, -5.1814e-01,  1.2664e-01]],\n",
      "\n",
      "         [[ 7.0704e-01,  4.0882e-01,  7.4801e-01,  6.0405e-01,  7.5174e-01],\n",
      "          [ 3.6637e-01,  3.5773e-01,  4.5229e-01,  4.6329e-01,  6.0282e-01],\n",
      "          [ 5.8510e-01,  5.2672e-01,  7.2035e-01,  4.9908e-01,  8.4549e-01],\n",
      "          [ 3.9990e-01,  1.9604e-01,  3.2503e-01,  3.9397e-01,  6.9779e-01],\n",
      "          [ 3.3359e-01,  1.3362e-01,  3.9617e-01,  2.3446e-01,  7.6780e-01],\n",
      "          [ 2.2031e-01,  1.5365e-01,  2.8514e-01,  6.6793e-02,  4.7883e-01],\n",
      "          [ 3.5880e-01,  1.0287e-02,  2.9480e-01,  5.0127e-02,  5.4440e-01],\n",
      "          [ 1.9074e-01,  3.1350e-01,  2.3671e-01,  2.2952e-01,  5.0821e-01],\n",
      "          [ 3.1973e-01,  3.9710e-01,  3.6401e-01,  2.2692e-01,  6.7493e-01],\n",
      "          [ 3.5241e-01,  2.7421e-01,  5.9355e-01,  2.9479e-01,  6.7455e-01],\n",
      "          [ 3.1487e-01,  2.4399e-01,  4.9083e-01,  1.2816e-01,  5.6690e-01],\n",
      "          [ 3.5398e-01, -8.0099e-02,  3.6375e-01, -1.0848e-01,  3.1202e-01],\n",
      "          [ 4.5272e-01,  1.3307e-01,  3.2617e-01,  3.5660e-01,  5.6430e-01],\n",
      "          [ 5.0360e-01, -6.0500e-02,  3.7465e-01,  2.4003e-01,  6.0082e-01],\n",
      "          [ 4.0948e-01,  5.6162e-02,  4.6547e-01,  3.1032e-01,  6.5890e-01],\n",
      "          [ 3.6747e-01,  1.1423e-01,  3.3512e-01,  2.0391e-01,  5.8515e-01]],\n",
      "\n",
      "         [[ 9.5918e-02, -2.2057e-01,  9.5161e-02,  1.3871e-01, -2.1811e-01],\n",
      "          [-1.7482e-01, -2.4057e-01, -3.0916e-01, -9.4965e-02, -4.1866e-01],\n",
      "          [-1.0535e-02, -2.6474e-01, -1.6523e-01, -7.8191e-02, -2.5325e-01],\n",
      "          [-1.1907e-01, -3.5235e-01, -1.9532e-01, -3.6468e-02, -1.8755e-01],\n",
      "          [-1.3804e-01, -4.6579e-01,  6.2108e-02, -2.6518e-02, -2.1515e-01],\n",
      "          [-1.6382e-01, -2.8122e-01, -1.2541e-01, -1.0835e-01, -1.5852e-01],\n",
      "          [-3.9010e-01, -6.1467e-01, -7.2482e-02, -1.1245e-01, -3.5148e-01],\n",
      "          [-3.6163e-01, -4.3392e-01, -5.6042e-02, -7.2159e-06, -2.8336e-01],\n",
      "          [-2.0266e-01, -2.2221e-01, -1.4168e-01, -4.3909e-02, -2.0073e-01],\n",
      "          [-8.7457e-02, -2.4009e-01,  4.7196e-02,  2.2994e-02, -1.4627e-01],\n",
      "          [-1.5805e-01, -1.2584e-01, -1.1369e-01, -4.1985e-02,  1.3957e-01],\n",
      "          [-1.1688e-01, -3.7550e-01, -1.1474e-01, -2.0883e-01, -9.7146e-02],\n",
      "          [-2.6211e-01, -4.1021e-01, -3.7725e-01, -4.8477e-01, -1.8037e-01],\n",
      "          [-2.9985e-01, -3.4017e-01, -2.6153e-01, -2.6426e-01, -2.5684e-01],\n",
      "          [-4.0012e-01, -3.0874e-01, -2.0689e-01, -3.4222e-01, -2.9306e-01],\n",
      "          [-5.4058e-01, -3.8424e-01, -3.6921e-01, -2.8669e-01, -3.2498e-01]],\n",
      "\n",
      "         [[-3.5989e-02,  2.2374e-01,  4.6077e-01,  7.1505e-01,  5.5487e-01],\n",
      "          [ 1.1702e-01, -2.2863e-02,  4.6248e-01,  3.5364e-01,  4.5131e-01],\n",
      "          [ 4.6448e-03,  5.3153e-02,  5.3521e-01,  5.4908e-01,  3.6551e-01],\n",
      "          [-9.8156e-02, -9.6728e-02,  4.1275e-01,  5.9341e-01,  4.2817e-01],\n",
      "          [ 1.9103e-01,  6.5914e-02,  5.2986e-01,  7.3176e-01,  6.6533e-01],\n",
      "          [ 1.5554e-01,  2.8392e-01,  5.8646e-01,  7.6569e-01,  8.4328e-01],\n",
      "          [ 1.4041e-01, -3.4325e-02,  2.9806e-01,  4.2104e-01,  5.4665e-01],\n",
      "          [-5.3183e-02,  1.0840e-01,  3.6689e-01,  6.3354e-01,  4.4612e-01],\n",
      "          [-1.5396e-01,  5.1541e-02,  2.7312e-01,  4.3356e-01,  3.0246e-01],\n",
      "          [ 3.1444e-02,  1.0787e-01,  3.8095e-01,  6.2746e-01,  5.4395e-01],\n",
      "          [ 5.2895e-02,  1.9573e-01,  2.7772e-01,  5.6170e-01,  3.6471e-01],\n",
      "          [ 2.4196e-02,  1.3421e-02,  3.7416e-01,  5.5451e-01,  4.2449e-01],\n",
      "          [ 1.6117e-01,  2.4315e-01,  3.1115e-01,  5.0029e-01,  5.1503e-01],\n",
      "          [ 9.3465e-02,  3.4622e-02,  4.8104e-01,  6.6322e-01,  3.3935e-01],\n",
      "          [ 1.4577e-01,  2.6139e-01,  4.7255e-01,  5.1257e-01,  5.0594e-01],\n",
      "          [ 1.0091e-01,  2.7453e-01,  4.5636e-01,  5.0554e-01,  5.5301e-01]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1317, 0.2288, 0.2686, 0.1466, 0.2243],\n",
      "          [0.1349, 0.2064, 0.3166, 0.1224, 0.2197],\n",
      "          [0.1219, 0.2033, 0.2791, 0.1455, 0.2502],\n",
      "          [0.1256, 0.2488, 0.2621, 0.1543, 0.2092],\n",
      "          [0.1348, 0.2028, 0.2800, 0.1672, 0.2153],\n",
      "          [0.1199, 0.2162, 0.3081, 0.1601, 0.1958],\n",
      "          [0.1373, 0.2618, 0.2739, 0.1352, 0.1919],\n",
      "          [0.1363, 0.2220, 0.2663, 0.1530, 0.2224],\n",
      "          [0.1128, 0.2018, 0.3488, 0.1233, 0.2133],\n",
      "          [0.1316, 0.2049, 0.2755, 0.1606, 0.2274],\n",
      "          [0.1035, 0.2064, 0.3304, 0.1535, 0.2062],\n",
      "          [0.1397, 0.2225, 0.2505, 0.1620, 0.2253],\n",
      "          [0.1178, 0.2398, 0.2927, 0.1376, 0.2121],\n",
      "          [0.1446, 0.2341, 0.2417, 0.1564, 0.2232],\n",
      "          [0.1421, 0.2336, 0.2727, 0.1366, 0.2151],\n",
      "          [0.1371, 0.2344, 0.2608, 0.1266, 0.2412]],\n",
      "\n",
      "         [[0.2113, 0.1568, 0.2202, 0.1907, 0.2210],\n",
      "          [0.1835, 0.1819, 0.2000, 0.2022, 0.2324],\n",
      "          [0.1886, 0.1779, 0.2159, 0.1730, 0.2447],\n",
      "          [0.1967, 0.1604, 0.1825, 0.1955, 0.2649],\n",
      "          [0.1875, 0.1535, 0.1996, 0.1698, 0.2895],\n",
      "          [0.1940, 0.1815, 0.2070, 0.1664, 0.2512],\n",
      "          [0.2182, 0.1540, 0.2047, 0.1603, 0.2627],\n",
      "          [0.1789, 0.2022, 0.1873, 0.1859, 0.2457],\n",
      "          [0.1830, 0.1978, 0.1913, 0.1668, 0.2611],\n",
      "          [0.1811, 0.1675, 0.2305, 0.1710, 0.2499],\n",
      "          [0.1908, 0.1778, 0.2275, 0.1583, 0.2455],\n",
      "          [0.2355, 0.1526, 0.2378, 0.1483, 0.2258],\n",
      "          [0.2158, 0.1568, 0.1901, 0.1960, 0.2413],\n",
      "          [0.2316, 0.1317, 0.2036, 0.1779, 0.2552],\n",
      "          [0.2021, 0.1419, 0.2137, 0.1830, 0.2593],\n",
      "          [0.2068, 0.1605, 0.2002, 0.1756, 0.2570]],\n",
      "\n",
      "         [[0.2221, 0.1619, 0.2219, 0.2318, 0.1623],\n",
      "          [0.2138, 0.2002, 0.1869, 0.2316, 0.1675],\n",
      "          [0.2298, 0.1782, 0.1969, 0.2148, 0.1803],\n",
      "          [0.2110, 0.1671, 0.1955, 0.2292, 0.1971],\n",
      "          [0.2006, 0.1445, 0.2450, 0.2242, 0.1857],\n",
      "          [0.2004, 0.1782, 0.2082, 0.2118, 0.2014],\n",
      "          [0.1808, 0.1444, 0.2483, 0.2386, 0.1879],\n",
      "          [0.1723, 0.1603, 0.2338, 0.2473, 0.1863],\n",
      "          [0.1917, 0.1880, 0.2037, 0.2246, 0.1920],\n",
      "          [0.1975, 0.1696, 0.2260, 0.2206, 0.1863],\n",
      "          [0.1803, 0.1862, 0.1884, 0.2024, 0.2427],\n",
      "          [0.2125, 0.1641, 0.2129, 0.1938, 0.2167],\n",
      "          [0.2156, 0.1859, 0.1921, 0.1725, 0.2339],\n",
      "          [0.1969, 0.1891, 0.2046, 0.2040, 0.2055],\n",
      "          [0.1824, 0.1999, 0.2213, 0.1933, 0.2030],\n",
      "          [0.1699, 0.1987, 0.2017, 0.2190, 0.2108]],\n",
      "\n",
      "         [[0.1272, 0.1649, 0.2090, 0.2695, 0.2296],\n",
      "          [0.1682, 0.1462, 0.2376, 0.2131, 0.2349],\n",
      "          [0.1448, 0.1520, 0.2461, 0.2495, 0.2077],\n",
      "          [0.1359, 0.1361, 0.2265, 0.2714, 0.2301],\n",
      "          [0.1513, 0.1335, 0.2123, 0.2598, 0.2431],\n",
      "          [0.1332, 0.1515, 0.2050, 0.2452, 0.2650],\n",
      "          [0.1714, 0.1439, 0.2006, 0.2269, 0.2572],\n",
      "          [0.1364, 0.1603, 0.2076, 0.2710, 0.2247],\n",
      "          [0.1401, 0.1720, 0.2147, 0.2521, 0.2211],\n",
      "          [0.1432, 0.1546, 0.2031, 0.2599, 0.2391],\n",
      "          [0.1554, 0.1793, 0.1946, 0.2585, 0.2123],\n",
      "          [0.1515, 0.1499, 0.2150, 0.2575, 0.2261],\n",
      "          [0.1646, 0.1787, 0.1912, 0.2310, 0.2345],\n",
      "          [0.1547, 0.1459, 0.2280, 0.2735, 0.1979],\n",
      "          [0.1566, 0.1758, 0.2171, 0.2260, 0.2245],\n",
      "          [0.1495, 0.1779, 0.2134, 0.2241, 0.2350]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 16, 16])\n",
      "scaled_prod: \n",
      " tensor([[[[-2.4650e-01, -3.4137e-01, -2.8615e-01,  ..., -4.5123e-01,\n",
      "           -4.1150e-01, -4.4268e-01],\n",
      "          [-4.3489e-01, -5.4310e-01, -5.9655e-01,  ..., -7.0283e-01,\n",
      "           -7.0056e-01, -7.4900e-01],\n",
      "          [-4.2493e-01, -5.1379e-01, -5.3663e-01,  ..., -6.5131e-01,\n",
      "           -6.2357e-01, -6.4858e-01],\n",
      "          ...,\n",
      "          [-2.3917e-01, -3.1334e-01, -3.4382e-01,  ..., -5.9361e-01,\n",
      "           -4.9184e-01, -4.5857e-01],\n",
      "          [-1.2221e-01, -1.7846e-01, -2.2213e-01,  ..., -4.9693e-01,\n",
      "           -4.1891e-01, -4.3718e-01],\n",
      "          [-3.6215e-02, -3.4852e-02, -2.9422e-02,  ..., -2.9019e-01,\n",
      "           -2.9393e-01, -2.8392e-01]],\n",
      "\n",
      "         [[ 3.0243e-01,  4.2469e-01,  6.0822e-01,  ...,  5.1776e-01,\n",
      "            3.5493e-01,  5.0222e-01],\n",
      "          [ 1.8240e-01,  3.8942e-01,  5.9489e-01,  ...,  6.6360e-01,\n",
      "            2.8244e-01,  4.7042e-01],\n",
      "          [ 2.1348e-01,  3.4515e-01,  4.3892e-01,  ...,  5.3265e-01,\n",
      "            2.7923e-01,  4.2719e-01],\n",
      "          ...,\n",
      "          [-6.9711e-02,  5.2868e-04,  3.2112e-01,  ...,  3.8358e-01,\n",
      "            7.2301e-02,  1.9184e-01],\n",
      "          [ 3.0607e-01,  4.2788e-01,  6.3997e-01,  ...,  6.0249e-01,\n",
      "            4.0004e-01,  5.5431e-01],\n",
      "          [ 1.4355e-01,  1.5607e-01,  4.1580e-01,  ...,  4.9653e-01,\n",
      "            3.1649e-01,  4.9276e-01]],\n",
      "\n",
      "         [[ 3.3510e-01,  2.3555e-01,  2.7683e-01,  ...,  1.7336e-01,\n",
      "            9.3833e-02,  3.4600e-02],\n",
      "          [ 1.6161e-01,  1.2555e-02,  2.3299e-01,  ...,  7.4397e-02,\n",
      "            4.4365e-02, -1.9761e-02],\n",
      "          [ 3.0784e-01,  2.1950e-01,  2.8945e-01,  ...,  1.5041e-01,\n",
      "            1.5716e-01,  1.2693e-01],\n",
      "          ...,\n",
      "          [ 2.0485e-01,  2.7034e-01,  2.9614e-01,  ...,  1.5127e-01,\n",
      "            1.3500e-01,  1.4909e-01],\n",
      "          [ 1.4505e-01,  1.0749e-01,  2.1157e-01,  ...,  8.4375e-02,\n",
      "           -9.6579e-03,  3.8855e-02],\n",
      "          [ 2.6122e-01,  3.4265e-01,  3.0786e-01,  ...,  1.9188e-01,\n",
      "            2.2556e-01,  3.3238e-01]],\n",
      "\n",
      "         [[ 3.5449e-01,  1.3955e-01,  3.8258e-01,  ...,  2.5712e-01,\n",
      "            2.0804e-01,  1.5020e-01],\n",
      "          [ 2.7276e-01,  5.8613e-02,  2.7676e-01,  ...,  1.6697e-01,\n",
      "            8.7345e-02,  4.6054e-02],\n",
      "          [ 3.7379e-01,  6.2104e-02,  3.1193e-01,  ...,  2.2083e-01,\n",
      "            1.7914e-01,  1.0853e-01],\n",
      "          ...,\n",
      "          [ 5.1445e-01,  2.7521e-01,  5.8219e-01,  ...,  4.3298e-01,\n",
      "            2.9521e-01,  2.3063e-01],\n",
      "          [ 5.9308e-01,  2.1354e-01,  5.0024e-01,  ...,  4.3554e-01,\n",
      "            3.8418e-01,  2.9192e-01],\n",
      "          [ 4.8380e-01,  1.6004e-01,  4.6548e-01,  ...,  3.4675e-01,\n",
      "            2.4796e-01,  2.3624e-01]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.0699, 0.0636, 0.0672,  ..., 0.0570, 0.0593, 0.0574],\n",
      "          [0.0725, 0.0650, 0.0616,  ..., 0.0554, 0.0556, 0.0529],\n",
      "          [0.0713, 0.0652, 0.0638,  ..., 0.0569, 0.0585, 0.0570],\n",
      "          ...,\n",
      "          [0.0725, 0.0673, 0.0653,  ..., 0.0508, 0.0563, 0.0582],\n",
      "          [0.0738, 0.0697, 0.0668,  ..., 0.0507, 0.0548, 0.0538],\n",
      "          [0.0689, 0.0690, 0.0694,  ..., 0.0535, 0.0533, 0.0538]],\n",
      "\n",
      "         [[0.0531, 0.0600, 0.0721,  ..., 0.0659, 0.0560, 0.0649],\n",
      "          [0.0475, 0.0584, 0.0717,  ..., 0.0768, 0.0525, 0.0633],\n",
      "          [0.0540, 0.0616, 0.0677,  ..., 0.0743, 0.0577, 0.0669],\n",
      "          ...,\n",
      "          [0.0524, 0.0562, 0.0774,  ..., 0.0824, 0.0604, 0.0680],\n",
      "          [0.0522, 0.0590, 0.0729,  ..., 0.0702, 0.0573, 0.0669],\n",
      "          [0.0513, 0.0520, 0.0674,  ..., 0.0730, 0.0610, 0.0728]],\n",
      "\n",
      "         [[0.0671, 0.0607, 0.0633,  ..., 0.0571, 0.0527, 0.0497],\n",
      "          [0.0645, 0.0555, 0.0692,  ..., 0.0591, 0.0573, 0.0538],\n",
      "          [0.0663, 0.0607, 0.0651,  ..., 0.0567, 0.0571, 0.0554],\n",
      "          ...,\n",
      "          [0.0618, 0.0660, 0.0677,  ..., 0.0586, 0.0576, 0.0585],\n",
      "          [0.0641, 0.0617, 0.0685,  ..., 0.0603, 0.0549, 0.0576],\n",
      "          [0.0618, 0.0670, 0.0647,  ..., 0.0576, 0.0596, 0.0663]],\n",
      "\n",
      "         [[0.0742, 0.0598, 0.0763,  ..., 0.0673, 0.0641, 0.0605],\n",
      "          [0.0723, 0.0584, 0.0726,  ..., 0.0651, 0.0601, 0.0577],\n",
      "          [0.0752, 0.0551, 0.0707,  ..., 0.0645, 0.0619, 0.0577],\n",
      "          ...,\n",
      "          [0.0737, 0.0581, 0.0789,  ..., 0.0680, 0.0592, 0.0555],\n",
      "          [0.0794, 0.0543, 0.0724,  ..., 0.0678, 0.0645, 0.0588],\n",
      "          [0.0797, 0.0577, 0.0783,  ..., 0.0695, 0.0630, 0.0622]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([1, 4, 16, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.2133, -0.6827,  0.5437, -0.1955, -0.4385],\n",
      "          [ 0.1583, -0.5790,  0.3339, -0.2313, -0.2476],\n",
      "          [ 0.1583, -0.6453,  0.4041, -0.0757, -0.2990],\n",
      "          [ 0.1468, -0.4603,  0.5560,  0.2305, -0.2964],\n",
      "          [ 0.2261, -0.3878,  0.5017,  0.0853, -0.2142],\n",
      "          [ 0.2069, -0.4856,  0.6413,  0.1121, -0.4535],\n",
      "          [ 0.2866, -0.5680,  0.3593, -0.1290, -0.3979],\n",
      "          [ 0.2022, -0.5002,  0.4392, -0.0701, -0.3769],\n",
      "          [ 0.1791, -0.3737,  0.3195, -0.0870, -0.2578],\n",
      "          [ 0.2277, -0.5616,  0.4371, -0.1067, -0.3462],\n",
      "          [ 0.0935, -0.5719,  0.3927, -0.0273, -0.3284],\n",
      "          [ 0.2029, -0.6241,  0.3933, -0.0470, -0.3915],\n",
      "          [ 0.0138, -0.8280,  0.3293, -0.2002, -0.4301],\n",
      "          [ 0.0876, -0.6531,  0.4069, -0.0083, -0.3333],\n",
      "          [ 0.0839, -0.6715,  0.6876,  0.0221, -0.3711],\n",
      "          [-0.0308, -0.8655,  0.5723,  0.0278, -0.3735]],\n",
      "\n",
      "         [[-0.0312, -0.6580, -0.4884, -0.5289, -0.0833],\n",
      "          [-0.2709, -0.8845, -0.7668, -0.4715, -0.3632],\n",
      "          [-0.0901, -0.8398, -0.4975, -0.3626, -0.2427],\n",
      "          [ 0.0639, -0.5917, -0.6870, -0.3063, -0.2101],\n",
      "          [-0.0542, -0.5088, -0.5949, -0.3828,  0.0395],\n",
      "          [-0.0330, -0.4247, -0.4775, -0.3813, -0.0100],\n",
      "          [-0.0858, -0.4470, -0.4326, -0.2292,  0.2052],\n",
      "          [-0.1094, -0.4684, -0.2882, -0.1516,  0.0219],\n",
      "          [ 0.0425, -0.5038, -0.5040, -0.2005,  0.0605],\n",
      "          [-0.0124, -0.4807, -0.4688, -0.1647, -0.0087],\n",
      "          [-0.1771, -0.7008, -0.5393, -0.3346, -0.0751],\n",
      "          [ 0.0470, -0.4772, -0.4398, -0.2436, -0.1234],\n",
      "          [-0.1668, -0.6272, -0.3958, -0.4586, -0.1144],\n",
      "          [-0.1538, -0.2658, -0.4735, -0.1989, -0.1331],\n",
      "          [-0.0556, -0.3595, -0.5243, -0.3908, -0.1540],\n",
      "          [-0.1810, -0.4967, -0.3844, -0.3863, -0.1956]],\n",
      "\n",
      "         [[ 0.0972, -0.1787,  0.3635,  0.0347,  0.0854],\n",
      "          [ 0.2251, -0.1273,  0.2056,  0.2048,  0.2074],\n",
      "          [ 0.1092, -0.0948,  0.2285, -0.2141,  0.1102],\n",
      "          [ 0.2036, -0.1015,  0.2185, -0.2228,  0.2228],\n",
      "          [ 0.0144, -0.1748,  0.3638, -0.3884, -0.0279],\n",
      "          [ 0.2160, -0.2049,  0.2349, -0.2877, -0.0442],\n",
      "          [-0.0331, -0.4447, -0.0703, -0.5369, -0.1909],\n",
      "          [ 0.1212, -0.2612,  0.2757, -0.0304,  0.0907],\n",
      "          [-0.1023, -0.3713, -0.1295, -0.2618, -0.2534],\n",
      "          [-0.0993, -0.2865,  0.1384, -0.1605, -0.2349],\n",
      "          [ 0.0946, -0.0769,  0.0757,  0.1162,  0.1355],\n",
      "          [-0.2630, -0.4588,  0.0902, -0.3430, -0.1813],\n",
      "          [-0.0437, -0.2554,  0.1947, -0.0534, -0.0753],\n",
      "          [-0.0933, -0.3594,  0.0621, -0.2712, -0.1612],\n",
      "          [-0.0677, -0.3650,  0.0280, -0.1551, -0.2317],\n",
      "          [ 0.0335, -0.2350,  0.1377, -0.2129, -0.0890]],\n",
      "\n",
      "         [[ 0.0660, -0.3831, -0.2817, -0.0804,  0.0104],\n",
      "          [-0.2455, -0.3555, -0.1898, -0.0084,  0.0783],\n",
      "          [-0.0500, -0.2565, -0.2894, -0.1098, -0.0375],\n",
      "          [ 0.3057, -0.1075, -0.0244,  0.2822,  0.2941],\n",
      "          [ 0.1910, -0.4355, -0.2033, -0.1526, -0.0850],\n",
      "          [ 0.2959, -0.2439, -0.1850,  0.1158, -0.0037],\n",
      "          [ 0.1685, -0.2608, -0.2338, -0.0180, -0.0295],\n",
      "          [-0.1541, -0.2867, -0.3470,  0.2186, -0.0201],\n",
      "          [ 0.0648, -0.1583, -0.3460,  0.1074,  0.0183],\n",
      "          [-0.0431, -0.1661, -0.2803,  0.0364,  0.1112],\n",
      "          [-0.0420, -0.3255, -0.3855,  0.0725,  0.0654],\n",
      "          [ 0.1779, -0.2882, -0.3616,  0.0530,  0.0449],\n",
      "          [ 0.1443, -0.1311, -0.2459,  0.0101, -0.0263],\n",
      "          [-0.1364, -0.2165, -0.2776, -0.1000,  0.0543],\n",
      "          [ 0.0584, -0.1776, -0.1682,  0.0212,  0.0226],\n",
      "          [ 0.0704, -0.1519, -0.2796,  0.0364, -0.0788]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2509, 0.1024, 0.3492, 0.1667, 0.1308],\n",
      "          [0.2491, 0.1192, 0.2969, 0.1687, 0.1660],\n",
      "          [0.2409, 0.1079, 0.3081, 0.1907, 0.1525],\n",
      "          [0.2092, 0.1140, 0.3150, 0.2275, 0.1343],\n",
      "          [0.2288, 0.1238, 0.3014, 0.1987, 0.1473],\n",
      "          [0.2237, 0.1119, 0.3454, 0.2035, 0.1156],\n",
      "          [0.2728, 0.1161, 0.2934, 0.1801, 0.1376],\n",
      "          [0.2448, 0.1213, 0.3103, 0.1865, 0.1372],\n",
      "          [0.2416, 0.1390, 0.2781, 0.1852, 0.1561],\n",
      "          [0.2521, 0.1145, 0.3109, 0.1805, 0.1420],\n",
      "          [0.2270, 0.1167, 0.3062, 0.2012, 0.1489],\n",
      "          [0.2514, 0.1099, 0.3041, 0.1958, 0.1387],\n",
      "          [0.2353, 0.1014, 0.3225, 0.1899, 0.1509],\n",
      "          [0.2263, 0.1079, 0.3115, 0.2057, 0.1486],\n",
      "          [0.2052, 0.0964, 0.3753, 0.1929, 0.1302],\n",
      "          [0.1987, 0.0863, 0.3632, 0.2107, 0.1411]],\n",
      "\n",
      "         [[0.2685, 0.1434, 0.1700, 0.1632, 0.2549],\n",
      "          [0.2577, 0.1395, 0.1569, 0.2109, 0.2350],\n",
      "          [0.2661, 0.1257, 0.1771, 0.2026, 0.2284],\n",
      "          [0.2905, 0.1508, 0.1371, 0.2006, 0.2209],\n",
      "          [0.2478, 0.1573, 0.1443, 0.1784, 0.2722],\n",
      "          [0.2471, 0.1670, 0.1585, 0.1745, 0.2529],\n",
      "          [0.2170, 0.1512, 0.1534, 0.1880, 0.2903],\n",
      "          [0.2158, 0.1507, 0.1805, 0.2069, 0.2461],\n",
      "          [0.2525, 0.1462, 0.1462, 0.1980, 0.2571],\n",
      "          [0.2426, 0.1519, 0.1537, 0.2083, 0.2435],\n",
      "          [0.2353, 0.1394, 0.1638, 0.2010, 0.2606],\n",
      "          [0.2633, 0.1559, 0.1618, 0.1969, 0.2221],\n",
      "          [0.2366, 0.1493, 0.1882, 0.1767, 0.2493],\n",
      "          [0.2175, 0.1945, 0.1580, 0.2079, 0.2221],\n",
      "          [0.2509, 0.1852, 0.1570, 0.1795, 0.2274],\n",
      "          [0.2301, 0.1678, 0.1878, 0.1874, 0.2268]],\n",
      "\n",
      "         [[0.2003, 0.1520, 0.2615, 0.1882, 0.1980],\n",
      "          [0.2152, 0.1513, 0.2111, 0.2109, 0.2115],\n",
      "          [0.2143, 0.1747, 0.2414, 0.1551, 0.2145],\n",
      "          [0.2260, 0.1666, 0.2294, 0.1476, 0.2304],\n",
      "          [0.2052, 0.1698, 0.2910, 0.1372, 0.1967],\n",
      "          [0.2469, 0.1621, 0.2516, 0.1492, 0.1903],\n",
      "          [0.2448, 0.1622, 0.2359, 0.1479, 0.2091],\n",
      "          [0.2137, 0.1458, 0.2495, 0.1837, 0.2073],\n",
      "          [0.2247, 0.1717, 0.2187, 0.1916, 0.1932],\n",
      "          [0.2036, 0.1689, 0.2582, 0.1915, 0.1778],\n",
      "          [0.2046, 0.1724, 0.2008, 0.2091, 0.2132],\n",
      "          [0.1903, 0.1565, 0.2710, 0.1757, 0.2065],\n",
      "          [0.1985, 0.1606, 0.2519, 0.1966, 0.1923],\n",
      "          [0.2125, 0.1629, 0.2482, 0.1779, 0.1985],\n",
      "          [0.2170, 0.1612, 0.2388, 0.1988, 0.1842],\n",
      "          [0.2202, 0.1684, 0.2444, 0.1721, 0.1949]],\n",
      "\n",
      "         [[0.2407, 0.1536, 0.1700, 0.2079, 0.2277],\n",
      "          [0.1785, 0.1599, 0.1887, 0.2262, 0.2467],\n",
      "          [0.2195, 0.1786, 0.1728, 0.2068, 0.2223],\n",
      "          [0.2301, 0.1522, 0.1654, 0.2248, 0.2275],\n",
      "          [0.2720, 0.1454, 0.1834, 0.1929, 0.2064],\n",
      "          [0.2647, 0.1543, 0.1637, 0.2211, 0.1962],\n",
      "          [0.2519, 0.1640, 0.1685, 0.2090, 0.2066],\n",
      "          [0.1888, 0.1654, 0.1557, 0.2741, 0.2159],\n",
      "          [0.2242, 0.1793, 0.1486, 0.2339, 0.2140],\n",
      "          [0.2031, 0.1796, 0.1602, 0.2200, 0.2370],\n",
      "          [0.2129, 0.1603, 0.1510, 0.2387, 0.2370],\n",
      "          [0.2520, 0.1581, 0.1469, 0.2224, 0.2206],\n",
      "          [0.2407, 0.1828, 0.1630, 0.2105, 0.2030],\n",
      "          [0.1985, 0.1832, 0.1723, 0.2058, 0.2402],\n",
      "          [0.2215, 0.1749, 0.1766, 0.2134, 0.2137],\n",
      "          [0.2308, 0.1848, 0.1626, 0.2231, 0.1988]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "[34, 34, 34, 34, 34]\n",
      "predicted sentence: \n",
      " CCCCC\n"
     ]
    }
   ],
   "source": [
    "# Predicting next words\n",
    "sent = \"This is a simple sentence\"\n",
    "encoded_sent = encoding.encode(sent)\n",
    "enc_x = torch.tensor(encoded_sent).unsqueeze(0)\n",
    "dec_x = torch.tensor(encoding.encode(\"C\")).unsqueeze(0)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "predicted_tokens = []\n",
    "for _ in range(5):\n",
    "    output = transformer(enc_x=enc_x, dec_x=dec_x)\n",
    "    softmaxed = F.softmax(output, dim=-1)\n",
    "    predicted = softmaxed.argmax(dim=-1)\n",
    "    predicted_tokens.append(predicted.tolist()[-1][-1]) \n",
    "    dec_x = torch.cat((dec_x, predicted), dim=-1)\n",
    "\n",
    "print(predicted_tokens)\n",
    "print(f\"predicted sentence: \\n {encoding.decode(predicted_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a4533-0646-49c1-a22f-04f055be1cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5588cc54-99dc-481d-8a78-d7d9e8adfd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hi.', 'START Salut!')\n",
      "('Stop!', 'START Arrête-toi !')\n",
      "('Hi.', 'START Salut!')\n",
      "('Run!', 'START Cours\\u202f!')\n",
      "('Run!', 'START Courez\\u202f!')\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "\n",
    "class Tokens(Enum):\n",
    "    START = \"START \"\n",
    "    END = \"<|endoftext|>\"\n",
    "    PAD = \" PAD\"\n",
    "    START_NUM = 23380\n",
    "    END_NUM = 100257\n",
    "    PAD_NUM = 62854\n",
    "    \n",
    "\n",
    "class EnFrDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        with open(file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            # we want data indexes start from 0, but filter out the first header row\n",
    "            for i, row in enumerate(reader, start=-1):\n",
    "                if i == -1:\n",
    "                    continue\n",
    "                en = row[0]\n",
    "                fr = Tokens.START.value + row[1]\n",
    "                self._data.append(tuple([en, fr]))\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "\n",
    "    class Iterator():\n",
    "\n",
    "        def __init__(self, outer):\n",
    "            self.cur = 0\n",
    "            self.outer = outer\n",
    "\n",
    "        def __next__(self):\n",
    "            if self.cur == len(self.outer._data):\n",
    "                raise StopIteration()\n",
    "            cur = self.outer._data[self.cur]\n",
    "            self.cur += 1\n",
    "            return cur\n",
    "\n",
    "    def __iter__(self):\n",
    "        return EnFrDataset.Iterator(self)\n",
    "    \n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = EnFrDataset(\"../data/eng_-french.csv\", val_ratio=0.1)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d09eb729-53bb-420b-a608-73b1365e6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([13347, 13], [23380], [8375])\n",
      "([6869, 0], [23380], [18733])\n",
      "([6869, 0], [23380], [18733])\n",
      "([36981, 0], [23380, 64105], [64105, 64])\n",
      "([12978, 0], [23380], [65381])\n"
     ]
    }
   ],
   "source": [
    "class TokEnFrDataset(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def build_train_sample(en_str: str, dec_str: str):\n",
    "        en_encoded = encoding.encode(en_str)\n",
    "        dec_encoded = encoding.encode(dec_str)\n",
    "        dec_encoded.append(Tokens.END_NUM.value)\n",
    "        en_sents = []\n",
    "        dec_sents = []\n",
    "        target_sents = []\n",
    "        \n",
    "        for i in range(1, len(dec_encoded)):\n",
    "            dec_sents.append(dec_encoded[:i])\n",
    "            target_sents.append(dec_encoded[1: i + 1])\n",
    "        en_sents.extend([en_encoded] * len(dec_sents))\n",
    "        return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        self._dataset = EnFrDataset(file, partition, val_ratio=0)\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        i = 0\n",
    "        for en, fr in self._dataset:\n",
    "            for sample in self.build_train_sample(en, fr):\n",
    "                self._data.append(sample)\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "                i += 1\n",
    "\n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french.csv\", val_ratio=0.1)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16252c2e-6c54-44c5-a4aa-4cfb4de90897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([6869, 0], [23380], [18733]), ([36981, 0], [23380, 64105], [64105, 64]), ([12978, 0], [23380], [65381]), ([35079, 13], [23380, 16233, 1088], [16233, 1088, 13]), ([10903, 0], [23380], [14549])]\n",
      "(tensor([[ 6869,     0],\n",
      "        [36981,     0],\n",
      "        [12978,     0],\n",
      "        [35079,    13],\n",
      "        [10903,     0]]), tensor([[23380, 62854, 62854],\n",
      "        [23380, 64105, 62854],\n",
      "        [23380, 62854, 62854],\n",
      "        [23380, 16233,  1088],\n",
      "        [23380, 62854, 62854]]), tensor([[18733, 62854, 62854],\n",
      "        [64105,    64, 62854],\n",
      "        [65381, 62854, 62854],\n",
      "        [16233,  1088,    13],\n",
      "        [14549, 62854, 62854]]), tensor([[1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1],\n",
      "        [1, 1]]), tensor([[1, 0, 0],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 1],\n",
      "        [1, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    print(batch)\n",
    "    _x, _y, _label = list(zip(*batch))\n",
    "    enc_x = pad_sequence([torch.tensor(t) for t in _x], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    enc_y = pad_sequence([torch.tensor(t) for t in _y], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    label = pad_sequence([torch.tensor(t) for t in _label], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    enc_mask = build_padding_mask(x, pad_token=Tokens.PAD_NUM.value)\n",
    "    dec_mask = build_padding_mask(y, pad_token=Tokens.PAD_NUM.value)\n",
    "    return enc_x, dec_x, label, enc_mask, dec_mask\n",
    "\n",
    "training_generator = DataLoader(dataset, collate_fn=collate, batch_size=5, num_workers=0)\n",
    "for batch in training_generator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98593c81-b0c3-4145-95bc-1965620ceb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "[([6869, 0], [23380], [18733]), ([36981, 0], [23380, 64105], [64105, 64]), ([12978, 0], [23380], [65381]), ([35079, 13], [23380, 16233, 1088], [16233, 1088, 13]), ([10903, 0], [23380], [14549])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Loop over epochs\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m local_batch, local_labels \u001b[38;5;129;01min\u001b[39;00m training_generator:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# Transfer to GPU\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         local_batch, local_labels \u001b[38;5;241m=\u001b[39m local_batch\u001b[38;5;241m.\u001b[39mto(device), local_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# Model computations\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import torch\n",
    "import math\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# torch.set_default_device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "training_params = {\n",
    "    'collate_fn': collate,\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0}\n",
    "max_epochs = 5\n",
    "model_path = \"../data/model.pth\"\n",
    "\n",
    "# Generators\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french.csv\", val_ratio=0.05)\n",
    "dataloader = DataLoader(dataset, **training_params)\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "def train_epoch():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    transformer.train(True)\n",
    "    dataset.partition = Partition.TRAIN\n",
    "\n",
    "    for enc_x, dec_x, label, enc_mask, dec_mask in dataloader:\n",
    "        enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000\n",
    "            print('Average batch loss: {}'.format(last_loss))\n",
    "            running_loss = 0.\n",
    "    print('Average epoch loss: {}'.format(last_loss))\n",
    "    return last_loss\n",
    "\n",
    "def validate_epoch():\n",
    "    running_vloss = 0.0\n",
    "    transformer.eval()\n",
    "    dataset.partition = PARTITION.VAL\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = vdata\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "            output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "            vloss = loss_fn(output, label)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('Average valid loss {}'.format(avg_vloss))\n",
    "\n",
    "    return avg_vloss\n",
    "\n",
    "def train():\n",
    "    best_vloss = math.inf\n",
    "    for epoch in range(max_epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        avg_train_loss = train_epoch()\n",
    "        \n",
    "        avg_val_loss = validate_epoch()\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "            torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819338a-9df0-465b-b1cf-a0a6e8a59d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef7bbe-c8a8-493e-8677-49d9c02dbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def build_train_sample(en_str: str, dec_str: str):\n",
    "    en_encoded = encoding.encode(en_str)\n",
    "    dec_encoded = encoding.encode(dec_str)\n",
    "    dec_encoded.append(Tokens.END_NUM.value)\n",
    "    en_sents = []\n",
    "    dec_sents = []\n",
    "    target_sents = []\n",
    "    \n",
    "    for i in range(1, len(dec_encoded)):\n",
    "        dec_sents.append(dec_encoded[:i])\n",
    "        target_sents.append(dec_encoded[1: i + 1])\n",
    "    en_sents.extend([en_encoded] * len(dec_sents))\n",
    "    return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "build_train_sample('Hi.', 'START Salut!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfee8d-38b9-4abb-b127-5e620ea387f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# def collate_fn\n",
    "\n",
    "dataset.partition = Partition.TRAIN\n",
    "training_generator = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "for i, s in enumerate(training_generator):\n",
    "    print(s)\n",
    "    if i > 2:\n",
    "        break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d5995-4736-4fec-9e51-e3672a9dc765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62b3c0-a20a-466d-b5fe-05b33071e963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0dbc9-7e82-48b4-9b90-0ddf28b10784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21175a-9751-4413-8cdb-579c52134778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207745b-3455-48f2-a6c1-d6546569344a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbfc3e-aca8-4edb-99e5-025c945d85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da787162-9955-4de5-a82d-60501d62ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08f9da-be37-4a19-992e-11b2c6e1ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a2346-9e74-4eab-aab3-bd5bb0a0d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.3]]), dim=-1)\n",
    "torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]).argmax(dim=-1)\n",
    "# torch.max(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]), dim=-1)\n",
    "\n",
    "t1 = torch.tensor([[0.1, 0.2]])\n",
    "t2 = torch.tensor([[0.3]])\n",
    "torch.cat((t1, t2), dim=1).tolist()[-1][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683599c0-fd00-40b0-8bb1-4e477a2114a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45e55c-6d60-409b-b26e-0b218224483a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e70b0-a647-455b-bdfb-332d095aafc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082af878-9425-4445-b4b4-592cef79a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba170-623a-486b-8b67-58360bde104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d1a8-69aa-4c60-b3b4-ebb2e31f1c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
