{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n",
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    print(softmaxed_prod.shape)\n",
    "    # print(softmaxed_prod)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x)\n",
    "self_attention(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2289c87e-8e24-485d-9c54-b973ea4dbd76",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim)\n",
    "    if mask is not None:\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    # print(softmaxed_prod.shape)\n",
    "    print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8107, 0.0956, 0.5541, 0.6064],\n",
      "          [0.8614, 0.9672, 0.2899, 0.2617]],\n",
      "\n",
      "         [[0.1665, 0.0174, 0.0265, 0.1541],\n",
      "          [0.4462, 0.7755, 0.7630, 0.1741]],\n",
      "\n",
      "         [[0.9035, 0.3409, 0.1472, 0.5035],\n",
      "          [0.7944, 0.2942, 0.1435, 0.3047]]],\n",
      "\n",
      "\n",
      "        [[[0.9461, 0.7428, 0.6981, 0.3009],\n",
      "          [0.4644, 0.4661, 0.7124, 0.3408]],\n",
      "\n",
      "         [[0.5917, 0.8288, 0.6877, 0.3619],\n",
      "          [0.7105, 0.0435, 0.3940, 0.8153]],\n",
      "\n",
      "         [[0.6516, 0.3904, 0.9171, 0.0267],\n",
      "          [0.8246, 0.1015, 0.6486, 0.0196]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6705, 0.1224,   -inf],\n",
      "          [0.1224, 0.0262,   -inf],\n",
      "          [0.5759, 0.1189,   -inf]],\n",
      "\n",
      "         [[0.9149,   -inf,   -inf],\n",
      "          [0.7005,   -inf,   -inf],\n",
      "          [0.5450,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[1.0124, 0.8822,   -inf],\n",
      "          [0.8822, 0.8205,   -inf],\n",
      "          [0.7774, 0.6747,   -inf]],\n",
      "\n",
      "         [[0.5283,   -inf,   -inf],\n",
      "          [0.4544,   -inf,   -inf],\n",
      "          [0.4495,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6337, 0.3663, 0.0000],\n",
      "          [0.5240, 0.4760, 0.0000],\n",
      "          [0.6123, 0.3877, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5325, 0.4675, 0.0000],\n",
      "          [0.5154, 0.4846, 0.0000],\n",
      "          [0.5256, 0.4744, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "wrong a: \n",
      " tensor([[[[0.5747, 0.0670, 0.3608, 0.4407],\n",
      "          [0.5040, 0.0584, 0.3030, 0.3911],\n",
      "          [0.5609, 0.0653, 0.3495, 0.4310]],\n",
      "\n",
      "         [[0.8614, 0.9672, 0.2899, 0.2617],\n",
      "          [0.8614, 0.9672, 0.2899, 0.2617],\n",
      "          [0.8614, 0.9672, 0.2899, 0.2617]]],\n",
      "\n",
      "\n",
      "        [[[0.7804, 0.7830, 0.6933, 0.3294],\n",
      "          [0.7744, 0.7845, 0.6931, 0.3305],\n",
      "          [0.7780, 0.7836, 0.6932, 0.3298]],\n",
      "\n",
      "         [[0.4644, 0.4661, 0.7124, 0.3408],\n",
      "          [0.4644, 0.4661, 0.7124, 0.3408],\n",
      "          [0.4644, 0.4661, 0.7124, 0.3408]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6705, 0.1224,   -inf],\n",
      "          [0.1224, 0.0262,   -inf],\n",
      "          [0.5759, 0.1189,   -inf]],\n",
      "\n",
      "         [[0.9149, 0.7005,   -inf],\n",
      "          [0.7005, 0.7064,   -inf],\n",
      "          [0.5450, 0.3726,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[1.0124,   -inf,   -inf],\n",
      "          [0.8822,   -inf,   -inf],\n",
      "          [0.7774,   -inf,   -inf]],\n",
      "\n",
      "         [[0.5283,   -inf,   -inf],\n",
      "          [0.4544,   -inf,   -inf],\n",
      "          [0.4495,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6337, 0.3663, 0.0000],\n",
      "          [0.5240, 0.4760, 0.0000],\n",
      "          [0.6123, 0.3877, 0.0000]],\n",
      "\n",
      "         [[0.5534, 0.4466, 0.0000],\n",
      "          [0.4985, 0.5015, 0.0000],\n",
      "          [0.5430, 0.4570, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.5747, 0.0670, 0.3608, 0.4407],\n",
      "          [0.5040, 0.0584, 0.3030, 0.3911],\n",
      "          [0.5609, 0.0653, 0.3495, 0.4310]],\n",
      "\n",
      "         [[0.6760, 0.8816, 0.5012, 0.2225],\n",
      "          [0.6532, 0.8710, 0.5271, 0.2177],\n",
      "          [0.6717, 0.8796, 0.5061, 0.2216]]],\n",
      "\n",
      "\n",
      "        [[[0.9461, 0.7428, 0.6981, 0.3009],\n",
      "          [0.9461, 0.7428, 0.6981, 0.3009],\n",
      "          [0.9461, 0.7428, 0.6981, 0.3009]],\n",
      "\n",
      "         [[0.4644, 0.4661, 0.7124, 0.3408],\n",
      "          [0.4644, 0.4661, 0.7124, 0.3408],\n",
      "          [0.4644, 0.4661, 0.7124, 0.3408]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.8107, 0.0956, 0.5541, 0.6064],\n",
      "          [0.8614, 0.9672, 0.2899, 0.2617]],\n",
      "\n",
      "         [[0.1665, 0.0174, 0.0265, 0.1541],\n",
      "          [0.4462, 0.7755, 0.7630, 0.1741]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.9461, 0.7428, 0.6981, 0.3009],\n",
      "          [0.4644, 0.4661, 0.7124, 0.3408]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6705, 0.1224,   -inf],\n",
      "          [0.1224, 0.0262,   -inf],\n",
      "          [0.5759, 0.1189,   -inf]],\n",
      "\n",
      "         [[0.9149, 0.7005,   -inf],\n",
      "          [0.7005, 0.7064,   -inf],\n",
      "          [0.5450, 0.3726,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[1.0124,   -inf,   -inf],\n",
      "          [0.8822,   -inf,   -inf],\n",
      "          [0.7774,   -inf,   -inf]],\n",
      "\n",
      "         [[0.5283,   -inf,   -inf],\n",
      "          [0.4544,   -inf,   -inf],\n",
      "          [0.4495,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6337, 0.3663, 0.0000],\n",
      "          [0.5240, 0.4760, 0.0000],\n",
      "          [0.6123, 0.3877, 0.0000]],\n",
      "\n",
      "         [[0.5534, 0.4466, 0.0000],\n",
      "          [0.4985, 0.5015, 0.0000],\n",
      "          [0.5430, 0.4570, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.5747, 0.0670, 0.3608, 0.4407],\n",
      "          [0.5040, 0.0584, 0.3030, 0.3911],\n",
      "          [0.5609, 0.0653, 0.3495, 0.4310]],\n",
      "\n",
      "         [[0.6760, 0.8816, 0.5012, 0.2225],\n",
      "          [0.6532, 0.8710, 0.5271, 0.2177],\n",
      "          [0.6717, 0.8796, 0.5061, 0.2216]]],\n",
      "\n",
      "\n",
      "        [[[0.9461, 0.7428, 0.6981, 0.3009],\n",
      "          [0.9461, 0.7428, 0.6981, 0.3009],\n",
      "          [0.9461, 0.7428, 0.6981, 0.3009]],\n",
      "\n",
      "         [[0.4644, 0.4661, 0.7124, 0.3408],\n",
      "          [0.4644, 0.4661, 0.7124, 0.3408],\n",
      "          [0.4644, 0.4661, 0.7124, 0.3408]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.3141, 0.3955, 0.1021, 0.6880],\n",
      "         [0.7010, 0.2207, 0.8252, 0.3629],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3778, 0.4127, 0.8539, 0.8166],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.3141, 0.3955],\n",
      "          [0.1021, 0.6880]],\n",
      "\n",
      "         [[0.7010, 0.2207],\n",
      "          [0.8252, 0.3629]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3778, 0.4127],\n",
      "          [0.8539, 0.8166]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.3141, 0.3955],\n",
      "          [0.7010, 0.2207],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1021, 0.6880],\n",
      "          [0.8252, 0.3629],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3778, 0.4127],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.8539, 0.8166],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.6146, 0.4485, 0.0069, 0.1439],\n",
      "         [0.7584, 0.6575, 0.2087, 0.1301],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.8289, 0.3480, 0.4888, 0.4677],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.6146, 0.4485],\n",
      "          [0.0069, 0.1439]],\n",
      "\n",
      "         [[0.7584, 0.6575],\n",
      "          [0.2087, 0.1301]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8289, 0.3480],\n",
      "          [0.4888, 0.4677]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.6146, 0.4485],\n",
      "          [0.7584, 0.6575],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.0069, 0.1439],\n",
      "          [0.2087, 0.1301],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8289, 0.3480],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.4888, 0.4677],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[0.5790, 0.7610,   -inf],\n",
      "          [0.7610, 1.0074,   -inf],\n",
      "          [0.7792, 1.0577,   -inf]],\n",
      "\n",
      "         [[0.0208, 0.0202,   -inf],\n",
      "          [0.0202, 0.0605,   -inf],\n",
      "          [0.0981, 0.1211,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8082,   -inf,   -inf],\n",
      "          [0.6942,   -inf,   -inf],\n",
      "          [0.5632,   -inf,   -inf]],\n",
      "\n",
      "         [[0.4577,   -inf,   -inf],\n",
      "          [0.5519,   -inf,   -inf],\n",
      "          [0.4678,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.5518e-01, 5.5962e-02, 8.0046e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [3.8971e-01, 8.8086e-01, 3.5836e-01, 5.5926e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [5.0078e-01, 3.9874e-01, 4.1347e-01, 9.2675e-01, 1.2846e-01, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [5.7735e-01, 9.5074e-02, 3.7995e-02, 4.0825e-01, 6.2555e-01, 8.8979e-01]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.7740e-01, 3.8995e-01, 5.3129e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [2.5028e-01, 9.7232e-01, 3.4073e-01, 6.3566e-01, 7.6435e-01, 1.0000e+02],\n",
      "        [2.9299e-01, 1.5181e-02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0226e-02, 4.2712e-01, 2.9903e-02, 7.8080e-01, 4.9445e-01, 9.5061e-01]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(build_causal_mask(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0014,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0200,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0106,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0455,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0663,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1093,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0432,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0136,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0497,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0321,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0193,  0.0161,    -inf,    -inf,    -inf],\n",
      "          [ 0.0559,  0.0301,    -inf,    -inf,    -inf],\n",
      "          [ 0.0820,  0.0419,    -inf,    -inf,    -inf],\n",
      "          [ 0.0651,  0.0405,    -inf,    -inf,    -inf],\n",
      "          [ 0.0236,  0.0120,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1215, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0389, -0.0389,    -inf,    -inf,    -inf],\n",
      "          [-0.0583, -0.0465,    -inf,    -inf,    -inf],\n",
      "          [-0.0661, -0.0494,    -inf,    -inf,    -inf],\n",
      "          [-0.0963, -0.0959,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0310,  0.0904,  0.0703,    -inf,    -inf],\n",
      "          [-0.0278,  0.1313,  0.1196,    -inf,    -inf],\n",
      "          [-0.0448,  0.0183,  0.0406,    -inf,    -inf],\n",
      "          [ 0.0325,  0.0915,  0.0801,    -inf,    -inf],\n",
      "          [-0.0045,  0.0109,  0.0241,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0175, -0.0401, -0.0312,    -inf,    -inf],\n",
      "          [-0.0018, -0.0296, -0.0210,    -inf,    -inf],\n",
      "          [-0.1117, -0.1680, -0.1341,    -inf,    -inf],\n",
      "          [-0.0345, -0.1364, -0.1037,    -inf,    -inf],\n",
      "          [ 0.0030, -0.0373, -0.0277,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1063, -0.0132, -0.0701, -0.0066,    -inf],\n",
      "          [ 0.0464,  0.0505,  0.0313,  0.0305,    -inf],\n",
      "          [ 0.0224,  0.0647,  0.0573,  0.0286,    -inf],\n",
      "          [ 0.0590,  0.0833,  0.0605,  0.0484,    -inf],\n",
      "          [ 0.0446,  0.0850,  0.0726,  0.0619,    -inf]],\n",
      "\n",
      "         [[-0.0746, -0.0513, -0.0384, -0.0648,    -inf],\n",
      "          [-0.0212, -0.0389, -0.0334, -0.0091,    -inf],\n",
      "          [-0.0296, -0.0335, -0.0229, -0.0223,    -inf],\n",
      "          [-0.1685, -0.1486, -0.1067, -0.1376,    -inf],\n",
      "          [-0.0719, -0.0206, -0.0019, -0.0765,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5008, 0.4992, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5064, 0.4936, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5100, 0.4900, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5061, 0.4939, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5029, 0.4971, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4982, 0.5018, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4971, 0.5029, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4958, 0.5042, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4999, 0.5001, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3224, 0.3422, 0.3354, 0.0000, 0.0000],\n",
      "          [0.3002, 0.3520, 0.3479, 0.0000, 0.0000],\n",
      "          [0.3170, 0.3377, 0.3453, 0.0000, 0.0000],\n",
      "          [0.3216, 0.3412, 0.3373, 0.0000, 0.0000],\n",
      "          [0.3285, 0.3336, 0.3380, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3374, 0.3298, 0.3328, 0.0000, 0.0000],\n",
      "          [0.3386, 0.3293, 0.3321, 0.0000, 0.0000],\n",
      "          [0.3421, 0.3234, 0.3345, 0.0000, 0.0000],\n",
      "          [0.3526, 0.3184, 0.3290, 0.0000, 0.0000],\n",
      "          [0.3413, 0.3278, 0.3309, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2763, 0.2452, 0.2316, 0.2468, 0.0000],\n",
      "          [0.2517, 0.2527, 0.2479, 0.2477, 0.0000],\n",
      "          [0.2448, 0.2554, 0.2535, 0.2463, 0.0000],\n",
      "          [0.2490, 0.2552, 0.2494, 0.2464, 0.0000],\n",
      "          [0.2447, 0.2548, 0.2516, 0.2490, 0.0000]],\n",
      "\n",
      "         [[0.2457, 0.2515, 0.2547, 0.2481, 0.0000],\n",
      "          [0.2511, 0.2467, 0.2481, 0.2541, 0.0000],\n",
      "          [0.2494, 0.2484, 0.2511, 0.2512, 0.0000],\n",
      "          [0.2430, 0.2479, 0.2585, 0.2506, 0.0000],\n",
      "          [0.2427, 0.2554, 0.2603, 0.2416, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[ 0.5771, -0.4720,  0.1874,  0.3054,  0.1922, -0.1960],\n",
      "         [ 0.5771, -0.4720,  0.1874,  0.3054,  0.1922, -0.1960],\n",
      "         [ 0.5771, -0.4720,  0.1874,  0.3054,  0.1922, -0.1960],\n",
      "         [ 0.5771, -0.4720,  0.1874,  0.3054,  0.1922, -0.1960],\n",
      "         [ 0.5771, -0.4720,  0.1874,  0.3054,  0.1922, -0.1960]],\n",
      "\n",
      "        [[ 0.6446, -0.4560,  0.1453,  0.2794,  0.2935, -0.2271],\n",
      "         [ 0.6455, -0.4552,  0.1447,  0.2791,  0.2940, -0.2279],\n",
      "         [ 0.6447, -0.4546,  0.1437,  0.2798,  0.2935, -0.2277],\n",
      "         [ 0.6441, -0.4551,  0.1441,  0.2800,  0.2932, -0.2272],\n",
      "         [ 0.6452, -0.4557,  0.1452,  0.2791,  0.2939, -0.2276]],\n",
      "\n",
      "        [[ 0.6679, -0.4224,  0.1376,  0.2925,  0.2727, -0.1438],\n",
      "         [ 0.6686, -0.4220,  0.1363,  0.2930,  0.2716, -0.1419],\n",
      "         [ 0.6672, -0.4230,  0.1369,  0.2925,  0.2732, -0.1443],\n",
      "         [ 0.6646, -0.4237,  0.1361,  0.2939,  0.2718, -0.1432],\n",
      "         [ 0.6665, -0.4224,  0.1374,  0.2929,  0.2730, -0.1445]],\n",
      "\n",
      "        [[ 0.5868, -0.4865,  0.1343,  0.2899,  0.3121, -0.1605],\n",
      "         [ 0.5856, -0.4842,  0.1332,  0.2895,  0.3169, -0.1621],\n",
      "         [ 0.5845, -0.4841,  0.1325,  0.2897,  0.3183, -0.1625],\n",
      "         [ 0.5839, -0.4856,  0.1324,  0.2898,  0.3176, -0.1623],\n",
      "         [ 0.5830, -0.4855,  0.1323,  0.2902,  0.3180, -0.1626]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0014,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0200,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0106,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0455,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0663,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1093,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0432,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0136,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0497,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0321,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0193,  0.0161,    -inf,    -inf,    -inf],\n",
      "          [ 0.0559,  0.0301,    -inf,    -inf,    -inf],\n",
      "          [ 0.0820,  0.0419,    -inf,    -inf,    -inf],\n",
      "          [ 0.0651,  0.0405,    -inf,    -inf,    -inf],\n",
      "          [ 0.0236,  0.0120,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1215, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0389, -0.0389,    -inf,    -inf,    -inf],\n",
      "          [-0.0583, -0.0465,    -inf,    -inf,    -inf],\n",
      "          [-0.0661, -0.0494,    -inf,    -inf,    -inf],\n",
      "          [-0.0963, -0.0959,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0310,  0.0904,  0.0703,    -inf,    -inf],\n",
      "          [-0.0278,  0.1313,  0.1196,    -inf,    -inf],\n",
      "          [-0.0448,  0.0183,  0.0406,    -inf,    -inf],\n",
      "          [ 0.0325,  0.0915,  0.0801,    -inf,    -inf],\n",
      "          [-0.0045,  0.0109,  0.0241,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0175, -0.0401, -0.0312,    -inf,    -inf],\n",
      "          [-0.0018, -0.0296, -0.0210,    -inf,    -inf],\n",
      "          [-0.1117, -0.1680, -0.1341,    -inf,    -inf],\n",
      "          [-0.0345, -0.1364, -0.1037,    -inf,    -inf],\n",
      "          [ 0.0030, -0.0373, -0.0277,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1063, -0.0132, -0.0701, -0.0066,    -inf],\n",
      "          [ 0.0464,  0.0505,  0.0313,  0.0305,    -inf],\n",
      "          [ 0.0224,  0.0647,  0.0573,  0.0286,    -inf],\n",
      "          [ 0.0590,  0.0833,  0.0605,  0.0484,    -inf],\n",
      "          [ 0.0446,  0.0850,  0.0726,  0.0619,    -inf]],\n",
      "\n",
      "         [[-0.0746, -0.0513, -0.0384, -0.0648,    -inf],\n",
      "          [-0.0212, -0.0389, -0.0334, -0.0091,    -inf],\n",
      "          [-0.0296, -0.0335, -0.0229, -0.0223,    -inf],\n",
      "          [-0.1685, -0.1486, -0.1067, -0.1376,    -inf],\n",
      "          [-0.0719, -0.0206, -0.0019, -0.0765,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5008, 0.4992, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5064, 0.4936, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5100, 0.4900, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5061, 0.4939, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5029, 0.4971, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4982, 0.5018, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4971, 0.5029, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4958, 0.5042, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4999, 0.5001, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3224, 0.3422, 0.3354, 0.0000, 0.0000],\n",
      "          [0.3002, 0.3520, 0.3479, 0.0000, 0.0000],\n",
      "          [0.3170, 0.3377, 0.3453, 0.0000, 0.0000],\n",
      "          [0.3216, 0.3412, 0.3373, 0.0000, 0.0000],\n",
      "          [0.3285, 0.3336, 0.3380, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3374, 0.3298, 0.3328, 0.0000, 0.0000],\n",
      "          [0.3386, 0.3293, 0.3321, 0.0000, 0.0000],\n",
      "          [0.3421, 0.3234, 0.3345, 0.0000, 0.0000],\n",
      "          [0.3526, 0.3184, 0.3290, 0.0000, 0.0000],\n",
      "          [0.3413, 0.3278, 0.3309, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2763, 0.2452, 0.2316, 0.2468, 0.0000],\n",
      "          [0.2517, 0.2527, 0.2479, 0.2477, 0.0000],\n",
      "          [0.2448, 0.2554, 0.2535, 0.2463, 0.0000],\n",
      "          [0.2490, 0.2552, 0.2494, 0.2464, 0.0000],\n",
      "          [0.2447, 0.2548, 0.2516, 0.2490, 0.0000]],\n",
      "\n",
      "         [[0.2457, 0.2515, 0.2547, 0.2481, 0.0000],\n",
      "          [0.2511, 0.2467, 0.2481, 0.2541, 0.0000],\n",
      "          [0.2494, 0.2484, 0.2511, 0.2512, 0.0000],\n",
      "          [0.2430, 0.2479, 0.2585, 0.2506, 0.0000],\n",
      "          [0.2427, 0.2554, 0.2603, 0.2416, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(5, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([3, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 3.0806e-02,  3.6268e-02,  3.3409e-02],\n",
      "          [ 5.9624e-02,  1.1791e-02,  4.0476e-02],\n",
      "          [ 1.2191e-01,  9.2553e-02,  5.7812e-02]],\n",
      "\n",
      "         [[-1.4589e-01, -8.7561e-02, -8.8475e-02],\n",
      "          [-7.5746e-02, -4.1570e-02, -5.2788e-02],\n",
      "          [-1.0533e-01, -6.1622e-02, -7.0239e-02]],\n",
      "\n",
      "         [[-1.0476e-02, -1.0742e-01,  7.7187e-03],\n",
      "          [ 4.6579e-02, -8.6033e-02,  8.7998e-02],\n",
      "          [-3.9985e-04, -1.2417e-01,  5.3330e-02]],\n",
      "\n",
      "         [[-7.3987e-02, -1.7411e-02, -1.6982e-01],\n",
      "          [-1.5220e-02,  5.1016e-02, -1.0202e-01],\n",
      "          [ 2.4815e-02,  3.3514e-02,  3.2538e-02]],\n",
      "\n",
      "         [[ 5.1677e-02,  3.3789e-02,  3.2034e-02],\n",
      "          [ 1.3216e-01, -1.5081e-02,  6.2646e-02],\n",
      "          [ 1.3450e-01,  9.6251e-02,  1.1767e-01]],\n",
      "\n",
      "         [[ 1.6321e-01,  1.9196e-01,  1.8176e-01],\n",
      "          [ 1.7644e-01,  2.1071e-01,  2.3093e-01],\n",
      "          [ 2.0261e-01,  2.9076e-01,  3.4641e-01]],\n",
      "\n",
      "         [[ 1.0951e-02, -1.1630e-01, -1.2198e-01],\n",
      "          [ 1.5782e-01,  7.5039e-02, -7.2253e-02],\n",
      "          [ 1.7094e-01,  6.0526e-02,  3.9533e-02]],\n",
      "\n",
      "         [[ 5.4775e-02, -1.5312e-02,  6.3576e-02],\n",
      "          [-1.0704e-01, -9.3907e-02, -6.0709e-02],\n",
      "          [ 8.3312e-02,  8.4133e-02,  1.0924e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3420e-01,  7.5496e-02, -3.4151e-02],\n",
      "          [-7.3939e-02,  8.4070e-02, -4.1453e-02],\n",
      "          [-1.8919e-02,  1.3119e-01,  8.8483e-02]],\n",
      "\n",
      "         [[-1.9949e-01, -9.3400e-02, -9.9849e-02],\n",
      "          [-2.2902e-01, -1.6789e-01, -9.7246e-02],\n",
      "          [-1.8175e-01, -1.7924e-01, -1.1034e-01]],\n",
      "\n",
      "         [[-7.1216e-03,  1.0851e-01, -3.5608e-02],\n",
      "          [ 1.6505e-02,  1.1343e-01,  1.4047e-02],\n",
      "          [ 6.6901e-03,  2.2614e-01,  2.1164e-02]],\n",
      "\n",
      "         [[-2.4999e-02,  5.5091e-02, -1.0960e-01],\n",
      "          [-1.4347e-01, -5.8202e-02, -1.6842e-01],\n",
      "          [-1.1891e-01, -8.3696e-02, -1.7275e-01]],\n",
      "\n",
      "         [[ 1.1402e-01, -3.9109e-02,  1.6636e-02],\n",
      "          [ 1.7062e-01, -2.1314e-02,  1.9139e-02],\n",
      "          [ 1.5318e-01, -8.3021e-03,  4.7982e-02]],\n",
      "\n",
      "         [[ 2.7266e-01,  2.0085e-01,  1.5999e-01],\n",
      "          [ 2.3805e-01,  2.1203e-01,  1.6853e-01],\n",
      "          [ 3.0081e-01,  2.5901e-01,  3.0678e-01]],\n",
      "\n",
      "         [[-1.1087e-01, -8.4294e-02, -9.4535e-02],\n",
      "          [-2.8816e-02, -3.9785e-02, -4.5841e-02],\n",
      "          [ 6.4680e-03, -2.4201e-02, -2.5964e-02]],\n",
      "\n",
      "         [[-5.2596e-02, -7.7847e-02, -1.0400e-01],\n",
      "          [ 1.9888e-02,  2.5077e-02, -1.7206e-02],\n",
      "          [-7.4883e-02, -4.6669e-02, -4.9033e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.3244e-02,  6.1719e-02,  1.1178e-01],\n",
      "          [ 4.9804e-03, -8.4235e-03,  8.8778e-02],\n",
      "          [ 1.8692e-02, -2.6487e-02,  2.6147e-02]],\n",
      "\n",
      "         [[-1.1597e-01, -5.8689e-02, -1.0192e-01],\n",
      "          [-3.9039e-02, -2.6203e-03, -5.1497e-02],\n",
      "          [-1.2375e-01, -7.8273e-02, -1.0024e-01]],\n",
      "\n",
      "         [[ 4.2964e-02,  3.1014e-02,  3.2723e-02],\n",
      "          [-5.3811e-02, -1.5304e-02, -2.2416e-02],\n",
      "          [-6.1111e-02, -5.8971e-02, -4.7932e-02]],\n",
      "\n",
      "         [[-1.7629e-01, -8.4343e-02, -5.2883e-02],\n",
      "          [-1.7174e-01, -8.9247e-02, -5.8921e-02],\n",
      "          [-2.6246e-01, -1.5675e-01, -1.0366e-01]],\n",
      "\n",
      "         [[ 1.2200e-01,  5.7021e-03, -3.2398e-02],\n",
      "          [ 1.7341e-01,  5.2608e-02,  1.8959e-04],\n",
      "          [ 9.3367e-02, -2.6069e-03, -9.7962e-03]],\n",
      "\n",
      "         [[ 2.1885e-01,  1.6837e-01,  1.6758e-01],\n",
      "          [ 1.8804e-01,  1.6061e-01,  1.8600e-01],\n",
      "          [ 2.4826e-01,  1.4756e-01,  2.3448e-01]],\n",
      "\n",
      "         [[-3.1281e-02,  8.6279e-02,  1.2856e-01],\n",
      "          [-5.9601e-02,  1.2770e-01,  1.1046e-01],\n",
      "          [-1.0335e-01,  5.3377e-02,  4.3938e-02]],\n",
      "\n",
      "         [[-2.3449e-02,  9.9285e-03, -2.1514e-02],\n",
      "          [-4.0457e-02, -1.1128e-01, -8.3721e-03],\n",
      "          [-1.0159e-01, -9.4710e-02, -6.4077e-02]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3324, 0.3343, 0.3333],\n",
      "          [0.3408, 0.3249, 0.3343],\n",
      "          [0.3438, 0.3338, 0.3224]],\n",
      "\n",
      "         [[0.3206, 0.3399, 0.3395],\n",
      "          [0.3270, 0.3384, 0.3346],\n",
      "          [0.3246, 0.3391, 0.3362]],\n",
      "\n",
      "         [[0.3418, 0.3102, 0.3480],\n",
      "          [0.3427, 0.3001, 0.3572],\n",
      "          [0.3403, 0.3007, 0.3591]],\n",
      "\n",
      "         [[0.3371, 0.3567, 0.3063],\n",
      "          [0.3350, 0.3579, 0.3071],\n",
      "          [0.3315, 0.3344, 0.3341]],\n",
      "\n",
      "         [[0.3375, 0.3315, 0.3310],\n",
      "          [0.3577, 0.3087, 0.3336],\n",
      "          [0.3395, 0.3267, 0.3338]],\n",
      "\n",
      "         [[0.3281, 0.3377, 0.3342],\n",
      "          [0.3235, 0.3348, 0.3417],\n",
      "          [0.3080, 0.3364, 0.3556]],\n",
      "\n",
      "         [[0.3628, 0.3195, 0.3177],\n",
      "          [0.3683, 0.3391, 0.2926],\n",
      "          [0.3607, 0.3230, 0.3163]],\n",
      "\n",
      "         [[0.3400, 0.3170, 0.3430],\n",
      "          [0.3267, 0.3310, 0.3422],\n",
      "          [0.3304, 0.3306, 0.3390]]],\n",
      "\n",
      "\n",
      "        [[[0.2995, 0.3694, 0.3311],\n",
      "          [0.3121, 0.3655, 0.3224],\n",
      "          [0.3053, 0.3548, 0.3399]],\n",
      "\n",
      "         [[0.3109, 0.3457, 0.3435],\n",
      "          [0.3121, 0.3318, 0.3561],\n",
      "          [0.3250, 0.3259, 0.3491]],\n",
      "\n",
      "         [[0.3232, 0.3628, 0.3141],\n",
      "          [0.3227, 0.3555, 0.3219],\n",
      "          [0.3068, 0.3820, 0.3112]],\n",
      "\n",
      "         [[0.3331, 0.3609, 0.3061],\n",
      "          [0.3263, 0.3554, 0.3183],\n",
      "          [0.3352, 0.3472, 0.3176]],\n",
      "\n",
      "         [[0.3616, 0.3103, 0.3281],\n",
      "          [0.3725, 0.3074, 0.3201],\n",
      "          [0.3635, 0.3093, 0.3272]],\n",
      "\n",
      "         [[0.3541, 0.3296, 0.3164],\n",
      "          [0.3440, 0.3351, 0.3209],\n",
      "          [0.3373, 0.3235, 0.3393]],\n",
      "\n",
      "         [[0.3286, 0.3374, 0.3340],\n",
      "          [0.3365, 0.3328, 0.3308],\n",
      "          [0.3404, 0.3301, 0.3295]],\n",
      "\n",
      "         [[0.3419, 0.3334, 0.3248],\n",
      "          [0.3368, 0.3386, 0.3246],\n",
      "          [0.3274, 0.3367, 0.3359]]],\n",
      "\n",
      "\n",
      "        [[[0.3215, 0.3308, 0.3477],\n",
      "          [0.3253, 0.3210, 0.3537],\n",
      "          [0.3375, 0.3226, 0.3400]],\n",
      "\n",
      "         [[0.3254, 0.3446, 0.3300],\n",
      "          [0.3306, 0.3429, 0.3265],\n",
      "          [0.3257, 0.3409, 0.3334]],\n",
      "\n",
      "         [[0.3358, 0.3318, 0.3324],\n",
      "          [0.3256, 0.3384, 0.3360],\n",
      "          [0.3316, 0.3323, 0.3360]],\n",
      "\n",
      "         [[0.3098, 0.3397, 0.3505],\n",
      "          [0.3120, 0.3388, 0.3492],\n",
      "          [0.3045, 0.3385, 0.3570]],\n",
      "\n",
      "         [[0.3640, 0.3240, 0.3119],\n",
      "          [0.3667, 0.3250, 0.3084],\n",
      "          [0.3558, 0.3233, 0.3209]],\n",
      "\n",
      "         [[0.3447, 0.3278, 0.3275],\n",
      "          [0.3366, 0.3275, 0.3359],\n",
      "          [0.3460, 0.3128, 0.3412]],\n",
      "\n",
      "         [[0.3032, 0.3410, 0.3558],\n",
      "          [0.2949, 0.3556, 0.3495],\n",
      "          [0.3004, 0.3514, 0.3481]],\n",
      "\n",
      "         [[0.3294, 0.3406, 0.3300],\n",
      "          [0.3374, 0.3143, 0.3484],\n",
      "          [0.3284, 0.3307, 0.3409]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-8.8507e-02,  3.0612e-02, -1.4397e-01],\n",
      "          [ 2.0472e-01, -1.0860e-01,  1.3086e-04],\n",
      "          [-6.0635e-02, -2.0549e-01, -1.5546e-02]],\n",
      "\n",
      "         [[-1.3060e-01, -7.7520e-03, -2.1857e-01],\n",
      "          [ 5.0694e-02, -2.6777e-02, -1.7208e-01],\n",
      "          [ 1.4561e-01, -1.7228e-02, -8.9293e-02]],\n",
      "\n",
      "         [[-1.4076e-01, -2.0828e-01,  5.9584e-03],\n",
      "          [-4.3054e-02, -9.1310e-02, -2.3225e-01],\n",
      "          [-4.4446e-02, -1.3883e-01, -1.3765e-01]],\n",
      "\n",
      "         [[ 3.6815e-02, -6.0098e-03, -1.0161e-01],\n",
      "          [-1.1115e-02, -1.4039e-01, -1.4587e-01],\n",
      "          [-1.4469e-01, -2.4625e-01, -1.1160e-02]],\n",
      "\n",
      "         [[-3.1741e-01, -1.0064e-01, -1.8030e-01],\n",
      "          [ 2.7645e-03, -3.6001e-02,  1.9972e-02],\n",
      "          [ 3.9643e-02,  1.0780e-01,  7.6731e-02]],\n",
      "\n",
      "         [[ 1.7247e-01,  2.8647e-01,  4.1020e-03],\n",
      "          [ 4.9795e-02,  1.2634e-01,  5.6332e-02],\n",
      "          [-1.9627e-01, -5.9937e-02, -1.5708e-01]],\n",
      "\n",
      "         [[ 1.5904e-01,  1.5359e-01,  1.4801e-01],\n",
      "          [-1.7281e-01, -1.7605e-01, -2.7583e-01],\n",
      "          [ 4.3807e-02, -2.3076e-01, -3.7939e-01]],\n",
      "\n",
      "         [[-1.8747e-01, -3.5763e-01, -1.5795e-01],\n",
      "          [-1.1109e-01, -2.2095e-01, -1.9217e-02],\n",
      "          [-5.3921e-03, -2.0281e-01,  4.6717e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0229e-01, -1.3308e-01, -4.8534e-02],\n",
      "          [ 9.7099e-02,  1.2009e-01,  5.3068e-02],\n",
      "          [ 1.2166e-01, -5.8485e-02,  1.8423e-02]],\n",
      "\n",
      "         [[ 7.4387e-02,  1.8016e-01,  5.3028e-02],\n",
      "          [-1.6197e-01, -4.8860e-02, -2.0605e-01],\n",
      "          [-3.1589e-01, -3.3923e-01, -4.0578e-01]],\n",
      "\n",
      "         [[-8.1069e-03,  3.3825e-02,  1.9372e-01],\n",
      "          [ 2.1793e-01,  2.1498e-01,  2.8169e-01],\n",
      "          [-4.5033e-01, -5.0218e-01, -2.7180e-01]],\n",
      "\n",
      "         [[-2.6482e-01, -1.5082e-01, -5.4104e-02],\n",
      "          [-7.6324e-02, -9.4377e-02, -3.2155e-02],\n",
      "          [-1.2585e-01, -4.2971e-02,  1.0117e-02]],\n",
      "\n",
      "         [[ 7.1441e-02, -1.0030e-02,  3.2174e-01],\n",
      "          [ 1.2389e-01,  9.6461e-02,  1.6531e-01],\n",
      "          [-7.9267e-02, -1.1789e-01, -9.5555e-02]],\n",
      "\n",
      "         [[-2.6670e-01,  3.1965e-02, -1.5777e-01],\n",
      "          [-9.5048e-02, -1.3952e-01,  1.4370e-01],\n",
      "          [-3.3062e-02,  5.9086e-02, -3.9235e-02]],\n",
      "\n",
      "         [[-4.0061e-01, -5.3018e-03, -1.0301e-01],\n",
      "          [-4.3715e-01, -2.0107e-01, -3.4905e-01],\n",
      "          [-1.0795e-01,  2.3564e-01,  2.5368e-01]],\n",
      "\n",
      "         [[ 2.2705e-04, -1.3724e-01, -5.1749e-02],\n",
      "          [ 1.2356e-01,  3.5202e-02, -1.7856e-01],\n",
      "          [ 1.4318e-02,  3.0157e-02, -1.7656e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6723e-01,  7.6559e-03, -1.1935e-02],\n",
      "          [ 3.4343e-02, -1.5267e-01, -2.0902e-01],\n",
      "          [-2.1442e-01, -2.6431e-01, -1.2874e-01]],\n",
      "\n",
      "         [[ 7.5030e-02,  5.4707e-02,  9.1812e-02],\n",
      "          [ 6.6498e-02, -1.4743e-01, -2.2329e-02],\n",
      "          [-3.4596e-01, -3.5999e-01, -3.7699e-01]],\n",
      "\n",
      "         [[ 6.0139e-02, -1.1698e-01,  2.1984e-01],\n",
      "          [ 2.4314e-01,  1.8321e-01,  1.2738e-01],\n",
      "          [ 1.6068e-02,  8.4365e-02,  1.6669e-01]],\n",
      "\n",
      "         [[ 3.1310e-01,  7.3640e-02,  1.6542e-01],\n",
      "          [ 1.1159e-01, -1.0177e-02, -1.9939e-01],\n",
      "          [-1.0761e-01, -8.9569e-02, -1.4431e-01]],\n",
      "\n",
      "         [[-2.1525e-02, -1.5816e-01, -1.8400e-01],\n",
      "          [ 1.4177e-03, -1.3796e-01,  1.3773e-02],\n",
      "          [ 3.4829e-02,  1.3641e-02,  1.6546e-01]],\n",
      "\n",
      "         [[ 1.3826e-01,  1.8474e-01,  3.6930e-02],\n",
      "          [-9.9633e-03, -3.5925e-02, -1.6914e-01],\n",
      "          [-2.0859e-02,  1.0045e-01, -2.4842e-02]],\n",
      "\n",
      "         [[ 7.8617e-03,  1.4810e-01, -5.5889e-02],\n",
      "          [-1.8276e-01, -1.1316e-01, -1.3658e-01],\n",
      "          [-4.2518e-01, -2.2349e-01, -3.6848e-01]],\n",
      "\n",
      "         [[-1.6873e-01, -2.2510e-01, -1.3901e-01],\n",
      "          [ 3.5165e-01,  1.4897e-01,  9.8051e-02],\n",
      "          [-1.8111e-01, -1.9906e-01, -3.1391e-01]]]], grad_fn=<DivBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3255, 0.3666, 0.3079],\n",
      "          [0.3928, 0.2871, 0.3201],\n",
      "          [0.3435, 0.2972, 0.3593]],\n",
      "\n",
      "         [[0.3282, 0.3712, 0.3006],\n",
      "          [0.3669, 0.3395, 0.2936],\n",
      "          [0.3787, 0.3218, 0.2994]],\n",
      "\n",
      "         [[0.3233, 0.3022, 0.3744],\n",
      "          [0.3596, 0.3427, 0.2977],\n",
      "          [0.3545, 0.3226, 0.3229]],\n",
      "\n",
      "         [[0.3535, 0.3387, 0.3078],\n",
      "          [0.3633, 0.3192, 0.3175],\n",
      "          [0.3283, 0.2966, 0.3752]],\n",
      "\n",
      "         [[0.2951, 0.3665, 0.3384],\n",
      "          [0.3356, 0.3229, 0.3415],\n",
      "          [0.3217, 0.3444, 0.3339]],\n",
      "\n",
      "         [[0.3372, 0.3779, 0.2849],\n",
      "          [0.3240, 0.3498, 0.3262],\n",
      "          [0.3139, 0.3597, 0.3264]],\n",
      "\n",
      "         [[0.3352, 0.3333, 0.3315],\n",
      "          [0.3450, 0.3438, 0.3112],\n",
      "          [0.4141, 0.3147, 0.2712]],\n",
      "\n",
      "         [[0.3480, 0.2936, 0.3584],\n",
      "          [0.3342, 0.2994, 0.3664],\n",
      "          [0.3479, 0.2856, 0.3665]]],\n",
      "\n",
      "\n",
      "        [[[0.3306, 0.3206, 0.3488],\n",
      "          [0.3355, 0.3434, 0.3211],\n",
      "          [0.3654, 0.3051, 0.3295]],\n",
      "\n",
      "         [[0.3236, 0.3597, 0.3167],\n",
      "          [0.3250, 0.3640, 0.3110],\n",
      "          [0.3459, 0.3379, 0.3162]],\n",
      "\n",
      "         [[0.3061, 0.3193, 0.3746],\n",
      "          [0.3265, 0.3255, 0.3480],\n",
      "          [0.3180, 0.3019, 0.3801]],\n",
      "\n",
      "         [[0.2980, 0.3340, 0.3679],\n",
      "          [0.3303, 0.3244, 0.3452],\n",
      "          [0.3094, 0.3361, 0.3545]],\n",
      "\n",
      "         [[0.3119, 0.2875, 0.4006],\n",
      "          [0.3316, 0.3227, 0.3457],\n",
      "          [0.3394, 0.3266, 0.3340]],\n",
      "\n",
      "         [[0.2888, 0.3893, 0.3220],\n",
      "          [0.3100, 0.2965, 0.3936],\n",
      "          [0.3236, 0.3548, 0.3216]],\n",
      "\n",
      "         [[0.2610, 0.3875, 0.3515],\n",
      "          [0.2978, 0.3771, 0.3252],\n",
      "          [0.2600, 0.3666, 0.3733]],\n",
      "\n",
      "         [[0.3545, 0.3090, 0.3365],\n",
      "          [0.3767, 0.3448, 0.2785],\n",
      "          [0.3518, 0.3575, 0.2907]]],\n",
      "\n",
      "\n",
      "        [[[0.3720, 0.3171, 0.3109],\n",
      "          [0.3826, 0.3174, 0.3000],\n",
      "          [0.3289, 0.3129, 0.3583]],\n",
      "\n",
      "         [[0.3337, 0.3270, 0.3393],\n",
      "          [0.3673, 0.2966, 0.3361],\n",
      "          [0.3384, 0.3336, 0.3280]],\n",
      "\n",
      "         [[0.3321, 0.2782, 0.3896],\n",
      "          [0.3530, 0.3325, 0.3145],\n",
      "          [0.3093, 0.3311, 0.3596]],\n",
      "\n",
      "         [[0.3774, 0.2970, 0.3256],\n",
      "          [0.3820, 0.3382, 0.2799],\n",
      "          [0.3353, 0.3414, 0.3232]],\n",
      "\n",
      "         [[0.3673, 0.3204, 0.3122],\n",
      "          [0.3469, 0.3018, 0.3513],\n",
      "          [0.3207, 0.3139, 0.3654]],\n",
      "\n",
      "         [[0.3388, 0.3550, 0.3062],\n",
      "          [0.3537, 0.3446, 0.3017],\n",
      "          [0.3200, 0.3613, 0.3187]],\n",
      "\n",
      "         [[0.3238, 0.3725, 0.3038],\n",
      "          [0.3206, 0.3437, 0.3357],\n",
      "          [0.3047, 0.3728, 0.3225]],\n",
      "\n",
      "         [[0.3361, 0.3177, 0.3462],\n",
      "          [0.3857, 0.3150, 0.2993],\n",
      "          [0.3499, 0.3437, 0.3064]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 512])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d, h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, x, y, self_mask=None, cross_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, y, y, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer()\n",
    "x = torch.rand(3, 3, 512)\n",
    "y = torch.rand(3, 3, 512)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "decoder_layer(x, y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5632df4-ca5b-459d-84eb-6a072c1f7966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c71b86a9-0ef8-40b8-a0ec-35af5b149823",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19e9d8-8b9d-4a12-9e41-b81eabef1866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False,  True]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
