{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkhokhlush/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n",
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    print(softmaxed_prod.shape)\n",
    "    # print(softmaxed_prod)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x)\n",
    "self_attention(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2289c87e-8e24-485d-9c54-b973ea4dbd76",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d626059-fd88-4a03-8b20-ffbf0115fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayerWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayerWithoutMask()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851eff3d-4816-4410-9184-f30c157e4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayerWithoutMask(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = EncoderWithoutMask()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With masks\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim)\n",
    "    if mask is not None:\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    # print(softmaxed_prod.shape)\n",
    "    print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7957, 0.7063, 0.5598, 0.2659],\n",
      "          [0.0494, 0.9377, 0.0659, 0.6770]],\n",
      "\n",
      "         [[0.4999, 0.0281, 0.6543, 0.8388],\n",
      "          [0.2058, 0.1664, 0.6835, 0.2599]],\n",
      "\n",
      "         [[0.0629, 0.8896, 0.1117, 0.4803],\n",
      "          [0.2376, 0.6843, 0.7369, 0.6880]]],\n",
      "\n",
      "\n",
      "        [[[0.3646, 0.3135, 0.5643, 0.1532],\n",
      "          [0.5727, 0.7404, 0.3154, 0.7842]],\n",
      "\n",
      "         [[0.8865, 0.5465, 0.5193, 0.5839],\n",
      "          [0.4778, 0.0358, 0.8320, 0.3449]],\n",
      "\n",
      "         [[0.6231, 0.6892, 0.2825, 0.2539],\n",
      "          [0.1048, 0.5844, 0.4393, 0.0215]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.7580, 0.5035,   -inf],\n",
      "          [0.5035, 0.6912,   -inf],\n",
      "          [0.4343, 0.2662,   -inf]],\n",
      "\n",
      "         [[0.6722,   -inf,   -inf],\n",
      "          [0.1936,   -inf,   -inf],\n",
      "          [0.5839,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.2866, 0.4385,   -inf],\n",
      "          [0.4385, 0.8476,   -inf],\n",
      "          [0.3208, 0.6120,   -inf]],\n",
      "\n",
      "         [[0.7954,   -inf,   -inf],\n",
      "          [0.4165,   -inf,   -inf],\n",
      "          [0.3241,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5633, 0.4367, 0.0000],\n",
      "          [0.4532, 0.5468, 0.0000],\n",
      "          [0.5419, 0.4581, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4621, 0.5379, 0.0000],\n",
      "          [0.3991, 0.6009, 0.0000],\n",
      "          [0.4277, 0.5723, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "wrong a: \n",
      " tensor([[[[0.6665, 0.4102, 0.6011, 0.5161],\n",
      "          [0.6339, 0.3355, 0.6115, 0.5792],\n",
      "          [0.6602, 0.3957, 0.6031, 0.5283]],\n",
      "\n",
      "         [[0.0494, 0.9377, 0.0659, 0.6770],\n",
      "          [0.0494, 0.9377, 0.0659, 0.6770],\n",
      "          [0.0494, 0.9377, 0.0659, 0.6770]]],\n",
      "\n",
      "\n",
      "        [[[0.6453, 0.4389, 0.5401, 0.3849],\n",
      "          [0.6782, 0.4535, 0.5372, 0.4120],\n",
      "          [0.6633, 0.4469, 0.5385, 0.3997]],\n",
      "\n",
      "         [[0.5727, 0.7404, 0.3154, 0.7842],\n",
      "          [0.5727, 0.7404, 0.3154, 0.7842],\n",
      "          [0.5727, 0.7404, 0.3154, 0.7842]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.7580, 0.5035,   -inf],\n",
      "          [0.5035, 0.6912,   -inf],\n",
      "          [0.4343, 0.2662,   -inf]],\n",
      "\n",
      "         [[0.6722, 0.1936,   -inf],\n",
      "          [0.1936, 0.3024,   -inf],\n",
      "          [0.5839, 0.4226,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.2866,   -inf,   -inf],\n",
      "          [0.4385,   -inf,   -inf],\n",
      "          [0.3208,   -inf,   -inf]],\n",
      "\n",
      "         [[0.7954,   -inf,   -inf],\n",
      "          [0.4165,   -inf,   -inf],\n",
      "          [0.3241,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5633, 0.4367, 0.0000],\n",
      "          [0.4532, 0.5468, 0.0000],\n",
      "          [0.5419, 0.4581, 0.0000]],\n",
      "\n",
      "         [[0.6174, 0.3826, 0.0000],\n",
      "          [0.4728, 0.5272, 0.0000],\n",
      "          [0.5402, 0.4598, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.6665, 0.4102, 0.6011, 0.5161],\n",
      "          [0.6339, 0.3355, 0.6115, 0.5792],\n",
      "          [0.6602, 0.3957, 0.6031, 0.5283]],\n",
      "\n",
      "         [[0.1092, 0.6426, 0.3022, 0.5175],\n",
      "          [0.1319, 0.5311, 0.3915, 0.4571],\n",
      "          [0.1213, 0.5831, 0.3499, 0.4853]]],\n",
      "\n",
      "\n",
      "        [[[0.3646, 0.3135, 0.5643, 0.1532],\n",
      "          [0.3646, 0.3135, 0.5643, 0.1532],\n",
      "          [0.3646, 0.3135, 0.5643, 0.1532]],\n",
      "\n",
      "         [[0.5727, 0.7404, 0.3154, 0.7842],\n",
      "          [0.5727, 0.7404, 0.3154, 0.7842],\n",
      "          [0.5727, 0.7404, 0.3154, 0.7842]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.7957, 0.7063, 0.5598, 0.2659],\n",
      "          [0.0494, 0.9377, 0.0659, 0.6770]],\n",
      "\n",
      "         [[0.4999, 0.0281, 0.6543, 0.8388],\n",
      "          [0.2058, 0.1664, 0.6835, 0.2599]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.3646, 0.3135, 0.5643, 0.1532],\n",
      "          [0.5727, 0.7404, 0.3154, 0.7842]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.7580, 0.5035,   -inf],\n",
      "          [0.5035, 0.6912,   -inf],\n",
      "          [0.4343, 0.2662,   -inf]],\n",
      "\n",
      "         [[0.6722, 0.1936,   -inf],\n",
      "          [0.1936, 0.3024,   -inf],\n",
      "          [0.5839, 0.4226,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.2866,   -inf,   -inf],\n",
      "          [0.4385,   -inf,   -inf],\n",
      "          [0.3208,   -inf,   -inf]],\n",
      "\n",
      "         [[0.7954,   -inf,   -inf],\n",
      "          [0.4165,   -inf,   -inf],\n",
      "          [0.3241,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5633, 0.4367, 0.0000],\n",
      "          [0.4532, 0.5468, 0.0000],\n",
      "          [0.5419, 0.4581, 0.0000]],\n",
      "\n",
      "         [[0.6174, 0.3826, 0.0000],\n",
      "          [0.4728, 0.5272, 0.0000],\n",
      "          [0.5402, 0.4598, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.6665, 0.4102, 0.6011, 0.5161],\n",
      "          [0.6339, 0.3355, 0.6115, 0.5792],\n",
      "          [0.6602, 0.3957, 0.6031, 0.5283]],\n",
      "\n",
      "         [[0.1092, 0.6426, 0.3022, 0.5175],\n",
      "          [0.1319, 0.5311, 0.3915, 0.4571],\n",
      "          [0.1213, 0.5831, 0.3499, 0.4853]]],\n",
      "\n",
      "\n",
      "        [[[0.3646, 0.3135, 0.5643, 0.1532],\n",
      "          [0.3646, 0.3135, 0.5643, 0.1532],\n",
      "          [0.3646, 0.3135, 0.5643, 0.1532]],\n",
      "\n",
      "         [[0.5727, 0.7404, 0.3154, 0.7842],\n",
      "          [0.5727, 0.7404, 0.3154, 0.7842],\n",
      "          [0.5727, 0.7404, 0.3154, 0.7842]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.0620, 0.4805, 0.3683, 0.0122],\n",
      "         [0.5602, 0.2121, 0.1370, 0.5828],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5538, 0.2025, 0.4294, 0.7124],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.0620, 0.4805],\n",
      "          [0.3683, 0.0122]],\n",
      "\n",
      "         [[0.5602, 0.2121],\n",
      "          [0.1370, 0.5828]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5538, 0.2025],\n",
      "          [0.4294, 0.7124]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.0620, 0.4805],\n",
      "          [0.5602, 0.2121],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3683, 0.0122],\n",
      "          [0.1370, 0.5828],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5538, 0.2025],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4294, 0.7124],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.9874, 0.6652, 0.2045, 0.9944],\n",
      "         [0.7274, 0.5876, 0.5513, 0.2056],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.1961, 0.1914, 0.8254, 0.0580],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.9874, 0.6652],\n",
      "          [0.2045, 0.9944]],\n",
      "\n",
      "         [[0.7274, 0.5876],\n",
      "          [0.5513, 0.2056]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.1961, 0.1914],\n",
      "          [0.8254, 0.0580]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.9874, 0.6652],\n",
      "          [0.7274, 0.5876],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.2045, 0.9944],\n",
      "          [0.5513, 0.2056],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.1961, 0.1914],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.8254, 0.0580],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[1.4174, 1.1090,   -inf],\n",
      "          [1.1090, 0.8743,   -inf],\n",
      "          [0.9846, 0.8088,   -inf]],\n",
      "\n",
      "         [[1.0308, 0.3172,   -inf],\n",
      "          [0.3172, 0.3462,   -inf],\n",
      "          [0.3087, 0.5088,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.0751,   -inf,   -inf],\n",
      "          [0.1223,   -inf,   -inf],\n",
      "          [0.1016,   -inf,   -inf]],\n",
      "\n",
      "         [[0.6847,   -inf,   -inf],\n",
      "          [0.2631,   -inf,   -inf],\n",
      "          [0.0848,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.6003e-01, 8.5463e-01, 5.2323e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [5.8220e-02, 9.8273e-01, 7.0252e-01, 9.6254e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [7.1605e-01, 2.3946e-01, 3.7660e-01, 8.4458e-02, 6.7052e-01, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.7030e-01, 6.5145e-01, 2.6381e-01, 5.5274e-01, 1.9605e-01, 4.2324e-01]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.2794e-01, 9.1636e-01, 4.2591e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [6.1976e-02, 1.2187e-01, 3.5957e-01, 9.7289e-01, 6.0137e-01, 1.0000e+02],\n",
      "        [3.9378e-01, 3.5955e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [9.1267e-01, 2.9761e-02, 7.0394e-01, 5.2136e-01, 7.8422e-01, 4.4895e-01]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(build_causal_mask(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0949,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0638,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2310,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1594,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1003,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0284,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0956,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0518,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0793,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0465,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1360,  0.2006,    -inf,    -inf,    -inf],\n",
      "          [ 0.2414,  0.2303,    -inf,    -inf,    -inf],\n",
      "          [ 0.1301,  0.1254,    -inf,    -inf,    -inf],\n",
      "          [ 0.2398,  0.2475,    -inf,    -inf,    -inf],\n",
      "          [ 0.0798,  0.1438,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1861, -0.0559,    -inf,    -inf,    -inf],\n",
      "          [ 0.1560,  0.0567,    -inf,    -inf,    -inf],\n",
      "          [-0.0504, -0.0268,    -inf,    -inf,    -inf],\n",
      "          [ 0.0743,  0.0557,    -inf,    -inf,    -inf],\n",
      "          [-0.0952, -0.0647,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1024,  0.1367,  0.0805,    -inf,    -inf],\n",
      "          [ 0.1629,  0.2314,  0.1701,    -inf,    -inf],\n",
      "          [ 0.2017,  0.2758,  0.2275,    -inf,    -inf],\n",
      "          [ 0.0579,  0.0744,  0.0502,    -inf,    -inf],\n",
      "          [ 0.2015,  0.3122,  0.2343,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0512, -0.0316, -0.0288,    -inf,    -inf],\n",
      "          [-0.0144, -0.0057, -0.0095,    -inf,    -inf],\n",
      "          [-0.0979, -0.1047, -0.0648,    -inf,    -inf],\n",
      "          [ 0.1606,  0.1530,  0.1209,    -inf,    -inf],\n",
      "          [-0.1625, -0.1336, -0.1110,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1585,  0.1276,  0.2413,  0.1884,    -inf],\n",
      "          [ 0.0637,  0.0281,  0.1319,  0.0842,    -inf],\n",
      "          [ 0.2207,  0.1804,  0.3491,  0.2705,    -inf],\n",
      "          [ 0.0884,  0.0443,  0.2116,  0.1345,    -inf],\n",
      "          [ 0.0895,  0.0579,  0.1703,  0.1184,    -inf]],\n",
      "\n",
      "         [[-0.0186, -0.0185, -0.0144, -0.0031,    -inf],\n",
      "          [-0.0135, -0.0184, -0.0164,  0.0256,    -inf],\n",
      "          [ 0.0667,  0.1113,  0.1394,  0.1910,    -inf],\n",
      "          [ 0.0329,  0.0301,  0.0236,  0.0377,    -inf],\n",
      "          [-0.0256, -0.0319, -0.0307, -0.0096,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4839, 0.5161, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5028, 0.4972, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5012, 0.4988, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4981, 0.5019, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4840, 0.5160, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4675, 0.5325, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5248, 0.4752, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4941, 0.5059, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5046, 0.4954, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4924, 0.5076, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3319, 0.3434, 0.3247, 0.0000, 0.0000],\n",
      "          [0.3249, 0.3479, 0.3272, 0.0000, 0.0000],\n",
      "          [0.3223, 0.3470, 0.3307, 0.0000, 0.0000],\n",
      "          [0.3323, 0.3379, 0.3298, 0.0000, 0.0000],\n",
      "          [0.3174, 0.3546, 0.3280, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3287, 0.3352, 0.3361, 0.0000, 0.0000],\n",
      "          [0.3318, 0.3347, 0.3335, 0.0000, 0.0000],\n",
      "          [0.3304, 0.3281, 0.3415, 0.0000, 0.0000],\n",
      "          [0.3386, 0.3360, 0.3254, 0.0000, 0.0000],\n",
      "          [0.3244, 0.3340, 0.3416, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2447, 0.2373, 0.2659, 0.2522, 0.0000],\n",
      "          [0.2465, 0.2379, 0.2639, 0.2516, 0.0000],\n",
      "          [0.2411, 0.2315, 0.2741, 0.2534, 0.0000],\n",
      "          [0.2418, 0.2314, 0.2735, 0.2532, 0.0000],\n",
      "          [0.2450, 0.2373, 0.2656, 0.2521, 0.0000]],\n",
      "\n",
      "         [[0.2488, 0.2488, 0.2498, 0.2526, 0.0000],\n",
      "          [0.2480, 0.2468, 0.2473, 0.2579, 0.0000],\n",
      "          [0.2351, 0.2458, 0.2528, 0.2662, 0.0000],\n",
      "          [0.2504, 0.2498, 0.2481, 0.2517, 0.0000],\n",
      "          [0.2497, 0.2481, 0.2484, 0.2537, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[ 0.4056,  0.1888, -0.0774,  0.1630,  0.1265,  0.3000],\n",
      "         [ 0.4056,  0.1888, -0.0774,  0.1630,  0.1265,  0.3000],\n",
      "         [ 0.4056,  0.1888, -0.0774,  0.1630,  0.1265,  0.3000],\n",
      "         [ 0.4056,  0.1888, -0.0774,  0.1630,  0.1265,  0.3000],\n",
      "         [ 0.4056,  0.1888, -0.0774,  0.1630,  0.1265,  0.3000]],\n",
      "\n",
      "        [[ 0.2845,  0.2440, -0.3032, -0.0766,  0.2963,  0.2984],\n",
      "         [ 0.2876,  0.2472, -0.3036, -0.0856,  0.3022,  0.2908],\n",
      "         [ 0.2873,  0.2464, -0.3040, -0.0837,  0.3005,  0.2925],\n",
      "         [ 0.2868,  0.2463, -0.3036, -0.0831,  0.3005,  0.2929],\n",
      "         [ 0.2846,  0.2445, -0.3027, -0.0778,  0.2975,  0.2974]],\n",
      "\n",
      "        [[ 0.4257,  0.2465, -0.1635, -0.0102,  0.2108,  0.2376],\n",
      "         [ 0.4246,  0.2466, -0.1642, -0.0104,  0.2106,  0.2380],\n",
      "         [ 0.4232,  0.2461, -0.1654, -0.0094,  0.2101,  0.2390],\n",
      "         [ 0.4261,  0.2469, -0.1618, -0.0100,  0.2097,  0.2362],\n",
      "         [ 0.4230,  0.2462, -0.1663, -0.0102,  0.2106,  0.2395]],\n",
      "\n",
      "        [[ 0.3849,  0.2486, -0.1877, -0.0768,  0.3679,  0.2557],\n",
      "         [ 0.3852,  0.2486, -0.1869, -0.0762,  0.3665,  0.2554],\n",
      "         [ 0.3877,  0.2495, -0.1855, -0.0780,  0.3677,  0.2545],\n",
      "         [ 0.3847,  0.2482, -0.1884, -0.0773,  0.3697,  0.2568],\n",
      "         [ 0.3849,  0.2485, -0.1876, -0.0766,  0.3676,  0.2558]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0949,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0638,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2310,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1594,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1003,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0284,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0956,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0518,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0793,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0465,    -inf,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1360,  0.2006,    -inf,    -inf,    -inf],\n",
      "          [ 0.2414,  0.2303,    -inf,    -inf,    -inf],\n",
      "          [ 0.1301,  0.1254,    -inf,    -inf,    -inf],\n",
      "          [ 0.2398,  0.2475,    -inf,    -inf,    -inf],\n",
      "          [ 0.0798,  0.1438,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1861, -0.0559,    -inf,    -inf,    -inf],\n",
      "          [ 0.1560,  0.0567,    -inf,    -inf,    -inf],\n",
      "          [-0.0504, -0.0268,    -inf,    -inf,    -inf],\n",
      "          [ 0.0743,  0.0557,    -inf,    -inf,    -inf],\n",
      "          [-0.0952, -0.0647,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1024,  0.1367,  0.0805,    -inf,    -inf],\n",
      "          [ 0.1629,  0.2314,  0.1701,    -inf,    -inf],\n",
      "          [ 0.2017,  0.2758,  0.2275,    -inf,    -inf],\n",
      "          [ 0.0579,  0.0744,  0.0502,    -inf,    -inf],\n",
      "          [ 0.2015,  0.3122,  0.2343,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0512, -0.0316, -0.0288,    -inf,    -inf],\n",
      "          [-0.0144, -0.0057, -0.0095,    -inf,    -inf],\n",
      "          [-0.0979, -0.1047, -0.0648,    -inf,    -inf],\n",
      "          [ 0.1606,  0.1530,  0.1209,    -inf,    -inf],\n",
      "          [-0.1625, -0.1336, -0.1110,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1585,  0.1276,  0.2413,  0.1884,    -inf],\n",
      "          [ 0.0637,  0.0281,  0.1319,  0.0842,    -inf],\n",
      "          [ 0.2207,  0.1804,  0.3491,  0.2705,    -inf],\n",
      "          [ 0.0884,  0.0443,  0.2116,  0.1345,    -inf],\n",
      "          [ 0.0895,  0.0579,  0.1703,  0.1184,    -inf]],\n",
      "\n",
      "         [[-0.0186, -0.0185, -0.0144, -0.0031,    -inf],\n",
      "          [-0.0135, -0.0184, -0.0164,  0.0256,    -inf],\n",
      "          [ 0.0667,  0.1113,  0.1394,  0.1910,    -inf],\n",
      "          [ 0.0329,  0.0301,  0.0236,  0.0377,    -inf],\n",
      "          [-0.0256, -0.0319, -0.0307, -0.0096,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4839, 0.5161, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5028, 0.4972, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5012, 0.4988, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4981, 0.5019, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4840, 0.5160, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4675, 0.5325, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5248, 0.4752, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4941, 0.5059, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5046, 0.4954, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4924, 0.5076, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3319, 0.3434, 0.3247, 0.0000, 0.0000],\n",
      "          [0.3249, 0.3479, 0.3272, 0.0000, 0.0000],\n",
      "          [0.3223, 0.3470, 0.3307, 0.0000, 0.0000],\n",
      "          [0.3323, 0.3379, 0.3298, 0.0000, 0.0000],\n",
      "          [0.3174, 0.3546, 0.3280, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3287, 0.3352, 0.3361, 0.0000, 0.0000],\n",
      "          [0.3318, 0.3347, 0.3335, 0.0000, 0.0000],\n",
      "          [0.3304, 0.3281, 0.3415, 0.0000, 0.0000],\n",
      "          [0.3386, 0.3360, 0.3254, 0.0000, 0.0000],\n",
      "          [0.3244, 0.3340, 0.3416, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2447, 0.2373, 0.2659, 0.2522, 0.0000],\n",
      "          [0.2465, 0.2379, 0.2639, 0.2516, 0.0000],\n",
      "          [0.2411, 0.2315, 0.2741, 0.2534, 0.0000],\n",
      "          [0.2418, 0.2314, 0.2735, 0.2532, 0.0000],\n",
      "          [0.2450, 0.2373, 0.2656, 0.2521, 0.0000]],\n",
      "\n",
      "         [[0.2488, 0.2488, 0.2498, 0.2526, 0.0000],\n",
      "          [0.2480, 0.2468, 0.2473, 0.2579, 0.0000],\n",
      "          [0.2351, 0.2458, 0.2528, 0.2662, 0.0000],\n",
      "          [0.2504, 0.2498, 0.2481, 0.2517, 0.0000],\n",
      "          [0.2497, 0.2481, 0.2484, 0.2537, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0145,  0.0141,    -inf],\n",
      "          [-0.0397, -0.0603,    -inf],\n",
      "          [-0.0865, -0.0288,    -inf]],\n",
      "\n",
      "         [[-0.1940, -0.2124,    -inf],\n",
      "          [-0.1396, -0.1466,    -inf],\n",
      "          [-0.1975, -0.1571,    -inf]],\n",
      "\n",
      "         [[ 0.0143,  0.0537,    -inf],\n",
      "          [-0.0944, -0.0512,    -inf],\n",
      "          [ 0.0082,  0.0785,    -inf]],\n",
      "\n",
      "         [[ 0.1111,  0.1258,    -inf],\n",
      "          [ 0.0597,  0.0069,    -inf],\n",
      "          [ 0.1016,  0.1416,    -inf]],\n",
      "\n",
      "         [[ 0.0042,  0.0855,    -inf],\n",
      "          [ 0.0302,  0.0165,    -inf],\n",
      "          [-0.0415,  0.0010,    -inf]],\n",
      "\n",
      "         [[-0.0716, -0.0317,    -inf],\n",
      "          [-0.0558, -0.0022,    -inf],\n",
      "          [-0.0272,  0.0291,    -inf]],\n",
      "\n",
      "         [[ 0.0861,  0.0071,    -inf],\n",
      "          [ 0.0926,  0.0846,    -inf],\n",
      "          [ 0.0433,  0.0501,    -inf]],\n",
      "\n",
      "         [[-0.1378, -0.1020,    -inf],\n",
      "          [-0.0628,  0.0148,    -inf],\n",
      "          [-0.0032, -0.0089,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0365,    -inf,    -inf],\n",
      "          [-0.0584,    -inf,    -inf],\n",
      "          [-0.0416,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1948,    -inf,    -inf],\n",
      "          [-0.1702,    -inf,    -inf],\n",
      "          [-0.1425,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2196,    -inf,    -inf],\n",
      "          [-0.1416,    -inf,    -inf],\n",
      "          [-0.1507,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2120,    -inf,    -inf],\n",
      "          [ 0.1415,    -inf,    -inf],\n",
      "          [ 0.1779,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1006,    -inf,    -inf],\n",
      "          [ 0.1032,    -inf,    -inf],\n",
      "          [-0.0496,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1043,    -inf,    -inf],\n",
      "          [-0.0601,    -inf,    -inf],\n",
      "          [-0.0111,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1888,    -inf,    -inf],\n",
      "          [ 0.1915,    -inf,    -inf],\n",
      "          [ 0.0713,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0420,    -inf,    -inf],\n",
      "          [ 0.0399,    -inf,    -inf],\n",
      "          [ 0.0152,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4928, 0.5072, 0.0000],\n",
      "          [0.5052, 0.4948, 0.0000],\n",
      "          [0.4856, 0.5144, 0.0000]],\n",
      "\n",
      "         [[0.5046, 0.4954, 0.0000],\n",
      "          [0.5017, 0.4983, 0.0000],\n",
      "          [0.4899, 0.5101, 0.0000]],\n",
      "\n",
      "         [[0.4901, 0.5099, 0.0000],\n",
      "          [0.4892, 0.5108, 0.0000],\n",
      "          [0.4824, 0.5176, 0.0000]],\n",
      "\n",
      "         [[0.4963, 0.5037, 0.0000],\n",
      "          [0.5132, 0.4868, 0.0000],\n",
      "          [0.4900, 0.5100, 0.0000]],\n",
      "\n",
      "         [[0.4797, 0.5203, 0.0000],\n",
      "          [0.5034, 0.4966, 0.0000],\n",
      "          [0.4894, 0.5106, 0.0000]],\n",
      "\n",
      "         [[0.4900, 0.5100, 0.0000],\n",
      "          [0.4866, 0.5134, 0.0000],\n",
      "          [0.4859, 0.5141, 0.0000]],\n",
      "\n",
      "         [[0.5197, 0.4803, 0.0000],\n",
      "          [0.5020, 0.4980, 0.0000],\n",
      "          [0.4983, 0.5017, 0.0000]],\n",
      "\n",
      "         [[0.4911, 0.5089, 0.0000],\n",
      "          [0.4806, 0.5194, 0.0000],\n",
      "          [0.5014, 0.4986, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, self_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "x = torch.rand(2, 3, 512)\n",
    "\n",
    "encoder_layer(x, self_mask=self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.2151, -0.0477,    -inf],\n",
      "          [-0.1885, -0.3447,    -inf],\n",
      "          [ 0.3269,  0.3816,    -inf]],\n",
      "\n",
      "         [[ 0.2025, -0.3009,    -inf],\n",
      "          [ 0.7424,  1.0167,    -inf],\n",
      "          [ 0.1579, -0.4013,    -inf]],\n",
      "\n",
      "         [[ 0.0114, -0.2519,    -inf],\n",
      "          [-0.0400, -0.4330,    -inf],\n",
      "          [-0.2089,  0.0188,    -inf]],\n",
      "\n",
      "         [[ 0.6433,  0.3118,    -inf],\n",
      "          [ 0.4537,  0.1976,    -inf],\n",
      "          [-0.5873, -0.3757,    -inf]],\n",
      "\n",
      "         [[-0.3121,  0.2585,    -inf],\n",
      "          [-0.5840, -0.3716,    -inf],\n",
      "          [ 0.1938, -0.7769,    -inf]],\n",
      "\n",
      "         [[ 0.0486,  0.0788,    -inf],\n",
      "          [-0.4131, -0.1348,    -inf],\n",
      "          [ 0.5153, -0.3483,    -inf]],\n",
      "\n",
      "         [[-0.1291, -0.7957,    -inf],\n",
      "          [-0.1932, -0.6685,    -inf],\n",
      "          [-1.2098, -0.6351,    -inf]],\n",
      "\n",
      "         [[-0.7896, -0.8036,    -inf],\n",
      "          [-0.8287, -0.7333,    -inf],\n",
      "          [ 0.0861,  0.3436,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2514,    -inf,    -inf],\n",
      "          [-1.4500,    -inf,    -inf],\n",
      "          [-0.8034,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2579,    -inf,    -inf],\n",
      "          [ 0.3270,    -inf,    -inf],\n",
      "          [ 0.2804,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3919,    -inf,    -inf],\n",
      "          [ 0.1993,    -inf,    -inf],\n",
      "          [ 0.1700,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1978,    -inf,    -inf],\n",
      "          [ 0.1385,    -inf,    -inf],\n",
      "          [-0.4050,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.7862,    -inf,    -inf],\n",
      "          [-1.3832,    -inf,    -inf],\n",
      "          [ 0.3139,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2387,    -inf,    -inf],\n",
      "          [-0.7242,    -inf,    -inf],\n",
      "          [-0.6487,    -inf,    -inf]],\n",
      "\n",
      "         [[ 1.1851,    -inf,    -inf],\n",
      "          [ 0.0302,    -inf,    -inf],\n",
      "          [ 0.2135,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3787,    -inf,    -inf],\n",
      "          [ 0.0764,    -inf,    -inf],\n",
      "          [-0.0286,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5653, 0.4347, 0.0000],\n",
      "          [0.5390, 0.4610, 0.0000],\n",
      "          [0.4863, 0.5137, 0.0000]],\n",
      "\n",
      "         [[0.6233, 0.3767, 0.0000],\n",
      "          [0.4318, 0.5682, 0.0000],\n",
      "          [0.6363, 0.3637, 0.0000]],\n",
      "\n",
      "         [[0.5655, 0.4345, 0.0000],\n",
      "          [0.5970, 0.4030, 0.0000],\n",
      "          [0.4433, 0.5567, 0.0000]],\n",
      "\n",
      "         [[0.5821, 0.4179, 0.0000],\n",
      "          [0.5637, 0.4363, 0.0000],\n",
      "          [0.4473, 0.5527, 0.0000]],\n",
      "\n",
      "         [[0.3611, 0.6389, 0.0000],\n",
      "          [0.4471, 0.5529, 0.0000],\n",
      "          [0.7253, 0.2747, 0.0000]],\n",
      "\n",
      "         [[0.4924, 0.5076, 0.0000],\n",
      "          [0.4309, 0.5691, 0.0000],\n",
      "          [0.7034, 0.2966, 0.0000]],\n",
      "\n",
      "         [[0.6607, 0.3393, 0.0000],\n",
      "          [0.6166, 0.3834, 0.0000],\n",
      "          [0.3601, 0.6399, 0.0000]],\n",
      "\n",
      "         [[0.5035, 0.4965, 0.0000],\n",
      "          [0.4762, 0.5238, 0.0000],\n",
      "          [0.4360, 0.5640, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2611, -0.0536,    -inf],\n",
      "          [-0.0880,  0.2201,    -inf],\n",
      "          [-0.3320,  0.1171,    -inf]],\n",
      "\n",
      "         [[ 0.3599,  0.2402,    -inf],\n",
      "          [ 0.2000,  0.0572,    -inf],\n",
      "          [-0.3541,  0.4574,    -inf]],\n",
      "\n",
      "         [[-0.0752, -0.1191,    -inf],\n",
      "          [ 0.5095,  0.2254,    -inf],\n",
      "          [ 0.6366,  0.1854,    -inf]],\n",
      "\n",
      "         [[ 0.1945, -0.1263,    -inf],\n",
      "          [-0.2349,  0.4050,    -inf],\n",
      "          [-0.2930, -0.0452,    -inf]],\n",
      "\n",
      "         [[ 0.0425, -0.7094,    -inf],\n",
      "          [ 0.1141, -0.2620,    -inf],\n",
      "          [-0.2347, -0.1589,    -inf]],\n",
      "\n",
      "         [[ 0.0607,  0.1943,    -inf],\n",
      "          [-0.0754,  0.2727,    -inf],\n",
      "          [-0.3928, -0.1806,    -inf]],\n",
      "\n",
      "         [[ 0.5154,  0.3473,    -inf],\n",
      "          [ 0.4048,  0.6874,    -inf],\n",
      "          [ 0.7775,  0.4277,    -inf]],\n",
      "\n",
      "         [[-0.0089,  0.0286,    -inf],\n",
      "          [-0.4052, -0.3809,    -inf],\n",
      "          [-0.2551, -0.1861,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0624,    -inf,    -inf],\n",
      "          [-0.1616,    -inf,    -inf],\n",
      "          [-0.4224,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0105,    -inf,    -inf],\n",
      "          [ 0.1372,    -inf,    -inf],\n",
      "          [ 0.1525,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6446,    -inf,    -inf],\n",
      "          [ 0.1648,    -inf,    -inf],\n",
      "          [ 0.0951,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1828,    -inf,    -inf],\n",
      "          [ 0.1627,    -inf,    -inf],\n",
      "          [-0.2874,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2288,    -inf,    -inf],\n",
      "          [ 0.3345,    -inf,    -inf],\n",
      "          [-0.2327,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2052,    -inf,    -inf],\n",
      "          [-0.1961,    -inf,    -inf],\n",
      "          [-0.4889,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1081,    -inf,    -inf],\n",
      "          [-0.4521,    -inf,    -inf],\n",
      "          [ 0.0164,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1292,    -inf,    -inf],\n",
      "          [ 0.0811,    -inf,    -inf],\n",
      "          [-0.2754,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4483, 0.5517, 0.0000],\n",
      "          [0.4236, 0.5764, 0.0000],\n",
      "          [0.3896, 0.6104, 0.0000]],\n",
      "\n",
      "         [[0.5299, 0.4701, 0.0000],\n",
      "          [0.5356, 0.4644, 0.0000],\n",
      "          [0.3076, 0.6924, 0.0000]],\n",
      "\n",
      "         [[0.5110, 0.4890, 0.0000],\n",
      "          [0.5705, 0.4295, 0.0000],\n",
      "          [0.6109, 0.3891, 0.0000]],\n",
      "\n",
      "         [[0.5795, 0.4205, 0.0000],\n",
      "          [0.3453, 0.6547, 0.0000],\n",
      "          [0.4384, 0.5616, 0.0000]],\n",
      "\n",
      "         [[0.6796, 0.3204, 0.0000],\n",
      "          [0.5929, 0.4071, 0.0000],\n",
      "          [0.4811, 0.5189, 0.0000]],\n",
      "\n",
      "         [[0.4666, 0.5334, 0.0000],\n",
      "          [0.4139, 0.5861, 0.0000],\n",
      "          [0.4471, 0.5529, 0.0000]],\n",
      "\n",
      "         [[0.5419, 0.4581, 0.0000],\n",
      "          [0.4298, 0.5702, 0.0000],\n",
      "          [0.5866, 0.4134, 0.0000]],\n",
      "\n",
      "         [[0.4906, 0.5094, 0.0000],\n",
      "          [0.4939, 0.5061, 0.0000],\n",
      "          [0.4828, 0.5172, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1112,  0.5917,    -inf],\n",
      "          [ 0.3347,  0.5880,    -inf],\n",
      "          [ 0.1764,  0.1467,    -inf]],\n",
      "\n",
      "         [[ 0.1540, -0.1701,    -inf],\n",
      "          [ 0.3511, -0.3159,    -inf],\n",
      "          [ 0.0220, -0.3690,    -inf]],\n",
      "\n",
      "         [[ 0.8604,  0.2011,    -inf],\n",
      "          [ 0.1854, -0.1530,    -inf],\n",
      "          [ 0.4619, -0.5958,    -inf]],\n",
      "\n",
      "         [[-0.1869,  0.4437,    -inf],\n",
      "          [ 0.0463,  0.0116,    -inf],\n",
      "          [ 0.0338, -0.0955,    -inf]],\n",
      "\n",
      "         [[-0.4545,  0.3864,    -inf],\n",
      "          [ 0.1310,  0.3556,    -inf],\n",
      "          [ 0.3186,  0.0783,    -inf]],\n",
      "\n",
      "         [[-0.7588, -0.5789,    -inf],\n",
      "          [ 0.0163, -0.0068,    -inf],\n",
      "          [ 0.3234, -0.3351,    -inf]],\n",
      "\n",
      "         [[ 0.1268, -0.1280,    -inf],\n",
      "          [-0.9681, -0.8860,    -inf],\n",
      "          [ 0.1030, -0.1050,    -inf]],\n",
      "\n",
      "         [[ 0.2915,  0.0387,    -inf],\n",
      "          [ 0.1784,  0.3237,    -inf],\n",
      "          [ 0.0455, -0.2014,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1813,    -inf,    -inf],\n",
      "          [ 0.0564,    -inf,    -inf],\n",
      "          [-0.2695,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0527,    -inf,    -inf],\n",
      "          [-0.0453,    -inf,    -inf],\n",
      "          [ 0.3762,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1854,    -inf,    -inf],\n",
      "          [ 0.6195,    -inf,    -inf],\n",
      "          [-0.0061,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4465,    -inf,    -inf],\n",
      "          [-0.1912,    -inf,    -inf],\n",
      "          [ 0.2536,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0020,    -inf,    -inf],\n",
      "          [-0.4348,    -inf,    -inf],\n",
      "          [ 0.2959,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3633,    -inf,    -inf],\n",
      "          [ 0.4026,    -inf,    -inf],\n",
      "          [-0.2583,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0637,    -inf,    -inf],\n",
      "          [-0.1375,    -inf,    -inf],\n",
      "          [ 0.0278,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3799,    -inf,    -inf],\n",
      "          [-0.0472,    -inf,    -inf],\n",
      "          [-0.0667,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3312, 0.6688, 0.0000],\n",
      "          [0.4370, 0.5630, 0.0000],\n",
      "          [0.5074, 0.4926, 0.0000]],\n",
      "\n",
      "         [[0.5803, 0.4197, 0.0000],\n",
      "          [0.6608, 0.3392, 0.0000],\n",
      "          [0.5965, 0.4035, 0.0000]],\n",
      "\n",
      "         [[0.6591, 0.3409, 0.0000],\n",
      "          [0.5838, 0.4162, 0.0000],\n",
      "          [0.7423, 0.2577, 0.0000]],\n",
      "\n",
      "         [[0.3474, 0.6526, 0.0000],\n",
      "          [0.5087, 0.4913, 0.0000],\n",
      "          [0.5323, 0.4677, 0.0000]],\n",
      "\n",
      "         [[0.3013, 0.6987, 0.0000],\n",
      "          [0.4441, 0.5559, 0.0000],\n",
      "          [0.5598, 0.4402, 0.0000]],\n",
      "\n",
      "         [[0.4552, 0.5448, 0.0000],\n",
      "          [0.5058, 0.4942, 0.0000],\n",
      "          [0.6589, 0.3411, 0.0000]],\n",
      "\n",
      "         [[0.5634, 0.4366, 0.0000],\n",
      "          [0.4795, 0.5205, 0.0000],\n",
      "          [0.5518, 0.4482, 0.0000]],\n",
      "\n",
      "         [[0.5629, 0.4371, 0.0000],\n",
      "          [0.4637, 0.5363, 0.0000],\n",
      "          [0.5614, 0.4386, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1967,  0.0402,    -inf],\n",
      "          [ 0.3240,  0.0230,    -inf],\n",
      "          [ 0.1055,  0.1216,    -inf]],\n",
      "\n",
      "         [[ 0.4443,  0.2728,    -inf],\n",
      "          [-0.0128,  0.3755,    -inf],\n",
      "          [ 0.5585,  0.2608,    -inf]],\n",
      "\n",
      "         [[ 0.1106, -0.3441,    -inf],\n",
      "          [-0.1771,  0.0067,    -inf],\n",
      "          [ 0.1002,  0.1773,    -inf]],\n",
      "\n",
      "         [[ 0.2421,  0.5913,    -inf],\n",
      "          [ 0.2494,  0.0135,    -inf],\n",
      "          [-0.4433, -0.1999,    -inf]],\n",
      "\n",
      "         [[ 0.2507,  0.2370,    -inf],\n",
      "          [ 0.2544,  0.2561,    -inf],\n",
      "          [-0.1174, -0.0828,    -inf]],\n",
      "\n",
      "         [[ 0.4049,  0.1398,    -inf],\n",
      "          [ 0.6993,  0.1462,    -inf],\n",
      "          [-0.1180, -0.4138,    -inf]],\n",
      "\n",
      "         [[ 0.2631,  0.3137,    -inf],\n",
      "          [-0.1930, -0.0898,    -inf],\n",
      "          [-0.1038,  0.0882,    -inf]],\n",
      "\n",
      "         [[-0.4635,  0.0850,    -inf],\n",
      "          [ 0.5542,  0.6643,    -inf],\n",
      "          [ 0.2550,  0.1795,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.5797,    -inf,    -inf],\n",
      "          [-0.2970,    -inf,    -inf],\n",
      "          [-0.2954,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1154,    -inf,    -inf],\n",
      "          [ 0.1055,    -inf,    -inf],\n",
      "          [-0.3795,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1446,    -inf,    -inf],\n",
      "          [-0.5375,    -inf,    -inf],\n",
      "          [-0.0857,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4555,    -inf,    -inf],\n",
      "          [ 0.0875,    -inf,    -inf],\n",
      "          [-0.1545,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0304,    -inf,    -inf],\n",
      "          [-0.3053,    -inf,    -inf],\n",
      "          [ 0.3292,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3402,    -inf,    -inf],\n",
      "          [-0.2559,    -inf,    -inf],\n",
      "          [-0.0471,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3742,    -inf,    -inf],\n",
      "          [ 0.4577,    -inf,    -inf],\n",
      "          [ 0.3109,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6588,    -inf,    -inf],\n",
      "          [ 0.7608,    -inf,    -inf],\n",
      "          [ 0.0087,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4410, 0.5590, 0.0000],\n",
      "          [0.5747, 0.4253, 0.0000],\n",
      "          [0.4960, 0.5040, 0.0000]],\n",
      "\n",
      "         [[0.5428, 0.4572, 0.0000],\n",
      "          [0.4041, 0.5959, 0.0000],\n",
      "          [0.5739, 0.4261, 0.0000]],\n",
      "\n",
      "         [[0.6117, 0.3883, 0.0000],\n",
      "          [0.4542, 0.5458, 0.0000],\n",
      "          [0.4807, 0.5193, 0.0000]],\n",
      "\n",
      "         [[0.4136, 0.5864, 0.0000],\n",
      "          [0.5587, 0.4413, 0.0000],\n",
      "          [0.4394, 0.5606, 0.0000]],\n",
      "\n",
      "         [[0.5034, 0.4966, 0.0000],\n",
      "          [0.4996, 0.5004, 0.0000],\n",
      "          [0.4913, 0.5087, 0.0000]],\n",
      "\n",
      "         [[0.5659, 0.4341, 0.0000],\n",
      "          [0.6348, 0.3652, 0.0000],\n",
      "          [0.5734, 0.4266, 0.0000]],\n",
      "\n",
      "         [[0.4874, 0.5126, 0.0000],\n",
      "          [0.4742, 0.5258, 0.0000],\n",
      "          [0.4521, 0.5479, 0.0000]],\n",
      "\n",
      "         [[0.3662, 0.6338, 0.0000],\n",
      "          [0.4725, 0.5275, 0.0000],\n",
      "          [0.5189, 0.4811, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.3655,  0.8187,    -inf],\n",
      "          [ 0.2296,  0.3932,    -inf],\n",
      "          [ 0.1716,  0.5350,    -inf]],\n",
      "\n",
      "         [[-0.5304, -0.3291,    -inf],\n",
      "          [ 0.0288, -0.2089,    -inf],\n",
      "          [-0.1745,  0.1032,    -inf]],\n",
      "\n",
      "         [[ 0.5146,  0.4248,    -inf],\n",
      "          [ 0.8803,  0.2729,    -inf],\n",
      "          [ 0.1029,  0.1862,    -inf]],\n",
      "\n",
      "         [[ 0.0840, -0.4107,    -inf],\n",
      "          [ 0.9552,  0.0348,    -inf],\n",
      "          [ 0.5105, -0.2200,    -inf]],\n",
      "\n",
      "         [[ 0.1276, -0.0118,    -inf],\n",
      "          [ 0.2092,  0.0235,    -inf],\n",
      "          [ 0.4381,  0.0214,    -inf]],\n",
      "\n",
      "         [[-0.0482,  0.0132,    -inf],\n",
      "          [-0.0198, -0.1602,    -inf],\n",
      "          [-0.2079, -0.1109,    -inf]],\n",
      "\n",
      "         [[-0.3182,  0.2738,    -inf],\n",
      "          [-0.4159,  0.1075,    -inf],\n",
      "          [-0.0905, -0.4420,    -inf]],\n",
      "\n",
      "         [[ 0.2087,  0.0029,    -inf],\n",
      "          [ 0.0744,  0.2051,    -inf],\n",
      "          [ 0.6974,  0.2664,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3207,    -inf,    -inf],\n",
      "          [ 0.1739,    -inf,    -inf],\n",
      "          [-0.0617,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0453,    -inf,    -inf],\n",
      "          [ 0.6738,    -inf,    -inf],\n",
      "          [ 0.2657,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1466,    -inf,    -inf],\n",
      "          [-0.0846,    -inf,    -inf],\n",
      "          [-0.1413,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1635,    -inf,    -inf],\n",
      "          [ 0.2191,    -inf,    -inf],\n",
      "          [-0.1635,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1616,    -inf,    -inf],\n",
      "          [-0.0556,    -inf,    -inf],\n",
      "          [ 0.3656,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2987,    -inf,    -inf],\n",
      "          [-0.0369,    -inf,    -inf],\n",
      "          [-0.0057,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1489,    -inf,    -inf],\n",
      "          [-0.2045,    -inf,    -inf],\n",
      "          [ 0.0396,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2907,    -inf,    -inf],\n",
      "          [ 0.1817,    -inf,    -inf],\n",
      "          [-0.0075,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3886, 0.6114, 0.0000],\n",
      "          [0.4592, 0.5408, 0.0000],\n",
      "          [0.4101, 0.5899, 0.0000]],\n",
      "\n",
      "         [[0.4498, 0.5502, 0.0000],\n",
      "          [0.5591, 0.4409, 0.0000],\n",
      "          [0.4310, 0.5690, 0.0000]],\n",
      "\n",
      "         [[0.5224, 0.4776, 0.0000],\n",
      "          [0.6474, 0.3526, 0.0000],\n",
      "          [0.4792, 0.5208, 0.0000]],\n",
      "\n",
      "         [[0.6212, 0.3788, 0.0000],\n",
      "          [0.7151, 0.2849, 0.0000],\n",
      "          [0.6749, 0.3251, 0.0000]],\n",
      "\n",
      "         [[0.5348, 0.4652, 0.0000],\n",
      "          [0.5463, 0.4537, 0.0000],\n",
      "          [0.6027, 0.3973, 0.0000]],\n",
      "\n",
      "         [[0.4847, 0.5153, 0.0000],\n",
      "          [0.5350, 0.4650, 0.0000],\n",
      "          [0.4758, 0.5242, 0.0000]],\n",
      "\n",
      "         [[0.3562, 0.6438, 0.0000],\n",
      "          [0.3721, 0.6279, 0.0000],\n",
      "          [0.5870, 0.4130, 0.0000]],\n",
      "\n",
      "         [[0.5513, 0.4487, 0.0000],\n",
      "          [0.4674, 0.5326, 0.0000],\n",
      "          [0.6061, 0.3939, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0318,  0.2466,    -inf],\n",
      "          [-0.4265, -0.6947,    -inf],\n",
      "          [ 0.3138,  0.0395,    -inf]],\n",
      "\n",
      "         [[-0.0703,  0.3492,    -inf],\n",
      "          [ 0.0880,  0.1440,    -inf],\n",
      "          [-0.1070,  0.1012,    -inf]],\n",
      "\n",
      "         [[-0.6844, -0.2911,    -inf],\n",
      "          [-0.1253,  0.0257,    -inf],\n",
      "          [-0.5974, -0.2731,    -inf]],\n",
      "\n",
      "         [[-0.0517, -0.3810,    -inf],\n",
      "          [-0.2963, -0.0996,    -inf],\n",
      "          [ 0.2560, -0.2759,    -inf]],\n",
      "\n",
      "         [[-0.4407, -0.0817,    -inf],\n",
      "          [-0.3915,  0.1011,    -inf],\n",
      "          [-0.3046,  0.0606,    -inf]],\n",
      "\n",
      "         [[ 0.3290, -0.0794,    -inf],\n",
      "          [ 0.3682,  0.4641,    -inf],\n",
      "          [ 0.0244,  0.0537,    -inf]],\n",
      "\n",
      "         [[ 0.2687,  0.1777,    -inf],\n",
      "          [-0.2314, -0.0234,    -inf],\n",
      "          [-0.1248,  0.2950,    -inf]],\n",
      "\n",
      "         [[ 0.3929, -0.0853,    -inf],\n",
      "          [ 0.3116,  0.6044,    -inf],\n",
      "          [ 0.1953, -0.0493,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5161,    -inf,    -inf],\n",
      "          [ 0.1165,    -inf,    -inf],\n",
      "          [-0.3733,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3338,    -inf,    -inf],\n",
      "          [ 0.1138,    -inf,    -inf],\n",
      "          [-0.0191,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4822,    -inf,    -inf],\n",
      "          [-0.2764,    -inf,    -inf],\n",
      "          [-0.2347,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0920,    -inf,    -inf],\n",
      "          [-0.2564,    -inf,    -inf],\n",
      "          [-0.1828,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0153,    -inf,    -inf],\n",
      "          [-0.0862,    -inf,    -inf],\n",
      "          [ 0.2052,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3523,    -inf,    -inf],\n",
      "          [ 0.2336,    -inf,    -inf],\n",
      "          [ 0.4403,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1625,    -inf,    -inf],\n",
      "          [-0.1200,    -inf,    -inf],\n",
      "          [ 0.2063,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1324,    -inf,    -inf],\n",
      "          [ 0.2410,    -inf,    -inf],\n",
      "          [-0.2486,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4465, 0.5535, 0.0000],\n",
      "          [0.5667, 0.4333, 0.0000],\n",
      "          [0.5681, 0.4319, 0.0000]],\n",
      "\n",
      "         [[0.3966, 0.6034, 0.0000],\n",
      "          [0.4860, 0.5140, 0.0000],\n",
      "          [0.4481, 0.5519, 0.0000]],\n",
      "\n",
      "         [[0.4029, 0.5971, 0.0000],\n",
      "          [0.4623, 0.5377, 0.0000],\n",
      "          [0.4196, 0.5804, 0.0000]],\n",
      "\n",
      "         [[0.5816, 0.4184, 0.0000],\n",
      "          [0.4510, 0.5490, 0.0000],\n",
      "          [0.6299, 0.3701, 0.0000]],\n",
      "\n",
      "         [[0.4112, 0.5888, 0.0000],\n",
      "          [0.3793, 0.6207, 0.0000],\n",
      "          [0.4097, 0.5903, 0.0000]],\n",
      "\n",
      "         [[0.6007, 0.3993, 0.0000],\n",
      "          [0.4760, 0.5240, 0.0000],\n",
      "          [0.4927, 0.5073, 0.0000]],\n",
      "\n",
      "         [[0.5227, 0.4773, 0.0000],\n",
      "          [0.4482, 0.5518, 0.0000],\n",
      "          [0.3966, 0.6034, 0.0000]],\n",
      "\n",
      "         [[0.6173, 0.3827, 0.0000],\n",
      "          [0.4273, 0.5727, 0.0000],\n",
      "          [0.5608, 0.4392, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x, self_mask = None):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask=self_mask)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "encoder(x, self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.2009,    -inf,    -inf],\n",
      "          [ 0.0009,    -inf,    -inf],\n",
      "          [ 0.1257,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0907,    -inf,    -inf],\n",
      "          [-0.1094,    -inf,    -inf],\n",
      "          [-0.1312,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0730,    -inf,    -inf],\n",
      "          [ 0.0427,    -inf,    -inf],\n",
      "          [ 0.0041,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0093,    -inf,    -inf],\n",
      "          [-0.0352,    -inf,    -inf],\n",
      "          [-0.1452,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0889,  0.0705,    -inf],\n",
      "          [ 0.1133,  0.0387,    -inf],\n",
      "          [ 0.1192,  0.1439,    -inf]],\n",
      "\n",
      "         [[ 0.1198, -0.0430,    -inf],\n",
      "          [-0.0133, -0.1476,    -inf],\n",
      "          [-0.0975, -0.2543,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5046, 0.4954, 0.0000],\n",
      "          [0.5186, 0.4814, 0.0000],\n",
      "          [0.4938, 0.5062, 0.0000]],\n",
      "\n",
      "         [[0.5406, 0.4594, 0.0000],\n",
      "          [0.5335, 0.4665, 0.0000],\n",
      "          [0.5391, 0.4609, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0657,  0.2235,  0.0450],\n",
      "          [-0.2600, -0.1685, -0.2328],\n",
      "          [ 0.0667,  0.0687,  0.0645]],\n",
      "\n",
      "         [[ 0.0153,  0.1376,  0.1842],\n",
      "          [-0.3821, -0.0684, -0.1113],\n",
      "          [-0.3170, -0.0296,  0.0776]]],\n",
      "\n",
      "\n",
      "        [[[-0.0038,    -inf,    -inf],\n",
      "          [-0.0250,    -inf,    -inf],\n",
      "          [-0.3404,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0800,    -inf,    -inf],\n",
      "          [ 0.1867,    -inf,    -inf],\n",
      "          [-0.1315,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0345, -0.1795,    -inf],\n",
      "          [-0.0933, -0.1347,    -inf],\n",
      "          [ 0.1892, -0.0122,    -inf]],\n",
      "\n",
      "         [[-0.3523, -0.4825,    -inf],\n",
      "          [-0.0996, -0.1179,    -inf],\n",
      "          [-0.0288,  0.1509,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3174, 0.3717, 0.3109],\n",
      "          [0.3202, 0.3508, 0.3290],\n",
      "          [0.3333, 0.3340, 0.3326]],\n",
      "\n",
      "         [[0.3017, 0.3410, 0.3573],\n",
      "          [0.2718, 0.3719, 0.3563],\n",
      "          [0.2620, 0.3492, 0.3888]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5362, 0.4638, 0.0000],\n",
      "          [0.5104, 0.4896, 0.0000],\n",
      "          [0.5502, 0.4498, 0.0000]],\n",
      "\n",
      "         [[0.5325, 0.4675, 0.0000],\n",
      "          [0.5046, 0.4954, 0.0000],\n",
      "          [0.4552, 0.5448, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = dec_x.size()\n",
    "        x = dec_x + self.attn_dropout(self.mhsa(dec_x, dec_x, dec_x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, enc_x, enc_x, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.9163,    -inf,    -inf],\n",
      "          [ 0.0890,    -inf,    -inf],\n",
      "          [ 0.5756,    -inf,    -inf]],\n",
      "\n",
      "         [[-1.7350,    -inf,    -inf],\n",
      "          [-0.0055,    -inf,    -inf],\n",
      "          [-1.2924,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1225,    -inf,    -inf],\n",
      "          [ 0.0783,    -inf,    -inf],\n",
      "          [ 0.1312,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2319,    -inf,    -inf],\n",
      "          [-1.4953,    -inf,    -inf],\n",
      "          [-1.1290,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.9586,  0.3578,    -inf],\n",
      "          [ 1.1101,  0.4091,    -inf],\n",
      "          [ 0.1652,  0.0870,    -inf]],\n",
      "\n",
      "         [[-0.1299, -0.2008,    -inf],\n",
      "          [ 0.0045, -0.9066,    -inf],\n",
      "          [-0.0321, -0.9740,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2114, 0.7886, 0.0000],\n",
      "          [0.6684, 0.3316, 0.0000],\n",
      "          [0.5195, 0.4805, 0.0000]],\n",
      "\n",
      "         [[0.5177, 0.4823, 0.0000],\n",
      "          [0.7132, 0.2868, 0.0000],\n",
      "          [0.7195, 0.2805, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 7.2021e-02,  1.7697e-01,  3.0168e-01],\n",
      "          [ 1.5657e-01,  2.0786e-01,  2.5909e-01],\n",
      "          [ 1.6911e-01,  2.0857e-01,  2.2052e-01]],\n",
      "\n",
      "         [[-7.2810e-02,  4.3885e-05,  4.3814e-02],\n",
      "          [ 9.5228e-02,  4.3257e-02,  9.6600e-02],\n",
      "          [ 1.0385e-01,  1.9241e-01,  2.6984e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7475e-01,        -inf,        -inf],\n",
      "          [ 1.2333e-01,        -inf,        -inf],\n",
      "          [-2.1595e-02,        -inf,        -inf]],\n",
      "\n",
      "         [[-1.5256e-02,        -inf,        -inf],\n",
      "          [-4.6327e-02,        -inf,        -inf],\n",
      "          [ 3.2032e-02,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.8153e-01, -3.1290e-01,        -inf],\n",
      "          [ 1.8408e-01,  3.0318e-01,        -inf],\n",
      "          [ 6.2168e-02, -1.6471e-01,        -inf]],\n",
      "\n",
      "         [[ 1.5769e-01,  2.6540e-01,        -inf],\n",
      "          [-9.0798e-02, -6.9855e-02,        -inf],\n",
      "          [-1.3652e-02,  1.4748e-02,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.2968, 0.3297, 0.3735],\n",
      "          [0.3164, 0.3330, 0.3506],\n",
      "          [0.3233, 0.3363, 0.3404]],\n",
      "\n",
      "         [[0.3126, 0.3362, 0.3512],\n",
      "          [0.3389, 0.3217, 0.3394],\n",
      "          [0.3055, 0.3338, 0.3607]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5328, 0.4672, 0.0000],\n",
      "          [0.4703, 0.5297, 0.0000],\n",
      "          [0.5565, 0.4435, 0.0000]],\n",
      "\n",
      "         [[0.4731, 0.5269, 0.0000],\n",
      "          [0.4948, 0.5052, 0.0000],\n",
      "          [0.4929, 0.5071, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.3364,    -inf,    -inf],\n",
      "          [-0.3997,    -inf,    -inf],\n",
      "          [-0.2892,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5409,    -inf,    -inf],\n",
      "          [ 0.1932,    -inf,    -inf],\n",
      "          [ 1.0030,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0927,    -inf,    -inf],\n",
      "          [-0.0661,    -inf,    -inf],\n",
      "          [ 0.1196,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2213,    -inf,    -inf],\n",
      "          [ 0.0570,    -inf,    -inf],\n",
      "          [-0.1145,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1943, -0.3334,    -inf],\n",
      "          [-0.2566,  0.1456,    -inf],\n",
      "          [-0.1503, -0.0876,    -inf]],\n",
      "\n",
      "         [[ 0.5280, -0.0609,    -inf],\n",
      "          [ 0.2560,  0.1367,    -inf],\n",
      "          [ 0.6238, -0.0572,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5347, 0.4653, 0.0000],\n",
      "          [0.4008, 0.5992, 0.0000],\n",
      "          [0.4843, 0.5157, 0.0000]],\n",
      "\n",
      "         [[0.6431, 0.3569, 0.0000],\n",
      "          [0.5298, 0.4702, 0.0000],\n",
      "          [0.6640, 0.3360, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0496, -0.0333, -0.1047],\n",
      "          [-0.1137, -0.1663, -0.2223],\n",
      "          [ 0.1268,  0.1908,  0.1421]],\n",
      "\n",
      "         [[ 0.1461,  0.1239, -0.0131],\n",
      "          [ 0.1665,  0.1093,  0.1075],\n",
      "          [ 0.1485,  0.1523,  0.1308]]],\n",
      "\n",
      "\n",
      "        [[[-0.0406,    -inf,    -inf],\n",
      "          [ 0.2230,    -inf,    -inf],\n",
      "          [ 0.3657,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2308,    -inf,    -inf],\n",
      "          [ 0.3661,    -inf,    -inf],\n",
      "          [ 0.4620,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.2766, -0.2709,    -inf],\n",
      "          [ 0.0611,  0.0134,    -inf],\n",
      "          [-0.0319, -0.0853,    -inf]],\n",
      "\n",
      "         [[-0.2106, -0.3741,    -inf],\n",
      "          [-0.1329,  0.0617,    -inf],\n",
      "          [-0.0524,  0.1493,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3375, 0.3431, 0.3194],\n",
      "          [0.3514, 0.3334, 0.3152],\n",
      "          [0.3245, 0.3460, 0.3295]],\n",
      "\n",
      "         [[0.3532, 0.3455, 0.3013],\n",
      "          [0.3464, 0.3271, 0.3265],\n",
      "          [0.3349, 0.3361, 0.3290]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4986, 0.5014, 0.0000],\n",
      "          [0.5119, 0.4881, 0.0000],\n",
      "          [0.5133, 0.4867, 0.0000]],\n",
      "\n",
      "         [[0.5408, 0.4592, 0.0000],\n",
      "          [0.4515, 0.5485, 0.0000],\n",
      "          [0.4498, 0.5502, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [DecoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=self_mask, cross_mask=cross_mask):\n",
    "        b, t = dec_x.size()\n",
    "        x = self.embed(dec_x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_x, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "    def get_embed_weights(self):\n",
    "        return self.embed.weight\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b5aba42-4209-4fbb-8cc9-a9e8a0236e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Output(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, d: int = 512, ff_weight = None):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(d, vocab_size)\n",
    "        # weight tying with the decoder embedding\n",
    "        if ff_weight is not None:\n",
    "            self.ff.weight = ff_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2477aec-7287-498b-bf31-cbb0bdfe154d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 1.0708,  0.8786,  0.1471],\n",
      "          [ 0.6345,  0.2321, -0.1412],\n",
      "          [ 0.4077,  0.2922,  0.3540]],\n",
      "\n",
      "         [[-0.6241,  0.4889, -0.2385],\n",
      "          [-0.0811,  0.6177, -0.1960],\n",
      "          [-0.2672,  0.4954,  0.0476]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8343,  1.0591,    -inf],\n",
      "          [ 0.8095,  1.0413,    -inf],\n",
      "          [ 0.3228,  0.5428,    -inf]],\n",
      "\n",
      "         [[-0.5556, -0.6223,    -inf],\n",
      "          [-0.3325, -0.4151,    -inf],\n",
      "          [-0.3452, -0.0575,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3044,    -inf,    -inf],\n",
      "          [ 0.1786,    -inf,    -inf],\n",
      "          [ 0.3484,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3582,    -inf,    -inf],\n",
      "          [-0.1866,    -inf,    -inf],\n",
      "          [ 0.1674,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4500, 0.3713, 0.1787],\n",
      "          [0.4697, 0.3141, 0.2162],\n",
      "          [0.3523, 0.3138, 0.3339]],\n",
      "\n",
      "         [[0.1814, 0.5520, 0.2667],\n",
      "          [0.2562, 0.5154, 0.2284],\n",
      "          [0.2215, 0.4749, 0.3035]]],\n",
      "\n",
      "\n",
      "        [[[0.4440, 0.5560, 0.0000],\n",
      "          [0.4423, 0.5577, 0.0000],\n",
      "          [0.4452, 0.5548, 0.0000]],\n",
      "\n",
      "         [[0.5167, 0.4833, 0.0000],\n",
      "          [0.5206, 0.4794, 0.0000],\n",
      "          [0.4286, 0.5714, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0790,  0.1294, -0.0447],\n",
      "          [-0.1009, -0.2660,  0.1243],\n",
      "          [-0.4927, -0.6123, -0.2082]],\n",
      "\n",
      "         [[ 0.1374, -0.3306, -0.2256],\n",
      "          [ 0.3815, -0.3147, -0.3149],\n",
      "          [ 0.4427, -0.1428, -0.1973]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1221,  0.0116,    -inf],\n",
      "          [ 0.1466, -0.1052,    -inf],\n",
      "          [-0.4456, -0.6872,    -inf]],\n",
      "\n",
      "         [[-0.1064, -0.0325,    -inf],\n",
      "          [ 0.0234, -0.0472,    -inf],\n",
      "          [-0.0838, -0.3613,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3756,    -inf,    -inf],\n",
      "          [-0.1235,    -inf,    -inf],\n",
      "          [-0.6125,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1545,    -inf,    -inf],\n",
      "          [ 0.4776,    -inf,    -inf],\n",
      "          [ 0.2488,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3407, 0.3583, 0.3010],\n",
      "          [0.3225, 0.2734, 0.4040],\n",
      "          [0.3109, 0.2759, 0.4132]],\n",
      "\n",
      "         [[0.4307, 0.2697, 0.2996],\n",
      "          [0.5008, 0.2496, 0.2496],\n",
      "          [0.4798, 0.2672, 0.2530]]],\n",
      "\n",
      "\n",
      "        [[[0.5276, 0.4724, 0.0000],\n",
      "          [0.5626, 0.4374, 0.0000],\n",
      "          [0.5601, 0.4399, 0.0000]],\n",
      "\n",
      "         [[0.4815, 0.5185, 0.0000],\n",
      "          [0.5176, 0.4824, 0.0000],\n",
      "          [0.5689, 0.4311, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1386,    -inf,    -inf],\n",
      "          [-0.1952,    -inf,    -inf],\n",
      "          [ 0.1341,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.8526,    -inf,    -inf],\n",
      "          [ 0.2595,    -inf,    -inf],\n",
      "          [-0.5413,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.4739,    -inf,    -inf],\n",
      "          [-0.0984,    -inf,    -inf],\n",
      "          [ 0.3642,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1798,    -inf,    -inf],\n",
      "          [-0.1997,    -inf,    -inf],\n",
      "          [ 0.3025,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.4260,    -inf,    -inf],\n",
      "          [ 0.1822,    -inf,    -inf],\n",
      "          [ 0.2213,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1354,    -inf,    -inf],\n",
      "          [-0.4073,    -inf,    -inf],\n",
      "          [-0.4064,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.4884,  0.2233,  0.4316],\n",
      "          [ 0.0021, -0.0663,  0.8415],\n",
      "          [-0.1953, -0.4159,  0.7118]],\n",
      "\n",
      "         [[-0.1705, -0.2726,  0.2471],\n",
      "          [-0.6394, -0.5596,  0.0958],\n",
      "          [-0.6430,  0.2884, -0.0488]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4340,  0.9152,    -inf],\n",
      "          [-0.3994, -0.0740,    -inf],\n",
      "          [-0.0818,  0.0653,    -inf]],\n",
      "\n",
      "         [[ 0.1508,  0.0385,    -inf],\n",
      "          [-0.3067, -0.3187,    -inf],\n",
      "          [ 0.0094,  0.1417,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9996,    -inf,    -inf],\n",
      "          [ 0.2270,    -inf,    -inf],\n",
      "          [ 0.1651,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1674,    -inf,    -inf],\n",
      "          [-0.3109,    -inf,    -inf],\n",
      "          [-0.3151,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3687, 0.2829, 0.3484],\n",
      "          [0.2354, 0.2198, 0.5448],\n",
      "          [0.2337, 0.1874, 0.5789]],\n",
      "\n",
      "         [[0.2923, 0.2639, 0.4438],\n",
      "          [0.2399, 0.2598, 0.5003],\n",
      "          [0.1869, 0.4744, 0.3386]]],\n",
      "\n",
      "\n",
      "        [[[0.3820, 0.6180, 0.0000],\n",
      "          [0.4194, 0.5806, 0.0000],\n",
      "          [0.4633, 0.5367, 0.0000]],\n",
      "\n",
      "         [[0.5281, 0.4719, 0.0000],\n",
      "          [0.5030, 0.4970, 0.0000],\n",
      "          [0.4670, 0.5330, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2957,    -inf,    -inf],\n",
      "          [-0.0533,    -inf,    -inf],\n",
      "          [-0.6387,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0553,    -inf,    -inf],\n",
      "          [ 0.2351,    -inf,    -inf],\n",
      "          [-0.0033,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.5101,    -inf,    -inf],\n",
      "          [-0.4047,    -inf,    -inf],\n",
      "          [-0.4162,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3307,    -inf,    -inf],\n",
      "          [-0.1250,    -inf,    -inf],\n",
      "          [ 0.1421,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.9653,    -inf,    -inf],\n",
      "          [-0.6236,    -inf,    -inf],\n",
      "          [-0.7260,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3316,    -inf,    -inf],\n",
      "          [-0.2597,    -inf,    -inf],\n",
      "          [-0.2943,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1866,  0.8909,  0.3258],\n",
      "          [ 0.8672,  1.4448, -0.1676],\n",
      "          [ 0.1749, -0.4140, -0.4826]],\n",
      "\n",
      "         [[-0.0540,  0.6338, -0.4523],\n",
      "          [-0.0360,  0.1466, -0.1582],\n",
      "          [-0.1395,  0.3549, -0.2182]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1811,  0.2186,    -inf],\n",
      "          [-0.2790, -0.2262,    -inf],\n",
      "          [-0.2766, -0.3218,    -inf]],\n",
      "\n",
      "         [[ 0.3296,  0.4218,    -inf],\n",
      "          [-0.5181, -0.3013,    -inf],\n",
      "          [-0.2201, -0.1254,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3370,    -inf,    -inf],\n",
      "          [-0.3279,    -inf,    -inf],\n",
      "          [-0.3308,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4889,    -inf,    -inf],\n",
      "          [-0.4235,    -inf,    -inf],\n",
      "          [-0.3316,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1784, 0.5239, 0.2977],\n",
      "          [0.3188, 0.5680, 0.1133],\n",
      "          [0.4824, 0.2677, 0.2499]],\n",
      "\n",
      "         [[0.2732, 0.5434, 0.1834],\n",
      "          [0.3241, 0.3891, 0.2868],\n",
      "          [0.2806, 0.4600, 0.2594]]],\n",
      "\n",
      "\n",
      "        [[[0.4906, 0.5094, 0.0000],\n",
      "          [0.4868, 0.5132, 0.0000],\n",
      "          [0.5113, 0.4887, 0.0000]],\n",
      "\n",
      "         [[0.4770, 0.5230, 0.0000],\n",
      "          [0.4460, 0.5540, 0.0000],\n",
      "          [0.4763, 0.5237, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 3, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8, embed_tying=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        if embed_tying:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d, ff_weight = self.decoder.get_embed_weights())\n",
    "        else:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d)\n",
    "\n",
    "    def forward(self, enc_x, dec_x, enc_mask, dec_mask):\n",
    "        encoded = self.encoder(enc_x, enc_mask)\n",
    "        decoded = self.decoder(dec_x=dec_x, enc_x=encoded, self_mask=dec_mask, cross_mask=enc_mask)\n",
    "        return self.output(decoded)\n",
    "\n",
    "transformer = Transformer(vocab_size=32, n=2, d=16, h=2)\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "enc_x = torch.tensor([[15, 7, 3], [10, 10, 0], [1, 0, 0]])\n",
    "dec_x = torch.tensor([[21, 8, 0], [25, 0, 0], [8, 0, 0]])\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=0)\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=0)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "print(transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
