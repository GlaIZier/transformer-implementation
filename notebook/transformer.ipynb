{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d7f736-b9af-48e3-adb5-18ab3536abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v, verbose=False):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    if verbose:\n",
    "        print(softmaxed_prod.shape)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x, verbose=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abcca093-9e66-4b46-8ec3-aded63ece6f8",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a9e6b-8461-4417-ab07-e5fbd79c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c5bf9-3800-4b22-b7d8-8886d4f15b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d626059-fd88-4a03-8b20-ffbf0115fe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayerWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayerWithoutMask()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851eff3d-4816-4410-9184-f30c157e4eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayerWithoutMask(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = EncoderWithoutMask()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With masks\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None, verbose=False):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    if verbose:\n",
    "        print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim)\n",
    "    if mask is not None:\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    if verbose:\n",
    "        print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    if verbose:\n",
    "        print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eac5e23-ea43-4295-ba24-a7edbe1a1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.2645, 0.8054, 0.6958, 0.9587],\n",
      "          [0.1500, 0.5244, 0.5921, 0.6156]],\n",
      "\n",
      "         [[0.3592, 0.8818, 0.2727, 0.2659],\n",
      "          [0.3101, 0.4034, 0.5034, 0.5532]],\n",
      "\n",
      "         [[0.7721, 0.1488, 0.3515, 0.2560],\n",
      "          [0.0466, 0.8870, 0.2661, 0.0137]]],\n",
      "\n",
      "\n",
      "        [[[0.8810, 0.3122, 0.8739, 0.3358],\n",
      "          [0.8760, 0.1660, 0.2207, 0.6067]],\n",
      "\n",
      "         [[0.6339, 0.5403, 0.4317, 0.1077],\n",
      "          [0.3124, 0.1320, 0.3176, 0.4907]],\n",
      "\n",
      "         [[0.6146, 0.7147, 0.5500, 0.9516],\n",
      "          [0.6748, 0.3788, 0.1871, 0.9141]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[1.0609, 0.6249,   -inf],\n",
      "          [0.6249, 0.5258,   -inf],\n",
      "          [0.4071, 0.2863,   -inf]],\n",
      "\n",
      "         [[0.5135,   -inf,   -inf],\n",
      "          [0.4484,   -inf,   -inf],\n",
      "          [0.3191,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8750, 0.5703,   -inf],\n",
      "          [0.5703, 0.4459,   -inf],\n",
      "          [0.7824, 0.5579,   -inf]],\n",
      "\n",
      "         [[0.6059,   -inf,   -inf],\n",
      "          [0.3317,   -inf,   -inf],\n",
      "          [0.6250,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6073, 0.3927, 0.0000],\n",
      "          [0.5248, 0.4752, 0.0000],\n",
      "          [0.5302, 0.4698, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5756, 0.4244, 0.0000],\n",
      "          [0.5311, 0.4689, 0.0000],\n",
      "          [0.5559, 0.4441, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "wrong a: \n",
      " tensor([[[[0.3017, 0.8354, 0.5297, 0.6866],\n",
      "          [0.3095, 0.8417, 0.4947, 0.6294],\n",
      "          [0.3090, 0.8413, 0.4970, 0.6332]],\n",
      "\n",
      "         [[0.1500, 0.5244, 0.5921, 0.6156],\n",
      "          [0.1500, 0.5244, 0.5921, 0.6156],\n",
      "          [0.1500, 0.5244, 0.5921, 0.6156]]],\n",
      "\n",
      "\n",
      "        [[[0.7761, 0.4090, 0.6862, 0.2390],\n",
      "          [0.7651, 0.4192, 0.6665, 0.2289],\n",
      "          [0.7712, 0.4135, 0.6775, 0.2345]],\n",
      "\n",
      "         [[0.8760, 0.1660, 0.2207, 0.6067],\n",
      "          [0.8760, 0.1660, 0.2207, 0.6067],\n",
      "          [0.8760, 0.1660, 0.2207, 0.6067]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[1.0609, 0.6249,   -inf],\n",
      "          [0.6249, 0.5258,   -inf],\n",
      "          [0.4071, 0.2863,   -inf]],\n",
      "\n",
      "         [[0.5135, 0.4484,   -inf],\n",
      "          [0.4484, 0.4092,   -inf],\n",
      "          [0.3191, 0.2569,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8750,   -inf,   -inf],\n",
      "          [0.5703,   -inf,   -inf],\n",
      "          [0.7824,   -inf,   -inf]],\n",
      "\n",
      "         [[0.6059,   -inf,   -inf],\n",
      "          [0.3317,   -inf,   -inf],\n",
      "          [0.6250,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6073, 0.3927, 0.0000],\n",
      "          [0.5248, 0.4752, 0.0000],\n",
      "          [0.5302, 0.4698, 0.0000]],\n",
      "\n",
      "         [[0.5163, 0.4837, 0.0000],\n",
      "          [0.5098, 0.4902, 0.0000],\n",
      "          [0.5155, 0.4845, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.3017, 0.8354, 0.5297, 0.6866],\n",
      "          [0.3095, 0.8417, 0.4947, 0.6294],\n",
      "          [0.3090, 0.8413, 0.4970, 0.6332]],\n",
      "\n",
      "         [[0.2274, 0.4659, 0.5492, 0.5854],\n",
      "          [0.2285, 0.4651, 0.5486, 0.5850],\n",
      "          [0.2275, 0.4658, 0.5492, 0.5854]]],\n",
      "\n",
      "\n",
      "        [[[0.8810, 0.3122, 0.8739, 0.3358],\n",
      "          [0.8810, 0.3122, 0.8739, 0.3358],\n",
      "          [0.8810, 0.3122, 0.8739, 0.3358]],\n",
      "\n",
      "         [[0.8760, 0.1660, 0.2207, 0.6067],\n",
      "          [0.8760, 0.1660, 0.2207, 0.6067],\n",
      "          [0.8760, 0.1660, 0.2207, 0.6067]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask, verbose=True)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask, verbose=True)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.2645, 0.8054, 0.6958, 0.9587],\n",
      "          [0.1500, 0.5244, 0.5921, 0.6156]],\n",
      "\n",
      "         [[0.3592, 0.8818, 0.2727, 0.2659],\n",
      "          [0.3101, 0.4034, 0.5034, 0.5532]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8810, 0.3122, 0.8739, 0.3358],\n",
      "          [0.8760, 0.1660, 0.2207, 0.6067]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[1.0609, 0.6249,   -inf],\n",
      "          [0.6249, 0.5258,   -inf],\n",
      "          [0.4071, 0.2863,   -inf]],\n",
      "\n",
      "         [[0.5135, 0.4484,   -inf],\n",
      "          [0.4484, 0.4092,   -inf],\n",
      "          [0.3191, 0.2569,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.8750,   -inf,   -inf],\n",
      "          [0.5703,   -inf,   -inf],\n",
      "          [0.7824,   -inf,   -inf]],\n",
      "\n",
      "         [[0.6059,   -inf,   -inf],\n",
      "          [0.3317,   -inf,   -inf],\n",
      "          [0.6250,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.6073, 0.3927, 0.0000],\n",
      "          [0.5248, 0.4752, 0.0000],\n",
      "          [0.5302, 0.4698, 0.0000]],\n",
      "\n",
      "         [[0.5163, 0.4837, 0.0000],\n",
      "          [0.5098, 0.4902, 0.0000],\n",
      "          [0.5155, 0.4845, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.3017, 0.8354, 0.5297, 0.6866],\n",
      "          [0.3095, 0.8417, 0.4947, 0.6294],\n",
      "          [0.3090, 0.8413, 0.4970, 0.6332]],\n",
      "\n",
      "         [[0.2274, 0.4659, 0.5492, 0.5854],\n",
      "          [0.2285, 0.4651, 0.5486, 0.5850],\n",
      "          [0.2275, 0.4658, 0.5492, 0.5854]]],\n",
      "\n",
      "\n",
      "        [[[0.8810, 0.3122, 0.8739, 0.3358],\n",
      "          [0.8810, 0.3122, 0.8739, 0.3358],\n",
      "          [0.8810, 0.3122, 0.8739, 0.3358]],\n",
      "\n",
      "         [[0.8760, 0.1660, 0.2207, 0.6067],\n",
      "          [0.8760, 0.1660, 0.2207, 0.6067],\n",
      "          [0.8760, 0.1660, 0.2207, 0.6067]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.2132, 0.4094, 0.4209, 0.6604],\n",
      "         [0.7768, 0.9561, 0.5645, 0.9780],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.9167, 0.6785, 0.5120, 0.2533],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.2132, 0.4094],\n",
      "          [0.4209, 0.6604]],\n",
      "\n",
      "         [[0.7768, 0.9561],\n",
      "          [0.5645, 0.9780]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9167, 0.6785],\n",
      "          [0.5120, 0.2533]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.2132, 0.4094],\n",
      "          [0.7768, 0.9561],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4209, 0.6604],\n",
      "          [0.5645, 0.9780],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9167, 0.6785],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5120, 0.2533],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.4504, 0.3736, 0.9258, 0.9811],\n",
      "         [0.7564, 0.6971, 0.0181, 0.7509],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.5988, 0.0013, 0.1299, 0.5951],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.4504, 0.3736],\n",
      "          [0.9258, 0.9811]],\n",
      "\n",
      "         [[0.7564, 0.6971],\n",
      "          [0.0181, 0.7509]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.5988, 0.0013],\n",
      "          [0.1299, 0.5951]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.4504, 0.3736],\n",
      "          [0.7564, 0.6971],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.9258, 0.9811],\n",
      "          [0.0181, 0.7509],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.5988, 0.0013],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.1299, 0.5951],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[0.3424, 0.6011,   -inf],\n",
      "          [0.6011, 1.0580,   -inf],\n",
      "          [0.4959, 0.8994,   -inf]],\n",
      "\n",
      "         [[1.8196, 0.7535,   -inf],\n",
      "          [0.7535, 0.5642,   -inf],\n",
      "          [0.6573, 0.2959,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.3585,   -inf,   -inf],\n",
      "          [0.3714,   -inf,   -inf],\n",
      "          [0.1768,   -inf,   -inf]],\n",
      "\n",
      "         [[0.3710,   -inf,   -inf],\n",
      "          [0.2306,   -inf,   -inf],\n",
      "          [0.3532,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x, verbose=True)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4884e-01, 5.8111e-01, 3.2646e-02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [3.8218e-01, 6.0412e-01, 9.5874e-01, 7.3729e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [2.1949e-01, 5.2574e-01, 1.8219e-01, 3.0381e-01, 3.3493e-02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [5.0838e-02, 8.8565e-01, 9.1128e-01, 6.9194e-01, 8.7682e-02, 1.8774e-01]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.4597e-01, 7.3858e-01, 4.0664e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.0673e-01, 7.6579e-01, 5.5747e-01, 2.5458e-01, 8.8127e-01, 1.0000e+02],\n",
      "        [2.1329e-01, 3.2319e-02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [7.1229e-01, 6.9040e-01, 9.4778e-01, 6.5091e-01, 6.9064e-01, 6.2781e-01]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(build_causal_mask(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "tensor([[[ 0.2611, -0.6307, -0.3905, -0.7425, -0.4419, -0.2336],\n",
      "         [ 0.2611, -0.6307, -0.3905, -0.7425, -0.4419, -0.2336],\n",
      "         [ 0.2611, -0.6307, -0.3905, -0.7425, -0.4419, -0.2336],\n",
      "         [ 0.2611, -0.6307, -0.3905, -0.7425, -0.4419, -0.2336],\n",
      "         [ 0.2611, -0.6307, -0.3905, -0.7425, -0.4419, -0.2336]],\n",
      "\n",
      "        [[ 0.2819, -0.4940, -0.1975, -0.5393, -0.3719, -0.2553],\n",
      "         [ 0.2807, -0.4931, -0.1937, -0.5372, -0.3702, -0.2567],\n",
      "         [ 0.2844, -0.4907, -0.2086, -0.5359, -0.3754, -0.2457],\n",
      "         [ 0.2812, -0.4957, -0.1937, -0.5414, -0.3709, -0.2590],\n",
      "         [ 0.2813, -0.4958, -0.1940, -0.5416, -0.3710, -0.2589]],\n",
      "\n",
      "        [[ 0.2812, -0.4437, -0.1266, -0.5612, -0.4856, -0.4035],\n",
      "         [ 0.2803, -0.4433, -0.1261, -0.5618, -0.4862, -0.4037],\n",
      "         [ 0.2797, -0.4421, -0.1288, -0.5622, -0.4880, -0.4012],\n",
      "         [ 0.2800, -0.4424, -0.1299, -0.5621, -0.4878, -0.4004],\n",
      "         [ 0.2804, -0.4437, -0.1255, -0.5616, -0.4855, -0.4043]],\n",
      "\n",
      "        [[ 0.2828, -0.4914, -0.4063, -0.5611, -0.4471, -0.1478],\n",
      "         [ 0.2827, -0.4904, -0.4073, -0.5595, -0.4471, -0.1464],\n",
      "         [ 0.2822, -0.4926, -0.4031, -0.5630, -0.4467, -0.1507],\n",
      "         [ 0.2835, -0.4909, -0.4080, -0.5607, -0.4478, -0.1465],\n",
      "         [ 0.2837, -0.4895, -0.4096, -0.5587, -0.4481, -0.1447]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # q and k/v might be of different sizes if lengths of decoder and encoders inputs are different\n",
    "        bq, tq, dq = q.size()\n",
    "        bk, tk, dk = k.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(bq, tq, self.h, self.dh)\n",
    "        wk = wk.view(bk, tk, self.h, self.dh)\n",
    "        wv = wv.view(bk, tk, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(bq, self.h, tq, self.dh).transpose(1, 2).contiguous().view(bq, tq, dq)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729ae6f8-a3f8-4386-837d-f1e67f022b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, self_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "x = torch.rand(2, 3, 512)\n",
    "\n",
    "encoder_layer(x, self_mask=self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d, h) for _ in range(n)])\n",
    "\n",
    "    def forward(self, x, self_mask = None):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask=self_mask)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "encoder(x, self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = dec_x.size()\n",
    "        x = dec_x + self.attn_dropout(self.mhsa(dec_x, dec_x, dec_x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, enc_x, enc_x, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [DecoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        b, t = dec_x.size()\n",
    "        x = self.embed(dec_x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_x, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "    def get_embed_weights(self):\n",
    "        return self.embed.weight\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b5aba42-4209-4fbb-8cc9-a9e8a0236e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Output(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, d: int = 512, ff_weight = None):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(d, vocab_size)\n",
    "        # weight tying with the decoder embedding\n",
    "        if ff_weight is not None:\n",
    "            self.ff.weight = ff_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2477aec-7287-498b-bf31-cbb0bdfe154d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: \n",
      " tensor([[1, 1, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 1, 1, 0]])\n",
      "torch.Size([3, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8, embed_tying=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        if embed_tying:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d, ff_weight = self.decoder.get_embed_weights())\n",
    "        else:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d)\n",
    "\n",
    "    def forward(self, enc_x, dec_x, enc_mask=None, dec_mask=None):\n",
    "        encoded = self.encoder(enc_x, enc_mask)\n",
    "        decoded = self.decoder(dec_x=dec_x, enc_x=encoded, self_mask=dec_mask, cross_mask=enc_mask)\n",
    "        return self.output(decoded)\n",
    "\n",
    "transformer = Transformer(vocab_size=32, n=2, d=16, h=2, embed_tying=False)\n",
    "enc_x = torch.tensor([[15, 7, 3], [10, 10, 0], [1, 0, 0]])\n",
    "dec_x = torch.tensor([[21, 8, 0, 0], [25, 0, 0, 0], [8, 1, 2, 3]])\n",
    "# dec_x = torch.tensor([[21, 8], [25, 0], [8, 1]])\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=0)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=0)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "print(transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f69bab-e9a5-44a7-af1d-f294f62d3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a06c43eb-c348-4905-9384-1b28378435b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9906,   4435, 100257, 100257, 100257],\n",
      "        [  2028,    374,    264,   4382,  11914],\n",
      "        [  7979, 100257, 100257, 100257, 100257]])\n",
      "tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "enc_mask: \n",
      " tensor([[1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 0, 0, 0]])\n",
      "output shape: torch.Size([3, 4, 100277])\n",
      "softmaxed[0, 0, :10]: tensor([0.0000e+00, 1.2247e-39, 0.0000e+00, 1.1350e-37, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "predicted: \n",
      " tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "predicted decoded: \n",
      " ['Bonjour<|endoftext|><|endoftext|><|endoftext|>', \"C'est une phrase\", 'START<|endoftext|><|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sents = [\"Hello World\", \"This is a simple sentence\", \"Me\"]\n",
    "encoded_sents = [encoding.encode(s) for s in sents]\n",
    "enc_x = pad_sequence([torch.tensor(es) for es in encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(enc_x)\n",
    "dec_sents = [\"Bonjour\", \"C'est une phrase\", \"START\"]\n",
    "dec_encoded_sents = [encoding.encode(s) for s in dec_sents]\n",
    "dec_x = pad_sequence([torch.tensor(es) for es in dec_encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(dec_x)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=100257)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=100257)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "print(f\"output shape: {output.shape}\")\n",
    "softmaxed = F.softmax(output, dim=-1)\n",
    "print(f\"softmaxed[0, 0, :10]: {softmaxed[0, 0, :10]}\")\n",
    "predicted = softmaxed.argmax(dim=-1)\n",
    "print(f\"predicted: \\n {predicted}\")\n",
    "\n",
    "predicted_list = predicted.tolist()\n",
    "predicted_decoded = [encoding.decode(l) for l in predicted_list]\n",
    "print(f\"predicted decoded: \\n {predicted_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "693b16e4-6684-4dbb-b21b-8c3650a10bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 34, 34, 34, 34]\n",
      "predicted sentence: \n",
      " CCCCC\n"
     ]
    }
   ],
   "source": [
    "# Predicting next words\n",
    "sent = \"This is a simple sentence\"\n",
    "encoded_sent = encoding.encode(sent)\n",
    "enc_x = torch.tensor(encoded_sent).unsqueeze(0)\n",
    "dec_x = torch.tensor(encoding.encode(\"C\")).unsqueeze(0)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "predicted_tokens = []\n",
    "for _ in range(5):\n",
    "    output = transformer(enc_x=enc_x, dec_x=dec_x)\n",
    "    softmaxed = F.softmax(output, dim=-1)\n",
    "    predicted = softmaxed.argmax(dim=-1)\n",
    "    predicted_tokens.append(predicted.tolist()[-1][-1]) \n",
    "    dec_x = torch.cat((dec_x, predicted), dim=-1)\n",
    "\n",
    "print(predicted_tokens)\n",
    "print(f\"predicted sentence: \\n {encoding.decode(predicted_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5588cc54-99dc-481d-8a78-d7d9e8adfd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hi.', 'START Salut!')\n",
      "('Stop!', 'START Arrte-toi !')\n",
      "('Hi.', 'START Salut!')\n",
      "('Run!', 'START Cours\\u202f!')\n",
      "('Run!', 'START Courez\\u202f!')\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "\n",
    "class Tokens(Enum):\n",
    "    START = \"START \"\n",
    "    END = \"<|endoftext|>\"\n",
    "    PAD = \" PAD\"\n",
    "    START_NUM = 23380\n",
    "    END_NUM = 100257\n",
    "    PAD_NUM = 62854\n",
    "    \n",
    "\n",
    "class EnFrDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        with open(file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            # we want data indexes start from 0, but filter out the first header row\n",
    "            for i, row in enumerate(reader, start=-1):\n",
    "                if i == -1:\n",
    "                    continue\n",
    "                en = row[0]\n",
    "                fr = Tokens.START.value + row[1]\n",
    "                self._data.append(tuple([en, fr]))\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "\n",
    "    class Iterator():\n",
    "\n",
    "        def __init__(self, outer):\n",
    "            self.cur = 0\n",
    "            self.outer = outer\n",
    "\n",
    "        def __next__(self):\n",
    "            if self.cur == len(self.outer._data):\n",
    "                raise StopIteration()\n",
    "            cur = self.outer._data[self.cur]\n",
    "            self.cur += 1\n",
    "            return cur\n",
    "\n",
    "    def __iter__(self):\n",
    "        return EnFrDataset.Iterator(self)\n",
    "    \n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = EnFrDataset(\"../data/eng_-french.csv\", val_ratio=0.1)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d09eb729-53bb-420b-a608-73b1365e6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([13347, 13], [23380], [8375])\n",
      "([6869, 0], [23380], [18733])\n",
      "([6869, 0], [23380], [18733])\n",
      "([36981, 0], [23380, 64105], [64105, 64])\n",
      "([12978, 0], [23380], [65381])\n"
     ]
    }
   ],
   "source": [
    "class TokEnFrDataset(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def build_train_sample(en_str: str, dec_str: str):\n",
    "        en_encoded = encoding.encode(en_str)\n",
    "        dec_encoded = encoding.encode(dec_str)\n",
    "        dec_encoded.append(Tokens.END_NUM.value)\n",
    "        en_sents = []\n",
    "        dec_sents = []\n",
    "        target_sents = []\n",
    "        \n",
    "        for i in range(1, len(dec_encoded)):\n",
    "            dec_sents.append(dec_encoded[:i])\n",
    "            target_sents.append(dec_encoded[1: i + 1])\n",
    "        en_sents.extend([en_encoded] * len(dec_sents))\n",
    "        return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        self._dataset = EnFrDataset(file, partition, val_ratio=0)\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        i = 0\n",
    "        for en, fr in self._dataset:\n",
    "            for sample in self.build_train_sample(en, fr):\n",
    "                self._data.append(sample)\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "                i += 1\n",
    "\n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french.csv\", val_ratio=0.1)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16252c2e-6c54-44c5-a4aa-4cfb4de90897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 6869,     0],\n",
      "        [36981,     0],\n",
      "        [12978,     0],\n",
      "        [35079,    13],\n",
      "        [10903,     0]]), tensor([[34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "         34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34]]), tensor([[18733, 62854, 62854],\n",
      "        [64105,    64, 62854],\n",
      "        [65381, 62854, 62854],\n",
      "        [16233,  1088,    13],\n",
      "        [14549, 62854, 62854]]), tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]]), tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    # print(batch)\n",
    "    _x, _y, _label = list(zip(*batch))\n",
    "    enc_x = pad_sequence([torch.tensor(t) for t in _x], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    enc_y = pad_sequence([torch.tensor(t) for t in _y], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    label = pad_sequence([torch.tensor(t) for t in _label], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    enc_mask = build_padding_mask(x, pad_token=Tokens.PAD_NUM.value)\n",
    "    dec_mask = build_padding_mask(y, pad_token=Tokens.PAD_NUM.value)\n",
    "    return enc_x, dec_x, label, enc_mask, dec_mask\n",
    "\n",
    "training_generator = DataLoader(dataset, collate_fn=collate, batch_size=5, num_workers=0)\n",
    "for batch in training_generator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98593c81-b0c3-4145-95bc-1965620ceb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (23) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m             model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(timestamp, epoch_number)\n\u001b[1;32m     79\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n\u001b[0;32m---> 81\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 73\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 73\u001b[0m     avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     avg_val_loss \u001b[38;5;241m=\u001b[39m validate_epoch()\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m avg_vloss \u001b[38;5;241m<\u001b[39m best_vloss:\n",
      "Cell \u001b[0;32mIn[32], line 37\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Clear grads\u001b[39;00m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, encoding\u001b[38;5;241m.\u001b[39mn_vocab), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     39\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, enc_x, dec_x, enc_mask, dec_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, enc_x, dec_x, enc_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dec_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 15\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(dec_x\u001b[38;5;241m=\u001b[39mdec_x, enc_x\u001b[38;5;241m=\u001b[39mencoded, self_mask\u001b[38;5;241m=\u001b[39mdec_mask, cross_mask\u001b[38;5;241m=\u001b[39menc_mask)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(decoded)\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, self_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(x)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, self_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, self_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m     b, t, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmhsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff2(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff1(x))))\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m, in \u001b[0;36mMHSAMasked.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m wv \u001b[38;5;241m=\u001b[39m wv\u001b[38;5;241m.\u001b[39mview(bk, tk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdh)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# b, t, h, dh\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if changing from 4 dim -> 3 dim: b*h, t, dh\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# changing the number of dims is not necessary as @ supports 4 dims\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mself_attention_masked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# b * h, t, dh\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mview(bq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, tq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdh)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bq, tq, dq)\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mself_attention_masked\u001b[0;34m(q, k, v, mask, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     scaled_prod \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_prod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_prod: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaled_prod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (23) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import torch\n",
    "import math\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# torch.set_default_device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "training_params = {\n",
    "    'collate_fn': collate,\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0}\n",
    "max_epochs = 5\n",
    "model_path = \"../data/model.pth\"\n",
    "\n",
    "# Generators\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french.csv\", val_ratio=0.05)\n",
    "dataloader = DataLoader(dataset, **training_params)\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "transformer = transformer.to(device)\n",
    "loss_fn = F.cross_entropy\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "def train_epoch():\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    transformer.train(True)\n",
    "    dataset.partition = Partition.TRAIN\n",
    "\n",
    "    for enc_x, dec_x, label, enc_mask, dec_mask in dataloader:\n",
    "        enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "        # Clear grads\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "        loss = loss_fn(output.view(-1, encoding.n_vocab), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000\n",
    "            print('Average batch loss: {}'.format(last_loss))\n",
    "            running_loss = 0.\n",
    "    print('Average epoch loss: {}'.format(last_loss))\n",
    "    return last_loss\n",
    "\n",
    "def validate_epoch():\n",
    "    running_vloss = 0.0\n",
    "    transformer.eval()\n",
    "    dataset.partition = PARTITION.VAL\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = vdata\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "            output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "            vloss = loss_fn(output, label)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('Average valid loss {}'.format(avg_vloss))\n",
    "\n",
    "    return avg_vloss\n",
    "\n",
    "def train():\n",
    "    best_vloss = math.inf\n",
    "    for epoch in range(max_epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        avg_train_loss = train_epoch()\n",
    "        \n",
    "        avg_val_loss = validate_epoch()\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819338a-9df0-465b-b1cf-a0a6e8a59d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((2, 3, 32), requires_grad=True)\n",
    "print(input.view(-1, 32).shape)\n",
    "target = torch.empty((2, 3), dtype=torch.long).random_(32)\n",
    "print(target)\n",
    "loss = F.cross_entropy(input.view(-1, 32), target.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05248576-1b7f-4994-80fe-c26d2dc1a81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef7bbe-c8a8-493e-8677-49d9c02dbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def build_train_sample(en_str: str, dec_str: str):\n",
    "    en_encoded = encoding.encode(en_str)\n",
    "    dec_encoded = encoding.encode(dec_str)\n",
    "    dec_encoded.append(Tokens.END_NUM.value)\n",
    "    en_sents = []\n",
    "    dec_sents = []\n",
    "    target_sents = []\n",
    "    \n",
    "    for i in range(1, len(dec_encoded)):\n",
    "        dec_sents.append(dec_encoded[:i])\n",
    "        target_sents.append(dec_encoded[1: i + 1])\n",
    "    en_sents.extend([en_encoded] * len(dec_sents))\n",
    "    return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "build_train_sample('Hi.', 'START Salut!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfee8d-38b9-4abb-b127-5e620ea387f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# def collate_fn\n",
    "\n",
    "dataset.partition = Partition.TRAIN\n",
    "training_generator = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "for i, s in enumerate(training_generator):\n",
    "    print(s)\n",
    "    if i > 2:\n",
    "        break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d5995-4736-4fec-9e51-e3672a9dc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty((2, 3), dtype=torch.long).random_(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ee0f4c4-a05a-4353-b58d-f50a60157f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n",
      "mps:0\n",
      "cpu\n",
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(transformer.encoder.layers[0].mhsa.wq.weight.device)\n",
    "print(next(transformer.parameters()).device)\n",
    "print(transformer.decoder.layers[0].mhsa.wq.weight.device)\n",
    "print(next(transformer.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd67961-4fe8-4c69-90ec-f351df169e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62b3c0-a20a-466d-b5fe-05b33071e963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0dbc9-7e82-48b4-9b90-0ddf28b10784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21175a-9751-4413-8cdb-579c52134778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207745b-3455-48f2-a6c1-d6546569344a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbfc3e-aca8-4edb-99e5-025c945d85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da787162-9955-4de5-a82d-60501d62ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08f9da-be37-4a19-992e-11b2c6e1ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a2346-9e74-4eab-aab3-bd5bb0a0d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.3]]), dim=-1)\n",
    "torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]).argmax(dim=-1)\n",
    "# torch.max(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]), dim=-1)\n",
    "\n",
    "t1 = torch.tensor([[0.1, 0.2]])\n",
    "t2 = torch.tensor([[0.3]])\n",
    "torch.cat((t1, t2), dim=1).tolist()[-1][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683599c0-fd00-40b0-8bb1-4e477a2114a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45e55c-6d60-409b-b26e-0b218224483a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e70b0-a647-455b-bdfb-332d095aafc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082af878-9425-4445-b4b4-592cef79a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba170-623a-486b-8b67-58360bde104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d1a8-69aa-4c60-b3b4-ebb2e31f1c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
