{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d7f736-b9af-48e3-adb5-18ab3536abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v, verbose=False):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    if verbose:\n",
    "        print(softmaxed_prod.shape)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x, verbose=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abcca093-9e66-4b46-8ec3-aded63ece6f8",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a9e6b-8461-4417-ab07-e5fbd79c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c5bf9-3800-4b22-b7d8-8886d4f15b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d626059-fd88-4a03-8b20-ffbf0115fe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayerWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayerWithoutMask()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851eff3d-4816-4410-9184-f30c157e4eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayerWithoutMask(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = EncoderWithoutMask()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(torch.tensor([[1, 0, 0], [1, 1, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With masks\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None, verbose=False):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    if verbose:\n",
    "        print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim), e.g. # b t -> b 1 1 t\n",
    "    if mask is not None:\n",
    "        mask = mask if scaled_prod.dim() == mask.dim() else reshape_mask(mask)\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    if verbose:\n",
    "        print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    if verbose:\n",
    "        print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eac5e23-ea43-4295-ba24-a7edbe1a1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.2958, 0.0894, 0.2666, 0.8261],\n",
      "          [0.3495, 0.9462, 0.6804, 0.8643]],\n",
      "\n",
      "         [[0.7622, 0.6016, 0.5023, 0.1705],\n",
      "          [0.2545, 0.0762, 0.8880, 0.7895]],\n",
      "\n",
      "         [[0.7329, 0.8554, 0.1766, 0.5058],\n",
      "          [0.6196, 0.3626, 0.7632, 0.3341]]],\n",
      "\n",
      "\n",
      "        [[[0.1962, 0.5187, 0.7169, 0.2796],\n",
      "          [0.8376, 0.3550, 0.7672, 0.7943]],\n",
      "\n",
      "         [[0.6612, 0.4665, 0.7727, 0.4648],\n",
      "          [0.9397, 0.3290, 0.2677, 0.5035]],\n",
      "\n",
      "         [[0.8944, 0.6231, 0.1245, 0.2457],\n",
      "          [0.3045, 0.2844, 0.5232, 0.7665]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[[0.4245, 0.2770,   -inf],\n",
      "           [0.2770, 0.6122,   -inf],\n",
      "           [0.3791, 0.6241,   -inf]],\n",
      "\n",
      "          [[1.1137, 0.7237,   -inf],\n",
      "           [0.7237, 0.7412,   -inf],\n",
      "           [0.6838, 0.5634,   -inf]]],\n",
      "\n",
      "\n",
      "         [[[0.4498, 0.5278,   -inf],\n",
      "           [0.5278, 0.7339,   -inf],\n",
      "           [0.3283, 0.5462,   -inf]],\n",
      "\n",
      "          [[1.0236, 0.7546,   -inf],\n",
      "           [0.7546, 0.6583,   -inf],\n",
      "           [0.6831, 0.4529,   -inf]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.4245,   -inf,   -inf],\n",
      "           [0.2770,   -inf,   -inf],\n",
      "           [0.3791,   -inf,   -inf]],\n",
      "\n",
      "          [[1.1137,   -inf,   -inf],\n",
      "           [0.7237,   -inf,   -inf],\n",
      "           [0.6838,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "         [[[0.4498,   -inf,   -inf],\n",
      "           [0.5278,   -inf,   -inf],\n",
      "           [0.3283,   -inf,   -inf]],\n",
      "\n",
      "          [[1.0236,   -inf,   -inf],\n",
      "           [0.7546,   -inf,   -inf],\n",
      "           [0.6831,   -inf,   -inf]]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[[0.5368, 0.4632, 0.0000],\n",
      "           [0.4170, 0.5830, 0.0000],\n",
      "           [0.4390, 0.5610, 0.0000]],\n",
      "\n",
      "          [[0.5963, 0.4037, 0.0000],\n",
      "           [0.4956, 0.5044, 0.0000],\n",
      "           [0.5301, 0.4699, 0.0000]]],\n",
      "\n",
      "\n",
      "         [[[0.4805, 0.5195, 0.0000],\n",
      "           [0.4486, 0.5514, 0.0000],\n",
      "           [0.4457, 0.5543, 0.0000]],\n",
      "\n",
      "          [[0.5668, 0.4332, 0.0000],\n",
      "           [0.5241, 0.4759, 0.0000],\n",
      "           [0.5573, 0.4427, 0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "          [[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "         [[[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "          [[1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000],\n",
      "           [1.0000, 0.0000, 0.0000]]]]])\n",
      "wrong a: \n",
      " tensor([[[[[0.5118, 0.3266, 0.3758, 0.5224],\n",
      "           [0.5677, 0.3880, 0.4040, 0.4439],\n",
      "           [0.5574, 0.3767, 0.3988, 0.4583]],\n",
      "\n",
      "          [[0.3112, 0.5950, 0.7642, 0.8341],\n",
      "           [0.3016, 0.5074, 0.7851, 0.8265],\n",
      "           [0.3049, 0.5374, 0.7779, 0.8291]]],\n",
      "\n",
      "\n",
      "         [[[0.4378, 0.4916, 0.7458, 0.3758],\n",
      "           [0.4526, 0.4899, 0.7476, 0.3817],\n",
      "           [0.4539, 0.4898, 0.7478, 0.3823]],\n",
      "\n",
      "          [[0.8819, 0.3437, 0.5508, 0.6683],\n",
      "           [0.8862, 0.3426, 0.5295, 0.6559],\n",
      "           [0.8828, 0.3435, 0.5461, 0.6656]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.2958, 0.0894, 0.2666, 0.8261],\n",
      "           [0.2958, 0.0894, 0.2666, 0.8261],\n",
      "           [0.2958, 0.0894, 0.2666, 0.8261]],\n",
      "\n",
      "          [[0.3495, 0.9462, 0.6804, 0.8643],\n",
      "           [0.3495, 0.9462, 0.6804, 0.8643],\n",
      "           [0.3495, 0.9462, 0.6804, 0.8643]]],\n",
      "\n",
      "\n",
      "         [[[0.1962, 0.5187, 0.7169, 0.2796],\n",
      "           [0.1962, 0.5187, 0.7169, 0.2796],\n",
      "           [0.1962, 0.5187, 0.7169, 0.2796]],\n",
      "\n",
      "          [[0.8376, 0.3550, 0.7672, 0.7943],\n",
      "           [0.8376, 0.3550, 0.7672, 0.7943],\n",
      "           [0.8376, 0.3550, 0.7672, 0.7943]]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.4245, 0.2770,   -inf],\n",
      "          [0.2770, 0.6122,   -inf],\n",
      "          [0.3791, 0.6241,   -inf]],\n",
      "\n",
      "         [[1.1137, 0.7237,   -inf],\n",
      "          [0.7237, 0.7412,   -inf],\n",
      "          [0.6838, 0.5634,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4498,   -inf,   -inf],\n",
      "          [0.5278,   -inf,   -inf],\n",
      "          [0.3283,   -inf,   -inf]],\n",
      "\n",
      "         [[1.0236,   -inf,   -inf],\n",
      "          [0.7546,   -inf,   -inf],\n",
      "          [0.6831,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5368, 0.4632, 0.0000],\n",
      "          [0.4170, 0.5830, 0.0000],\n",
      "          [0.4390, 0.5610, 0.0000]],\n",
      "\n",
      "         [[0.5963, 0.4037, 0.0000],\n",
      "          [0.4956, 0.5044, 0.0000],\n",
      "          [0.5301, 0.4699, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.5118, 0.3266, 0.3758, 0.5224],\n",
      "          [0.5677, 0.3880, 0.4040, 0.4439],\n",
      "          [0.5574, 0.3767, 0.3988, 0.4583]],\n",
      "\n",
      "         [[0.3112, 0.5950, 0.7642, 0.8341],\n",
      "          [0.3016, 0.5074, 0.7851, 0.8265],\n",
      "          [0.3049, 0.5374, 0.7779, 0.8291]]],\n",
      "\n",
      "\n",
      "        [[[0.1962, 0.5187, 0.7169, 0.2796],\n",
      "          [0.1962, 0.5187, 0.7169, 0.2796],\n",
      "          [0.1962, 0.5187, 0.7169, 0.2796]],\n",
      "\n",
      "         [[0.8376, 0.3550, 0.7672, 0.7943],\n",
      "          [0.8376, 0.3550, 0.7672, 0.7943],\n",
      "          [0.8376, 0.3550, 0.7672, 0.7943]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask, verbose=True)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask, verbose=True)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.2958, 0.0894, 0.2666, 0.8261],\n",
      "          [0.3495, 0.9462, 0.6804, 0.8643]],\n",
      "\n",
      "         [[0.7622, 0.6016, 0.5023, 0.1705],\n",
      "          [0.2545, 0.0762, 0.8880, 0.7895]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.1962, 0.5187, 0.7169, 0.2796],\n",
      "          [0.8376, 0.3550, 0.7672, 0.7943]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.4245, 0.2770,   -inf],\n",
      "          [0.2770, 0.6122,   -inf],\n",
      "          [0.3791, 0.6241,   -inf]],\n",
      "\n",
      "         [[1.1137, 0.7237,   -inf],\n",
      "          [0.7237, 0.7412,   -inf],\n",
      "          [0.6838, 0.5634,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4498,   -inf,   -inf],\n",
      "          [0.5278,   -inf,   -inf],\n",
      "          [0.3283,   -inf,   -inf]],\n",
      "\n",
      "         [[1.0236,   -inf,   -inf],\n",
      "          [0.7546,   -inf,   -inf],\n",
      "          [0.6831,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5368, 0.4632, 0.0000],\n",
      "          [0.4170, 0.5830, 0.0000],\n",
      "          [0.4390, 0.5610, 0.0000]],\n",
      "\n",
      "         [[0.5963, 0.4037, 0.0000],\n",
      "          [0.4956, 0.5044, 0.0000],\n",
      "          [0.5301, 0.4699, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.5118, 0.3266, 0.3758, 0.5224],\n",
      "          [0.5677, 0.3880, 0.4040, 0.4439],\n",
      "          [0.5574, 0.3767, 0.3988, 0.4583]],\n",
      "\n",
      "         [[0.3112, 0.5950, 0.7642, 0.8341],\n",
      "          [0.3016, 0.5074, 0.7851, 0.8265],\n",
      "          [0.3049, 0.5374, 0.7779, 0.8291]]],\n",
      "\n",
      "\n",
      "        [[[0.1962, 0.5187, 0.7169, 0.2796],\n",
      "          [0.1962, 0.5187, 0.7169, 0.2796],\n",
      "          [0.1962, 0.5187, 0.7169, 0.2796]],\n",
      "\n",
      "         [[0.8376, 0.3550, 0.7672, 0.7943],\n",
      "          [0.8376, 0.3550, 0.7672, 0.7943],\n",
      "          [0.8376, 0.3550, 0.7672, 0.7943]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.4257, 0.2534, 0.0774, 0.9322],\n",
      "         [0.1857, 0.6218, 0.4876, 0.5833],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1398, 0.5328, 0.0799, 0.0875],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.4257, 0.2534],\n",
      "          [0.0774, 0.9322]],\n",
      "\n",
      "         [[0.1857, 0.6218],\n",
      "          [0.4876, 0.5833]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1398, 0.5328],\n",
      "          [0.0799, 0.0875]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.4257, 0.2534],\n",
      "          [0.1857, 0.6218],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0774, 0.9322],\n",
      "          [0.4876, 0.5833],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1398, 0.5328],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0799, 0.0875],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[0.5766, 0.9864, 0.6510, 0.1057],\n",
      "         [0.7303, 0.0179, 0.4643, 0.7960],\n",
      "         [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "        [[0.1977, 0.1120, 0.2508, 0.1560],\n",
      "         [  -inf,   -inf,   -inf,   -inf],\n",
      "         [  -inf,   -inf,   -inf,   -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[0.5766, 0.9864],\n",
      "          [0.6510, 0.1057]],\n",
      "\n",
      "         [[0.7303, 0.0179],\n",
      "          [0.4643, 0.7960]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.1977, 0.1120],\n",
      "          [0.2508, 0.1560]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[0.5766, 0.9864],\n",
      "          [0.7303, 0.0179],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.6510, 0.1057],\n",
      "          [0.4643, 0.7960],\n",
      "          [  -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.1977, 0.1120],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]],\n",
      "\n",
      "         [[0.2508, 0.1560],\n",
      "          [  -inf,   -inf],\n",
      "          [  -inf,   -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[1.3054, 0.4388,   -inf],\n",
      "          [0.4388, 0.5337,   -inf],\n",
      "          [1.0854, 0.7076,   -inf]],\n",
      "\n",
      "         [[0.4350, 0.3864,   -inf],\n",
      "          [0.3864, 0.8492,   -inf],\n",
      "          [0.6735, 1.1256,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.0517,   -inf,   -inf],\n",
      "          [0.2534,   -inf,   -inf],\n",
      "          [0.1843,   -inf,   -inf]],\n",
      "\n",
      "         [[0.0872,   -inf,   -inf],\n",
      "          [0.1424,   -inf,   -inf],\n",
      "          [0.3876,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x, verbose=True)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1881e-01, 8.8276e-01, 2.8630e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.7504e-01, 8.9537e-01, 4.7654e-01, 9.1876e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [7.4581e-01, 4.9047e-01, 3.9946e-01, 2.4741e-01, 6.6694e-01, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.6323e-02, 5.9723e-01, 4.0982e-02, 7.1823e-01, 6.4752e-01, 9.8167e-01]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.8473e-01, 6.2087e-01, 6.2271e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [6.4629e-01, 3.9059e-02, 8.3976e-01, 9.2923e-01, 9.5110e-01, 1.0000e+02],\n",
      "        [4.3513e-01, 9.8206e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [7.8281e-01, 8.4389e-02, 9.7876e-01, 9.9684e-02, 1.1792e-01, 3.3050e-02]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "tensor([[[-0.5504,  0.3112, -0.2068, -0.0255,  0.6351,  0.0681],\n",
      "         [-0.5504,  0.3112, -0.2068, -0.0255,  0.6351,  0.0681],\n",
      "         [-0.5504,  0.3112, -0.2068, -0.0255,  0.6351,  0.0681],\n",
      "         [-0.5504,  0.3112, -0.2068, -0.0255,  0.6351,  0.0681],\n",
      "         [-0.5504,  0.3112, -0.2068, -0.0255,  0.6351,  0.0681]],\n",
      "\n",
      "        [[-0.4952,  0.1333, -0.1883, -0.0413,  0.5733, -0.0226],\n",
      "         [-0.5028,  0.1473, -0.1856, -0.0492,  0.5801, -0.0232],\n",
      "         [-0.4957,  0.1377, -0.1907, -0.0403,  0.5746, -0.0229],\n",
      "         [-0.4973,  0.1358, -0.1864, -0.0441,  0.5748, -0.0227],\n",
      "         [-0.4980,  0.1328, -0.1831, -0.0466,  0.5743, -0.0225]],\n",
      "\n",
      "        [[-0.4951,  0.1717, -0.2065, -0.0435,  0.5800, -0.0530],\n",
      "         [-0.4956,  0.1729, -0.2056, -0.0453,  0.5802, -0.0536],\n",
      "         [-0.4961,  0.1730, -0.2061, -0.0447,  0.5804, -0.0538],\n",
      "         [-0.4963,  0.1720, -0.2063, -0.0439,  0.5803, -0.0536],\n",
      "         [-0.4947,  0.1716, -0.2071, -0.0432,  0.5800, -0.0529]],\n",
      "\n",
      "        [[-0.4670,  0.1664, -0.1724, -0.0761,  0.5226, -0.1178],\n",
      "         [-0.4646,  0.1558, -0.1689, -0.0741,  0.5175, -0.1145],\n",
      "         [-0.4682,  0.1579, -0.1681, -0.0741,  0.5202, -0.1129],\n",
      "         [-0.4686,  0.1666, -0.1716, -0.0758,  0.5242, -0.1153],\n",
      "         [-0.4664,  0.1626, -0.1704, -0.0758,  0.5216, -0.1149]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # q and k/v might be of different sizes if lengths of decoder and encoders inputs are different\n",
    "        bq, tq, dq = q.size()\n",
    "        bk, tk, dk = k.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(bq, tq, self.h, self.dh)\n",
    "        wk = wk.view(bk, tk, self.h, self.dh)\n",
    "        wv = wv.view(bk, tk, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(bq, self.h, tq, self.dh).transpose(1, 2).contiguous().view(bq, tq, dq)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729ae6f8-a3f8-4386-837d-f1e67f022b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, self_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "x = torch.rand(2, 3, 512)\n",
    "\n",
    "encoder_layer(x, self_mask=self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d, h) for _ in range(n)])\n",
    "\n",
    "    def forward(self, x, self_mask = None):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask=self_mask)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "encoder(x, self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = dec_x.size()\n",
    "        x = dec_x + self.attn_dropout(self.mhsa(dec_x, dec_x, dec_x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, enc_x, enc_x, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d, h) for _ in range(n)])\n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        b, t = dec_x.size()\n",
    "        x = self.embed(dec_x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_x, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "    def get_embed_weights(self):\n",
    "        return self.embed.weight\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b5aba42-4209-4fbb-8cc9-a9e8a0236e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Output(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, d: int = 512, ff_weight = None):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(d, vocab_size)\n",
    "        # weight tying with the decoder embedding\n",
    "        if ff_weight is not None:\n",
    "            self.ff.weight = ff_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2477aec-7287-498b-bf31-cbb0bdfe154d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: \n",
      " tensor([[1, 1, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 1, 1, 0]])\n",
      "torch.Size([3, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8, embed_tying=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        if embed_tying:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d, ff_weight = self.decoder.get_embed_weights())\n",
    "        else:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d)\n",
    "\n",
    "    def forward(self, enc_x, dec_x, enc_mask=None, dec_mask=None):\n",
    "        encoded = self.encoder(enc_x, enc_mask)\n",
    "        decoded = self.decoder(dec_x=dec_x, enc_x=encoded, self_mask=dec_mask, cross_mask=enc_mask)\n",
    "        return self.output(decoded)\n",
    "\n",
    "transformer = Transformer(vocab_size=32, n=2, d=16, h=2, embed_tying=False)\n",
    "enc_x = torch.tensor([[15, 7, 3], [10, 10, 0], [1, 0, 0]])\n",
    "dec_x = torch.tensor([[21, 8, 0, 0], [25, 0, 0, 0], [8, 1, 2, 3]])\n",
    "# dec_x = torch.tensor([[21, 8], [25, 0], [8, 1]])\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=0)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=0)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "print(transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f69bab-e9a5-44a7-af1d-f294f62d3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a06c43eb-c348-4905-9384-1b28378435b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9906,   4435, 100257, 100257, 100257],\n",
      "        [  2028,    374,    264,   4382,  11914],\n",
      "        [  7979, 100257, 100257, 100257, 100257]])\n",
      "tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "enc_mask: \n",
      " tensor([[1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 0, 0, 0]])\n",
      "output shape: torch.Size([3, 4, 100277])\n",
      "softmaxed[0, 0, :10]: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "predicted: \n",
      " tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "predicted decoded: \n",
      " ['Bonjour<|endoftext|><|endoftext|><|endoftext|>', \"C'est une phrase\", 'START<|endoftext|><|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sents = [\"Hello World\", \"This is a simple sentence\", \"Me\"]\n",
    "encoded_sents = [encoding.encode(s) for s in sents]\n",
    "enc_x = pad_sequence([torch.tensor(es) for es in encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(enc_x)\n",
    "dec_sents = [\"Bonjour\", \"C'est une phrase\", \"START\"]\n",
    "dec_encoded_sents = [encoding.encode(s) for s in dec_sents]\n",
    "dec_x = pad_sequence([torch.tensor(es) for es in dec_encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(dec_x)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=100257)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=100257)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "print(f\"output shape: {output.shape}\")\n",
    "softmaxed = F.softmax(output, dim=-1)\n",
    "print(f\"softmaxed[0, 0, :10]: {softmaxed[0, 0, :10]}\")\n",
    "predicted = softmaxed.argmax(dim=-1)\n",
    "print(f\"predicted: \\n {predicted}\")\n",
    "\n",
    "predicted_list = predicted.tolist()\n",
    "predicted_decoded = [encoding.decode(l) for l in predicted_list]\n",
    "print(f\"predicted decoded: \\n {predicted_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "693b16e4-6684-4dbb-b21b-8c3650a10bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 34, 34, 34, 34]\n",
      "predicted sentence: \n",
      " CCCCC\n"
     ]
    }
   ],
   "source": [
    "# Predicting next words\n",
    "sent = \"This is a simple sentence\"\n",
    "encoded_sent = encoding.encode(sent)\n",
    "enc_x = torch.tensor(encoded_sent).unsqueeze(0)\n",
    "dec_x = torch.tensor(encoding.encode(\"C\")).unsqueeze(0)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "predicted_tokens = []\n",
    "for _ in range(5):\n",
    "    output = transformer(enc_x=enc_x, dec_x=dec_x)\n",
    "    softmaxed = F.softmax(output, dim=-1)\n",
    "    predicted = softmaxed.argmax(dim=-1)\n",
    "    predicted_tokens.append(predicted.tolist()[-1][-1]) \n",
    "    dec_x = torch.cat((dec_x, predicted), dim=-1)\n",
    "\n",
    "print(predicted_tokens)\n",
    "print(f\"predicted sentence: \\n {encoding.decode(predicted_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5588cc54-99dc-481d-8a78-d7d9e8adfd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello!', 'START Salut !')\n",
      "('Attack!', 'START Attaquez !')\n",
      "('Hello!', 'START Salut !')\n",
      "('I see.', 'START Je comprends.')\n",
      "('I try.', \"START J'essaye.\")\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "\n",
    "class Tokens(Enum):\n",
    "    START = \"START \"\n",
    "    END = \"<|endoftext|>\"\n",
    "    PAD = \" PAD\"\n",
    "    START_NUM = 23380\n",
    "    END_NUM = 100257\n",
    "    PAD_NUM = 62854\n",
    "    \n",
    "\n",
    "class EnFrDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        with open(file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            # we want data indexes start from 0, but filter out the first header row\n",
    "            for i, row in enumerate(reader, start=-1):\n",
    "                if i == -1:\n",
    "                    continue\n",
    "                en = row[0]\n",
    "                fr = Tokens.START.value + row[1]\n",
    "                self._data.append(tuple([en, fr]))\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "\n",
    "    class Iterator():\n",
    "\n",
    "        def __init__(self, outer):\n",
    "            self.cur = 0\n",
    "            self.outer = outer\n",
    "\n",
    "        def __next__(self):\n",
    "            if self.cur == len(self.outer._data):\n",
    "                raise StopIteration()\n",
    "            cur = self.outer._data[self.cur]\n",
    "            self.cur += 1\n",
    "            return cur\n",
    "\n",
    "    def __iter__(self):\n",
    "        return EnFrDataset.Iterator(self)\n",
    "    \n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = EnFrDataset(\"../data/eng_-french-nano.csv\", val_ratio=0.3)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d09eb729-53bb-420b-a608-73b1365e6dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([9906, 0], [23380], [8375])\n",
      "([40, 1518, 13], [23380], [14465])\n",
      "([40, 1518, 13], [23380], [14465])\n",
      "([40, 1518, 13], [23380, 14465, 60946, 82], [14465, 60946, 82, 13])\n",
      "([40, 1456, 13], [23380, 622], [622, 6])\n"
     ]
    }
   ],
   "source": [
    "class TokEnFrDataset(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def build_train_sample(en_str: str, dec_str: str):\n",
    "        en_encoded = encoding.encode(en_str)\n",
    "        dec_encoded = encoding.encode(dec_str)\n",
    "        dec_encoded.append(Tokens.END_NUM.value)\n",
    "        en_sents = []\n",
    "        dec_sents = []\n",
    "        target_sents = []\n",
    "        \n",
    "        for i in range(1, len(dec_encoded)):\n",
    "            dec_sents.append(dec_encoded[:i])\n",
    "            target_sents.append(dec_encoded[1: i + 1])\n",
    "        en_sents.extend([en_encoded] * len(dec_sents))\n",
    "        return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "    def __init__(self, file: Path | str, partition: Partition = Partition.TRAIN, val_ratio: float = 0.1):\n",
    "        self._dataset = EnFrDataset(file, partition, val_ratio=0)\n",
    "        # partition = TRAIN | VAL\n",
    "        self._partition = partition\n",
    "        self._val_ratio = val_ratio\n",
    "\n",
    "        self._data = []\n",
    "        self._train_map: dict[int, int] = {}\n",
    "        self._val_map: dict[int, int] = {}\n",
    "        train_id = 0\n",
    "        val_id = 0\n",
    "        i = 0\n",
    "        for en, fr in self._dataset:\n",
    "            for sample in self.build_train_sample(en, fr):\n",
    "                self._data.append(sample)\n",
    "                if int(i * val_ratio) == int((i - 1) * val_ratio):\n",
    "                    self._train_map[train_id] = i\n",
    "                    train_id += 1\n",
    "                else:\n",
    "                    self._val_map[val_id] = i\n",
    "                    val_id += 1\n",
    "                i += 1\n",
    "\n",
    "    @property\n",
    "    def partition(self):\n",
    "        return self._partition\n",
    "\n",
    "    @partition.setter\n",
    "    def partition(self, partition):\n",
    "        self._partition = partition\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._train_map) if self._partition == Partition.TRAIN else len(self._val_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[self._train_map[idx]] if self._partition == Partition.TRAIN else self._data[self._val_map[idx]]\n",
    "\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french-nano.csv\", val_ratio=0.3)\n",
    "train_sample = dataset[0]\n",
    "dataset.partition = Partition.VAL\n",
    "val_sample = dataset[0]\n",
    "assert train_sample != val_sample\n",
    "print(train_sample)\n",
    "print(val_sample)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16252c2e-6c54-44c5-a4aa-4cfb4de90897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  40, 1518,   13],\n",
      "        [  40, 1518,   13],\n",
      "        [  40, 1456,   13],\n",
      "        [  40, 1456,   13],\n",
      "        [  40, 2834,    0]]), tensor([[23380, 62854, 62854, 62854, 62854, 62854],\n",
      "        [23380, 14465, 60946,    82, 62854, 62854],\n",
      "        [23380,   622, 62854, 62854, 62854, 62854],\n",
      "        [23380,   622,     6,   434, 61055,    13],\n",
      "        [23380,   622, 34155, 62854, 62854, 62854]]), tensor([[ 14465,  62854,  62854,  62854,  62854,  62854],\n",
      "        [ 14465,  60946,     82,     13,  62854,  62854],\n",
      "        [   622,      6,  62854,  62854,  62854,  62854],\n",
      "        [   622,      6,    434,  61055,     13, 100257],\n",
      "        [   622,  34155,    342,  62854,  62854,  62854]]), tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]]), tensor([[1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    # print(batch)\n",
    "    _x, _y, _label = list(zip(*batch))\n",
    "    enc_x = pad_sequence([torch.tensor(t) for t in _x], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    dec_x = pad_sequence([torch.tensor(t) for t in _y], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    label = pad_sequence([torch.tensor(t) for t in _label], batch_first=True, padding_value=Tokens.PAD_NUM.value)\n",
    "    enc_mask = build_padding_mask(enc_x, pad_token=Tokens.PAD_NUM.value)\n",
    "    dec_mask = build_padding_mask(dec_x, pad_token=Tokens.PAD_NUM.value)\n",
    "    return enc_x, dec_x, label, enc_mask, dec_mask\n",
    "\n",
    "training_generator = DataLoader(dataset, collate_fn=collate, batch_size=5, num_workers=0)\n",
    "for batch in training_generator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98593c81-b0c3-4145-95bc-1965620ceb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Number of model's params: 13169205\n",
      "EPOCH 1:\n",
      "Average epoch loss: 22.232242584228516\n",
      "Average valid loss 34.81196975708008\n",
      "EPOCH 2:\n",
      "Average epoch loss: 21.342653846740724\n",
      "Average valid loss 30.77802276611328\n",
      "EPOCH 3:\n",
      "Average epoch loss: 20.467308807373048\n",
      "Average valid loss 29.754196166992188\n",
      "EPOCH 4:\n",
      "Average epoch loss: 19.161551475524902\n",
      "Average valid loss 32.197757720947266\n",
      "EPOCH 5:\n",
      "Average epoch loss: 18.128786087036133\n",
      "Average valid loss 31.178752899169922\n",
      "EPOCH 6:\n",
      "Average epoch loss: 18.322573471069337\n",
      "Average valid loss 26.48904800415039\n",
      "EPOCH 7:\n",
      "Average epoch loss: 16.847139263153075\n",
      "Average valid loss 24.466577529907227\n",
      "EPOCH 8:\n",
      "Average epoch loss: 16.73818368911743\n",
      "Average valid loss 26.39546775817871\n",
      "EPOCH 9:\n",
      "Average epoch loss: 17.362816333770752\n",
      "Average valid loss 23.45684814453125\n",
      "EPOCH 10:\n",
      "Average epoch loss: 16.146892261505126\n",
      "Average valid loss 24.626707077026367\n",
      "EPOCH 11:\n",
      "Average epoch loss: 16.961020851135252\n",
      "Average valid loss 22.789432525634766\n",
      "EPOCH 12:\n",
      "Average epoch loss: 15.656363105773925\n",
      "Average valid loss 21.20294761657715\n",
      "EPOCH 13:\n",
      "Average epoch loss: 15.570201873779297\n",
      "Average valid loss 21.487255096435547\n",
      "EPOCH 14:\n",
      "Average epoch loss: 15.672863674163818\n",
      "Average valid loss 20.562021255493164\n",
      "EPOCH 15:\n",
      "Average epoch loss: 15.205952548980713\n",
      "Average valid loss 20.335498809814453\n",
      "EPOCH 16:\n",
      "Average epoch loss: 14.552958965301514\n",
      "Average valid loss 20.89142608642578\n",
      "EPOCH 17:\n",
      "Average epoch loss: 14.154693222045898\n",
      "Average valid loss 20.52867889404297\n",
      "EPOCH 18:\n",
      "Average epoch loss: 14.0248948097229\n",
      "Average valid loss 17.299274444580078\n",
      "EPOCH 19:\n",
      "Average epoch loss: 13.24451141357422\n",
      "Average valid loss 19.934276580810547\n",
      "EPOCH 20:\n",
      "Average epoch loss: 13.681490612030029\n",
      "Average valid loss 18.855642318725586\n",
      "EPOCH 21:\n",
      "Average epoch loss: 13.775192165374756\n",
      "Average valid loss 17.008342742919922\n",
      "EPOCH 22:\n",
      "Average epoch loss: 13.711312389373779\n",
      "Average valid loss 16.759660720825195\n",
      "EPOCH 23:\n",
      "Average epoch loss: 13.678682422637939\n",
      "Average valid loss 20.239282608032227\n",
      "EPOCH 24:\n",
      "Average epoch loss: 13.385561180114745\n",
      "Average valid loss 18.932374954223633\n",
      "EPOCH 25:\n",
      "Average epoch loss: 13.22812261581421\n",
      "Average valid loss 14.934167861938477\n",
      "EPOCH 26:\n",
      "Average epoch loss: 12.45798635482788\n",
      "Average valid loss 19.83230209350586\n",
      "EPOCH 27:\n",
      "Average epoch loss: 12.734461498260497\n",
      "Average valid loss 18.0056209564209\n",
      "EPOCH 28:\n",
      "Average epoch loss: 12.219795894622802\n",
      "Average valid loss 15.093745231628418\n",
      "EPOCH 29:\n",
      "Average epoch loss: 12.380380153656006\n",
      "Average valid loss 16.834392547607422\n",
      "EPOCH 30:\n",
      "Average epoch loss: 12.483590030670166\n",
      "Average valid loss 17.289884567260742\n",
      "EPOCH 31:\n",
      "Average epoch loss: 12.284198379516601\n",
      "Average valid loss 19.26300811767578\n",
      "EPOCH 32:\n",
      "Average epoch loss: 12.41322946548462\n",
      "Average valid loss 19.01967430114746\n",
      "EPOCH 33:\n",
      "Average epoch loss: 12.078643894195556\n",
      "Average valid loss 21.664819717407227\n",
      "EPOCH 34:\n",
      "Average epoch loss: 12.028244304656983\n",
      "Average valid loss 18.980316162109375\n",
      "EPOCH 35:\n",
      "Average epoch loss: 12.438748455047607\n",
      "Average valid loss 13.838264465332031\n",
      "EPOCH 36:\n",
      "Average epoch loss: 12.35788688659668\n",
      "Average valid loss 18.509416580200195\n",
      "EPOCH 37:\n",
      "Average epoch loss: 11.46997528076172\n",
      "Average valid loss 16.580432891845703\n",
      "EPOCH 38:\n",
      "Average epoch loss: 12.172476959228515\n",
      "Average valid loss 16.452150344848633\n",
      "EPOCH 39:\n",
      "Average epoch loss: 11.783780956268311\n",
      "Average valid loss 15.561617851257324\n",
      "EPOCH 40:\n",
      "Average epoch loss: 11.584435367584229\n",
      "Average valid loss 14.24301528930664\n",
      "EPOCH 41:\n",
      "Average epoch loss: 11.50004014968872\n",
      "Average valid loss 16.252370834350586\n",
      "EPOCH 42:\n",
      "Average epoch loss: 11.27246379852295\n",
      "Average valid loss 18.359899520874023\n",
      "EPOCH 43:\n",
      "Average epoch loss: 11.255714225769044\n",
      "Average valid loss 16.076427459716797\n",
      "EPOCH 44:\n",
      "Average epoch loss: 10.97351007461548\n",
      "Average valid loss 17.122220993041992\n",
      "EPOCH 45:\n",
      "Average epoch loss: 11.614737415313721\n",
      "Average valid loss 17.693866729736328\n",
      "EPOCH 46:\n",
      "Average epoch loss: 11.16882390975952\n",
      "Average valid loss 15.898134231567383\n",
      "EPOCH 47:\n",
      "Average epoch loss: 11.554766941070557\n",
      "Average valid loss 14.128766059875488\n",
      "EPOCH 48:\n",
      "Average epoch loss: 10.929991149902344\n",
      "Average valid loss 15.464393615722656\n",
      "EPOCH 49:\n",
      "Average epoch loss: 11.557365417480469\n",
      "Average valid loss 17.865873336791992\n",
      "EPOCH 50:\n",
      "Average epoch loss: 11.078529834747314\n",
      "Average valid loss 14.780582427978516\n",
      "EPOCH 51:\n",
      "Average epoch loss: 10.836822462081908\n",
      "Average valid loss 20.04245376586914\n",
      "EPOCH 52:\n",
      "Average epoch loss: 11.436681175231934\n",
      "Average valid loss 14.641831398010254\n",
      "EPOCH 53:\n",
      "Average epoch loss: 10.91926817893982\n",
      "Average valid loss 15.566949844360352\n",
      "EPOCH 54:\n",
      "Average epoch loss: 10.901028919219971\n",
      "Average valid loss 19.78042984008789\n",
      "EPOCH 55:\n",
      "Average epoch loss: 10.581400775909424\n",
      "Average valid loss 16.11451530456543\n",
      "EPOCH 56:\n",
      "Average epoch loss: 10.975494289398194\n",
      "Average valid loss 14.415306091308594\n",
      "EPOCH 57:\n",
      "Average epoch loss: 10.671113872528077\n",
      "Average valid loss 11.97092056274414\n",
      "EPOCH 58:\n",
      "Average epoch loss: 10.879501247406006\n",
      "Average valid loss 14.27431583404541\n",
      "EPOCH 59:\n",
      "Average epoch loss: 10.60004539489746\n",
      "Average valid loss 13.534071922302246\n",
      "EPOCH 60:\n",
      "Average epoch loss: 10.44677152633667\n",
      "Average valid loss 17.95850372314453\n",
      "EPOCH 61:\n",
      "Average epoch loss: 10.167064809799195\n",
      "Average valid loss 11.311202049255371\n",
      "EPOCH 62:\n",
      "Average epoch loss: 10.394773197174072\n",
      "Average valid loss 17.26607322692871\n",
      "EPOCH 63:\n",
      "Average epoch loss: 10.240773820877076\n",
      "Average valid loss 15.15986156463623\n",
      "EPOCH 64:\n",
      "Average epoch loss: 10.381822156906129\n",
      "Average valid loss 12.429479598999023\n",
      "EPOCH 65:\n",
      "Average epoch loss: 9.898948049545288\n",
      "Average valid loss 15.276386260986328\n",
      "EPOCH 66:\n",
      "Average epoch loss: 10.363412189483643\n",
      "Average valid loss 13.746296882629395\n",
      "EPOCH 67:\n",
      "Average epoch loss: 10.342073059082031\n",
      "Average valid loss 13.168600082397461\n",
      "EPOCH 68:\n",
      "Average epoch loss: 9.842092609405517\n",
      "Average valid loss 16.768949508666992\n",
      "EPOCH 69:\n",
      "Average epoch loss: 9.62524700164795\n",
      "Average valid loss 13.546804428100586\n",
      "EPOCH 70:\n",
      "Average epoch loss: 9.983937883377076\n",
      "Average valid loss 14.101318359375\n",
      "EPOCH 71:\n",
      "Average epoch loss: 10.320185422897339\n",
      "Average valid loss 16.87152671813965\n",
      "EPOCH 72:\n",
      "Average epoch loss: 9.502392101287843\n",
      "Average valid loss 16.773880004882812\n",
      "EPOCH 73:\n",
      "Average epoch loss: 9.61949963569641\n",
      "Average valid loss 10.254072189331055\n",
      "EPOCH 74:\n",
      "Average epoch loss: 9.804955911636352\n",
      "Average valid loss 10.84814739227295\n",
      "EPOCH 75:\n",
      "Average epoch loss: 9.516901731491089\n",
      "Average valid loss 15.717266082763672\n",
      "EPOCH 76:\n",
      "Average epoch loss: 9.799032974243165\n",
      "Average valid loss 9.980484008789062\n",
      "EPOCH 77:\n",
      "Average epoch loss: 9.367843866348267\n",
      "Average valid loss 16.04448127746582\n",
      "EPOCH 78:\n",
      "Average epoch loss: 9.502779150009156\n",
      "Average valid loss 17.738950729370117\n",
      "EPOCH 79:\n",
      "Average epoch loss: 9.304021406173707\n",
      "Average valid loss 15.428309440612793\n",
      "EPOCH 80:\n",
      "Average epoch loss: 9.863327503204346\n",
      "Average valid loss 15.360099792480469\n",
      "EPOCH 81:\n",
      "Average epoch loss: 9.4356369972229\n",
      "Average valid loss 17.493640899658203\n",
      "EPOCH 82:\n",
      "Average epoch loss: 9.198538827896119\n",
      "Average valid loss 12.431022644042969\n",
      "EPOCH 83:\n",
      "Average epoch loss: 9.525255393981933\n",
      "Average valid loss 14.656867980957031\n",
      "EPOCH 84:\n",
      "Average epoch loss: 9.277587413787842\n",
      "Average valid loss 15.035833358764648\n",
      "EPOCH 85:\n",
      "Average epoch loss: 8.825722074508667\n",
      "Average valid loss 12.261638641357422\n",
      "EPOCH 86:\n",
      "Average epoch loss: 8.560890293121338\n",
      "Average valid loss 15.050796508789062\n",
      "EPOCH 87:\n",
      "Average epoch loss: 9.069940757751464\n",
      "Average valid loss 14.843212127685547\n",
      "EPOCH 88:\n",
      "Average epoch loss: 8.72652063369751\n",
      "Average valid loss 9.916520118713379\n",
      "EPOCH 89:\n",
      "Average epoch loss: 8.707223510742187\n",
      "Average valid loss 16.994760513305664\n",
      "EPOCH 90:\n",
      "Average epoch loss: 8.577794742584228\n",
      "Average valid loss 13.997562408447266\n",
      "EPOCH 91:\n",
      "Average epoch loss: 8.599981546401978\n",
      "Average valid loss 13.693075180053711\n",
      "EPOCH 92:\n",
      "Average epoch loss: 8.553101921081543\n",
      "Average valid loss 8.338672637939453\n",
      "EPOCH 93:\n",
      "Average epoch loss: 8.824965858459473\n",
      "Average valid loss 11.84368896484375\n",
      "EPOCH 94:\n",
      "Average epoch loss: 8.7377112865448\n",
      "Average valid loss 12.951618194580078\n",
      "EPOCH 95:\n",
      "Average epoch loss: 8.172999620437622\n",
      "Average valid loss 16.056304931640625\n",
      "EPOCH 96:\n",
      "Average epoch loss: 8.34044861793518\n",
      "Average valid loss 9.840083122253418\n",
      "EPOCH 97:\n",
      "Average epoch loss: 7.930649662017823\n",
      "Average valid loss 12.475336074829102\n",
      "EPOCH 98:\n",
      "Average epoch loss: 8.315434885025024\n",
      "Average valid loss 15.74462890625\n",
      "EPOCH 99:\n",
      "Average epoch loss: 8.098317766189576\n",
      "Average valid loss 16.24725914001465\n",
      "EPOCH 100:\n",
      "Average epoch loss: 7.9777978420257565\n",
      "Average valid loss 10.96768569946289\n",
      "EPOCH 101:\n",
      "Average epoch loss: 8.391145706176758\n",
      "Average valid loss 11.42110824584961\n",
      "EPOCH 102:\n",
      "Average epoch loss: 7.8261542320251465\n",
      "Average valid loss 15.963279724121094\n",
      "EPOCH 103:\n",
      "Average epoch loss: 7.854223680496216\n",
      "Average valid loss 11.903203964233398\n",
      "EPOCH 104:\n",
      "Average epoch loss: 8.051923751831055\n",
      "Average valid loss 11.839242935180664\n",
      "EPOCH 105:\n",
      "Average epoch loss: 7.648521184921265\n",
      "Average valid loss 8.836921691894531\n",
      "EPOCH 106:\n",
      "Average epoch loss: 7.838775205612182\n",
      "Average valid loss 13.376733779907227\n",
      "EPOCH 107:\n",
      "Average epoch loss: 7.856493902206421\n",
      "Average valid loss 15.566064834594727\n",
      "EPOCH 108:\n",
      "Average epoch loss: 7.629768562316895\n",
      "Average valid loss 13.228973388671875\n",
      "EPOCH 109:\n",
      "Average epoch loss: 7.7097279071807865\n",
      "Average valid loss 12.67669677734375\n",
      "EPOCH 110:\n",
      "Average epoch loss: 7.450741720199585\n",
      "Average valid loss 11.408143043518066\n",
      "EPOCH 111:\n",
      "Average epoch loss: 7.62093186378479\n",
      "Average valid loss 6.505259990692139\n",
      "EPOCH 112:\n",
      "Average epoch loss: 7.167128801345825\n",
      "Average valid loss 11.40577507019043\n",
      "EPOCH 113:\n",
      "Average epoch loss: 7.152921342849732\n",
      "Average valid loss 11.509130477905273\n",
      "EPOCH 114:\n",
      "Average epoch loss: 7.173534250259399\n",
      "Average valid loss 12.573188781738281\n",
      "EPOCH 115:\n",
      "Average epoch loss: 6.932653093338013\n",
      "Average valid loss 11.553549766540527\n",
      "EPOCH 116:\n",
      "Average epoch loss: 7.030699014663696\n",
      "Average valid loss 11.618518829345703\n",
      "EPOCH 117:\n",
      "Average epoch loss: 6.948940229415894\n",
      "Average valid loss 11.082602500915527\n",
      "EPOCH 118:\n",
      "Average epoch loss: 7.030288219451904\n",
      "Average valid loss 10.347883224487305\n",
      "EPOCH 119:\n",
      "Average epoch loss: 6.750614166259766\n",
      "Average valid loss 10.778343200683594\n",
      "EPOCH 120:\n",
      "Average epoch loss: 7.125996208190918\n",
      "Average valid loss 14.323198318481445\n",
      "EPOCH 121:\n",
      "Average epoch loss: 7.082237625122071\n",
      "Average valid loss 14.254956245422363\n",
      "EPOCH 122:\n",
      "Average epoch loss: 6.657922601699829\n",
      "Average valid loss 10.503414154052734\n",
      "EPOCH 123:\n",
      "Average epoch loss: 6.624230766296387\n",
      "Average valid loss 10.782194137573242\n",
      "EPOCH 124:\n",
      "Average epoch loss: 6.6395289421081545\n",
      "Average valid loss 7.719476699829102\n",
      "EPOCH 125:\n",
      "Average epoch loss: 6.787750339508056\n",
      "Average valid loss 10.08779239654541\n",
      "EPOCH 126:\n",
      "Average epoch loss: 6.551040124893189\n",
      "Average valid loss 9.198760032653809\n",
      "EPOCH 127:\n",
      "Average epoch loss: 6.43092713356018\n",
      "Average valid loss 9.030109405517578\n",
      "EPOCH 128:\n",
      "Average epoch loss: 6.392468976974487\n",
      "Average valid loss 11.68796443939209\n",
      "EPOCH 129:\n",
      "Average epoch loss: 6.516766977310181\n",
      "Average valid loss 10.058145523071289\n",
      "EPOCH 130:\n",
      "Average epoch loss: 6.290947341918946\n",
      "Average valid loss 13.362921714782715\n",
      "EPOCH 131:\n",
      "Average epoch loss: 6.279932975769043\n",
      "Average valid loss 9.88956069946289\n",
      "EPOCH 132:\n",
      "Average epoch loss: 6.051893854141236\n",
      "Average valid loss 13.17742919921875\n",
      "EPOCH 133:\n",
      "Average epoch loss: 5.998441123962403\n",
      "Average valid loss 10.835960388183594\n",
      "EPOCH 134:\n",
      "Average epoch loss: 5.943754100799561\n",
      "Average valid loss 4.576972484588623\n",
      "EPOCH 135:\n",
      "Average epoch loss: 5.951744174957275\n",
      "Average valid loss 8.269124984741211\n",
      "EPOCH 136:\n",
      "Average epoch loss: 5.915047359466553\n",
      "Average valid loss 9.425037384033203\n",
      "EPOCH 137:\n",
      "Average epoch loss: 6.045836019515991\n",
      "Average valid loss 7.830631256103516\n",
      "EPOCH 138:\n",
      "Average epoch loss: 6.233809185028076\n",
      "Average valid loss 9.398937225341797\n",
      "EPOCH 139:\n",
      "Average epoch loss: 5.959050846099854\n",
      "Average valid loss 4.2176008224487305\n",
      "EPOCH 140:\n",
      "Average epoch loss: 5.902110481262207\n",
      "Average valid loss 9.856039047241211\n",
      "EPOCH 141:\n",
      "Average epoch loss: 5.8766889572143555\n",
      "Average valid loss 7.811219215393066\n",
      "EPOCH 142:\n",
      "Average epoch loss: 5.644473028182984\n",
      "Average valid loss 6.481809616088867\n",
      "EPOCH 143:\n",
      "Average epoch loss: 5.492672657966613\n",
      "Average valid loss 9.691434860229492\n",
      "EPOCH 144:\n",
      "Average epoch loss: 5.502514934539795\n",
      "Average valid loss 8.634245872497559\n",
      "EPOCH 145:\n",
      "Average epoch loss: 5.425362896919251\n",
      "Average valid loss 10.405623435974121\n",
      "EPOCH 146:\n",
      "Average epoch loss: 5.425367307662964\n",
      "Average valid loss 8.446822166442871\n",
      "EPOCH 147:\n",
      "Average epoch loss: 5.389966988563538\n",
      "Average valid loss 9.430253028869629\n",
      "EPOCH 148:\n",
      "Average epoch loss: 5.207738208770752\n",
      "Average valid loss 11.625715255737305\n",
      "EPOCH 149:\n",
      "Average epoch loss: 5.408934879302978\n",
      "Average valid loss 3.577073335647583\n",
      "EPOCH 150:\n",
      "Average epoch loss: 5.460256791114807\n",
      "Average valid loss 3.5333974361419678\n",
      "EPOCH 151:\n",
      "Average epoch loss: 5.144228434562683\n",
      "Average valid loss 9.226042747497559\n",
      "EPOCH 152:\n",
      "Average epoch loss: 5.096163320541382\n",
      "Average valid loss 9.136345863342285\n",
      "EPOCH 153:\n",
      "Average epoch loss: 5.276193046569825\n",
      "Average valid loss 8.881385803222656\n",
      "EPOCH 154:\n",
      "Average epoch loss: 5.142847347259521\n",
      "Average valid loss 9.828841209411621\n",
      "EPOCH 155:\n",
      "Average epoch loss: 4.960391616821289\n",
      "Average valid loss 9.805173873901367\n",
      "EPOCH 156:\n",
      "Average epoch loss: 4.961760258674621\n",
      "Average valid loss 6.0638837814331055\n",
      "EPOCH 157:\n",
      "Average epoch loss: 5.170554685592651\n",
      "Average valid loss 8.84835433959961\n",
      "EPOCH 158:\n",
      "Average epoch loss: 4.9625403642654415\n",
      "Average valid loss 5.494606018066406\n",
      "EPOCH 159:\n",
      "Average epoch loss: 4.910771989822388\n",
      "Average valid loss 10.757622718811035\n",
      "EPOCH 160:\n",
      "Average epoch loss: 4.909118485450745\n",
      "Average valid loss 8.217056274414062\n",
      "EPOCH 161:\n",
      "Average epoch loss: 4.904959726333618\n",
      "Average valid loss 7.9295854568481445\n",
      "EPOCH 162:\n",
      "Average epoch loss: 4.651528429985047\n",
      "Average valid loss 8.888378143310547\n",
      "EPOCH 163:\n",
      "Average epoch loss: 4.636432933807373\n",
      "Average valid loss 3.05035138130188\n",
      "EPOCH 164:\n",
      "Average epoch loss: 4.591989016532898\n",
      "Average valid loss 5.075675010681152\n",
      "EPOCH 165:\n",
      "Average epoch loss: 4.728827452659607\n",
      "Average valid loss 7.63471794128418\n",
      "EPOCH 166:\n",
      "Average epoch loss: 4.483256077766418\n",
      "Average valid loss 6.654644966125488\n",
      "EPOCH 167:\n",
      "Average epoch loss: 4.628699278831482\n",
      "Average valid loss 7.459495544433594\n",
      "EPOCH 168:\n",
      "Average epoch loss: 4.47557303905487\n",
      "Average valid loss 8.948721885681152\n",
      "EPOCH 169:\n",
      "Average epoch loss: 4.407152056694031\n",
      "Average valid loss 7.4661455154418945\n",
      "EPOCH 170:\n",
      "Average epoch loss: 4.650771641731263\n",
      "Average valid loss 7.619081020355225\n",
      "EPOCH 171:\n",
      "Average epoch loss: 4.476552033424378\n",
      "Average valid loss 2.743385076522827\n",
      "EPOCH 172:\n",
      "Average epoch loss: 4.405990123748779\n",
      "Average valid loss 7.472439765930176\n",
      "EPOCH 173:\n",
      "Average epoch loss: 4.356992316246033\n",
      "Average valid loss 7.379508972167969\n",
      "EPOCH 174:\n",
      "Average epoch loss: 4.161486434936523\n",
      "Average valid loss 9.100311279296875\n",
      "EPOCH 175:\n",
      "Average epoch loss: 4.137981367111206\n",
      "Average valid loss 7.553710460662842\n",
      "EPOCH 176:\n",
      "Average epoch loss: 4.013159012794494\n",
      "Average valid loss 9.058061599731445\n",
      "EPOCH 177:\n",
      "Average epoch loss: 4.21409752368927\n",
      "Average valid loss 4.794834136962891\n",
      "EPOCH 178:\n",
      "Average epoch loss: 4.13614935874939\n",
      "Average valid loss 6.116146087646484\n",
      "EPOCH 179:\n",
      "Average epoch loss: 4.052762031555176\n",
      "Average valid loss 8.12912368774414\n",
      "EPOCH 180:\n",
      "Average epoch loss: 4.136456632614136\n",
      "Average valid loss 4.662210464477539\n",
      "EPOCH 181:\n",
      "Average epoch loss: 3.8247912406921385\n",
      "Average valid loss 5.457116603851318\n",
      "EPOCH 182:\n",
      "Average epoch loss: 3.913312387466431\n",
      "Average valid loss 3.046988010406494\n",
      "EPOCH 183:\n",
      "Average epoch loss: 3.837666368484497\n",
      "Average valid loss 8.127284049987793\n",
      "EPOCH 184:\n",
      "Average epoch loss: 3.8269230842590334\n",
      "Average valid loss 5.217645645141602\n",
      "EPOCH 185:\n",
      "Average epoch loss: 3.8502775907516478\n",
      "Average valid loss 7.578041076660156\n",
      "EPOCH 186:\n",
      "Average epoch loss: 3.8103786945343017\n",
      "Average valid loss 7.867789268493652\n",
      "EPOCH 187:\n",
      "Average epoch loss: 3.9008453130722045\n",
      "Average valid loss 6.351058006286621\n",
      "EPOCH 188:\n",
      "Average epoch loss: 3.783364725112915\n",
      "Average valid loss 4.981255531311035\n",
      "EPOCH 189:\n",
      "Average epoch loss: 3.6017374753952027\n",
      "Average valid loss 4.446674346923828\n",
      "EPOCH 190:\n",
      "Average epoch loss: 3.586160588264465\n",
      "Average valid loss 8.457279205322266\n",
      "EPOCH 191:\n",
      "Average epoch loss: 3.4834192514419557\n",
      "Average valid loss 2.175588607788086\n",
      "EPOCH 192:\n",
      "Average epoch loss: 3.581314754486084\n",
      "Average valid loss 4.651756286621094\n",
      "EPOCH 193:\n",
      "Average epoch loss: 3.482235288619995\n",
      "Average valid loss 3.8084330558776855\n",
      "EPOCH 194:\n",
      "Average epoch loss: 3.446380591392517\n",
      "Average valid loss 8.285652160644531\n",
      "EPOCH 195:\n",
      "Average epoch loss: 3.3714488983154296\n",
      "Average valid loss 3.6708178520202637\n",
      "EPOCH 196:\n",
      "Average epoch loss: 3.3051491498947145\n",
      "Average valid loss 5.737525939941406\n",
      "EPOCH 197:\n",
      "Average epoch loss: 3.468798017501831\n",
      "Average valid loss 5.723203182220459\n",
      "EPOCH 198:\n",
      "Average epoch loss: 3.1830262184143066\n",
      "Average valid loss 3.4323043823242188\n",
      "EPOCH 199:\n",
      "Average epoch loss: 3.2402720212936402\n",
      "Average valid loss 4.226133823394775\n",
      "EPOCH 200:\n",
      "Average epoch loss: 3.2588994026184084\n",
      "Average valid loss 5.519465446472168\n",
      "EPOCH 201:\n",
      "Average epoch loss: 3.1606358766555784\n",
      "Average valid loss 5.509217262268066\n",
      "EPOCH 202:\n",
      "Average epoch loss: 3.2671650886535644\n",
      "Average valid loss 5.5562520027160645\n",
      "EPOCH 203:\n",
      "Average epoch loss: 3.249363732337952\n",
      "Average valid loss 3.1496052742004395\n",
      "EPOCH 204:\n",
      "Average epoch loss: 3.1863107919692992\n",
      "Average valid loss 5.92813777923584\n",
      "EPOCH 205:\n",
      "Average epoch loss: 3.037150526046753\n",
      "Average valid loss 5.2170915603637695\n",
      "EPOCH 206:\n",
      "Average epoch loss: 3.0706784605979918\n",
      "Average valid loss 5.175121784210205\n",
      "EPOCH 207:\n",
      "Average epoch loss: 3.101900315284729\n",
      "Average valid loss 4.525948524475098\n",
      "EPOCH 208:\n",
      "Average epoch loss: 2.913363039493561\n",
      "Average valid loss 5.908750534057617\n",
      "EPOCH 209:\n",
      "Average epoch loss: 2.873624324798584\n",
      "Average valid loss 5.537430763244629\n",
      "EPOCH 210:\n",
      "Average epoch loss: 2.958285355567932\n",
      "Average valid loss 1.7145190238952637\n",
      "EPOCH 211:\n",
      "Average epoch loss: 2.802162766456604\n",
      "Average valid loss 5.494937896728516\n",
      "EPOCH 212:\n",
      "Average epoch loss: 2.8670177459716797\n",
      "Average valid loss 4.918164253234863\n",
      "EPOCH 213:\n",
      "Average epoch loss: 2.8030446052551268\n",
      "Average valid loss 1.915579915046692\n",
      "EPOCH 214:\n",
      "Average epoch loss: 2.916939401626587\n",
      "Average valid loss 2.2815804481506348\n",
      "EPOCH 215:\n",
      "Average epoch loss: 2.6538424611091616\n",
      "Average valid loss 4.579349994659424\n",
      "EPOCH 216:\n",
      "Average epoch loss: 2.556699752807617\n",
      "Average valid loss 4.681700706481934\n",
      "EPOCH 217:\n",
      "Average epoch loss: 2.610679042339325\n",
      "Average valid loss 4.508067607879639\n",
      "EPOCH 218:\n",
      "Average epoch loss: 2.6489155292510986\n",
      "Average valid loss 4.558656215667725\n",
      "EPOCH 219:\n",
      "Average epoch loss: 2.584921419620514\n",
      "Average valid loss 2.0936503410339355\n",
      "EPOCH 220:\n",
      "Average epoch loss: 2.4539188146591187\n",
      "Average valid loss 3.6471657752990723\n",
      "EPOCH 221:\n",
      "Average epoch loss: 2.6156570076942445\n",
      "Average valid loss 4.873089790344238\n",
      "EPOCH 222:\n",
      "Average epoch loss: 2.4765957951545716\n",
      "Average valid loss 3.5383524894714355\n",
      "EPOCH 223:\n",
      "Average epoch loss: 2.4436852097511292\n",
      "Average valid loss 3.8910164833068848\n",
      "EPOCH 224:\n",
      "Average epoch loss: 2.352119433879852\n",
      "Average valid loss 4.280623435974121\n",
      "EPOCH 225:\n",
      "Average epoch loss: 2.42171688079834\n",
      "Average valid loss 4.7379255294799805\n",
      "EPOCH 226:\n",
      "Average epoch loss: 2.2813915610313416\n",
      "Average valid loss 2.0780279636383057\n",
      "EPOCH 227:\n",
      "Average epoch loss: 2.32561000585556\n",
      "Average valid loss 2.089529514312744\n",
      "EPOCH 228:\n",
      "Average epoch loss: 2.2968925714492796\n",
      "Average valid loss 3.955324172973633\n",
      "EPOCH 229:\n",
      "Average epoch loss: 2.3302016735076903\n",
      "Average valid loss 1.3418083190917969\n",
      "EPOCH 230:\n",
      "Average epoch loss: 2.1004818081855774\n",
      "Average valid loss 1.3239033222198486\n",
      "EPOCH 231:\n",
      "Average epoch loss: 2.2209758877754213\n",
      "Average valid loss 3.855480670928955\n",
      "EPOCH 232:\n",
      "Average epoch loss: 2.1888223767280577\n",
      "Average valid loss 3.0739121437072754\n",
      "EPOCH 233:\n",
      "Average epoch loss: 2.2519834876060485\n",
      "Average valid loss 3.7734665870666504\n",
      "EPOCH 234:\n",
      "Average epoch loss: 2.0492859959602354\n",
      "Average valid loss 3.932382106781006\n",
      "EPOCH 235:\n",
      "Average epoch loss: 2.0442361354827883\n",
      "Average valid loss 3.608311653137207\n",
      "EPOCH 236:\n",
      "Average epoch loss: 2.145044219493866\n",
      "Average valid loss 2.653533458709717\n",
      "EPOCH 237:\n",
      "Average epoch loss: 2.034468388557434\n",
      "Average valid loss 3.5852394104003906\n",
      "EPOCH 238:\n",
      "Average epoch loss: 2.0033016324043276\n",
      "Average valid loss 3.482581615447998\n",
      "EPOCH 239:\n",
      "Average epoch loss: 1.98049556016922\n",
      "Average valid loss 3.663815498352051\n",
      "EPOCH 240:\n",
      "Average epoch loss: 1.937446618080139\n",
      "Average valid loss 2.2308690547943115\n",
      "EPOCH 241:\n",
      "Average epoch loss: 1.8569121718406678\n",
      "Average valid loss 3.340114116668701\n",
      "EPOCH 242:\n",
      "Average epoch loss: 2.000145173072815\n",
      "Average valid loss 4.027856826782227\n",
      "EPOCH 243:\n",
      "Average epoch loss: 1.9068965852260589\n",
      "Average valid loss 3.9762558937072754\n",
      "EPOCH 244:\n",
      "Average epoch loss: 1.8293357372283936\n",
      "Average valid loss 3.207130193710327\n",
      "EPOCH 245:\n",
      "Average epoch loss: 1.8160657644271851\n",
      "Average valid loss 1.2286325693130493\n",
      "EPOCH 246:\n",
      "Average epoch loss: 1.7142051339149476\n",
      "Average valid loss 3.0523841381073\n",
      "EPOCH 247:\n",
      "Average epoch loss: 1.8771021366119385\n",
      "Average valid loss 3.1656999588012695\n",
      "EPOCH 248:\n",
      "Average epoch loss: 1.6357541501522064\n",
      "Average valid loss 3.5634536743164062\n",
      "EPOCH 249:\n",
      "Average epoch loss: 1.664314728975296\n",
      "Average valid loss 3.530773401260376\n",
      "EPOCH 250:\n",
      "Average epoch loss: 1.6192199110984802\n",
      "Average valid loss 2.8797736167907715\n",
      "EPOCH 251:\n",
      "Average epoch loss: 1.6401303708553314\n",
      "Average valid loss 1.1373146772384644\n",
      "EPOCH 252:\n",
      "Average epoch loss: 1.6547625184059143\n",
      "Average valid loss 1.6156384944915771\n",
      "EPOCH 253:\n",
      "Average epoch loss: 1.5814559578895568\n",
      "Average valid loss 2.75099515914917\n",
      "EPOCH 254:\n",
      "Average epoch loss: 1.59237402677536\n",
      "Average valid loss 3.605043649673462\n",
      "EPOCH 255:\n",
      "Average epoch loss: 1.5677990674972535\n",
      "Average valid loss 5.84741735458374\n",
      "EPOCH 256:\n",
      "Average epoch loss: 1.4856196403503419\n",
      "Average valid loss 2.61319637298584\n",
      "EPOCH 257:\n",
      "Average epoch loss: 1.5047220945358277\n",
      "Average valid loss 1.9750409126281738\n",
      "EPOCH 258:\n",
      "Average epoch loss: 1.6060552597045898\n",
      "Average valid loss 0.9574893712997437\n",
      "EPOCH 259:\n",
      "Average epoch loss: 1.4877915620803832\n",
      "Average valid loss 1.4013800621032715\n",
      "EPOCH 260:\n",
      "Average epoch loss: 1.4445810317993164\n",
      "Average valid loss 2.6556975841522217\n",
      "EPOCH 261:\n",
      "Average epoch loss: 1.410443162918091\n",
      "Average valid loss 5.615875244140625\n",
      "EPOCH 262:\n",
      "Average epoch loss: 1.3805230736732483\n",
      "Average valid loss 0.8898931741714478\n",
      "EPOCH 263:\n",
      "Average epoch loss: 1.409130597114563\n",
      "Average valid loss 1.749314308166504\n",
      "EPOCH 264:\n",
      "Average epoch loss: 1.3106926441192628\n",
      "Average valid loss 0.926810085773468\n",
      "EPOCH 265:\n",
      "Average epoch loss: 1.3401199102401733\n",
      "Average valid loss 0.9281899929046631\n",
      "EPOCH 266:\n",
      "Average epoch loss: 1.34568549990654\n",
      "Average valid loss 2.05873441696167\n",
      "EPOCH 267:\n",
      "Average epoch loss: 1.3094986140727998\n",
      "Average valid loss 3.239546060562134\n",
      "EPOCH 268:\n",
      "Average epoch loss: 1.2747724175453186\n",
      "Average valid loss 0.8981468677520752\n",
      "EPOCH 269:\n",
      "Average epoch loss: 1.2821258068084718\n",
      "Average valid loss 1.0823465585708618\n",
      "EPOCH 270:\n",
      "Average epoch loss: 1.30813809633255\n",
      "Average valid loss 1.0746996402740479\n",
      "EPOCH 271:\n",
      "Average epoch loss: 1.2657562494277954\n",
      "Average valid loss 1.8587431907653809\n",
      "EPOCH 272:\n",
      "Average epoch loss: 1.2330455839633943\n",
      "Average valid loss 2.3896970748901367\n",
      "EPOCH 273:\n",
      "Average epoch loss: 1.237308669090271\n",
      "Average valid loss 2.299325704574585\n",
      "EPOCH 274:\n",
      "Average epoch loss: 1.3001248002052308\n",
      "Average valid loss 0.7970201969146729\n",
      "EPOCH 275:\n",
      "Average epoch loss: 1.2358006477355956\n",
      "Average valid loss 2.971287250518799\n",
      "EPOCH 276:\n",
      "Average epoch loss: 1.1392800718545915\n",
      "Average valid loss 2.9260172843933105\n",
      "EPOCH 277:\n",
      "Average epoch loss: 1.1180729568004608\n",
      "Average valid loss 0.6854507327079773\n",
      "EPOCH 278:\n",
      "Average epoch loss: 1.089979988336563\n",
      "Average valid loss 1.612502932548523\n",
      "EPOCH 279:\n",
      "Average epoch loss: 1.1597283840179444\n",
      "Average valid loss 2.0016989707946777\n",
      "EPOCH 280:\n",
      "Average epoch loss: 1.1371282458305358\n",
      "Average valid loss 2.0011558532714844\n",
      "EPOCH 281:\n",
      "Average epoch loss: 1.1008373975753785\n",
      "Average valid loss 0.7034870982170105\n",
      "EPOCH 282:\n",
      "Average epoch loss: 1.0818744838237762\n",
      "Average valid loss 0.6653429269790649\n",
      "EPOCH 283:\n",
      "Average epoch loss: 1.0852932572364806\n",
      "Average valid loss 0.6529772281646729\n",
      "EPOCH 284:\n",
      "Average epoch loss: 1.0700457751750947\n",
      "Average valid loss 1.840214729309082\n",
      "EPOCH 285:\n",
      "Average epoch loss: 1.0558826565742492\n",
      "Average valid loss 0.6056386232376099\n",
      "EPOCH 286:\n",
      "Average epoch loss: 1.0551998138427734\n",
      "Average valid loss 2.0093302726745605\n",
      "EPOCH 287:\n",
      "Average epoch loss: 1.0596986889839173\n",
      "Average valid loss 0.6098238825798035\n",
      "EPOCH 288:\n",
      "Average epoch loss: 0.9758067965507508\n",
      "Average valid loss 1.4182583093643188\n",
      "EPOCH 289:\n",
      "Average epoch loss: 0.893671053647995\n",
      "Average valid loss 1.6850987672805786\n",
      "EPOCH 290:\n",
      "Average epoch loss: 0.9943468809127808\n",
      "Average valid loss 0.5595136284828186\n",
      "EPOCH 291:\n",
      "Average epoch loss: 0.9317108631134033\n",
      "Average valid loss 0.543022871017456\n",
      "EPOCH 292:\n",
      "Average epoch loss: 0.9278584569692612\n",
      "Average valid loss 1.5568856000900269\n",
      "EPOCH 293:\n",
      "Average epoch loss: 0.9156666278839112\n",
      "Average valid loss 0.529457688331604\n",
      "EPOCH 294:\n",
      "Average epoch loss: 0.86297307908535\n",
      "Average valid loss 0.5160753726959229\n",
      "EPOCH 295:\n",
      "Average epoch loss: 0.9753881812095642\n",
      "Average valid loss 2.5158185958862305\n",
      "EPOCH 296:\n",
      "Average epoch loss: 0.8998304545879364\n",
      "Average valid loss 1.135547399520874\n",
      "EPOCH 297:\n",
      "Average epoch loss: 0.8514477252960205\n",
      "Average valid loss 0.8117060661315918\n",
      "EPOCH 298:\n",
      "Average epoch loss: 0.8797201931476593\n",
      "Average valid loss 4.268191337585449\n",
      "EPOCH 299:\n",
      "Average epoch loss: 0.8074881196022033\n",
      "Average valid loss 1.5750126838684082\n",
      "EPOCH 300:\n",
      "Average epoch loss: 0.8033691316843032\n",
      "Average valid loss 0.7548093795776367\n",
      "EPOCH 301:\n",
      "Average epoch loss: 0.7967222809791565\n",
      "Average valid loss 0.4848730266094208\n",
      "EPOCH 302:\n",
      "Average epoch loss: 0.7781407207250595\n",
      "Average valid loss 4.096960544586182\n",
      "EPOCH 303:\n",
      "Average epoch loss: 0.7661335706710816\n",
      "Average valid loss 0.47615477442741394\n",
      "EPOCH 304:\n",
      "Average epoch loss: 0.734253215789795\n",
      "Average valid loss 0.45256656408309937\n",
      "EPOCH 305:\n",
      "Average epoch loss: 0.7787184745073319\n",
      "Average valid loss 2.293488025665283\n",
      "EPOCH 306:\n",
      "Average epoch loss: 0.7725953608751297\n",
      "Average valid loss 0.5390781760215759\n",
      "EPOCH 307:\n",
      "Average epoch loss: 0.740281555056572\n",
      "Average valid loss 0.4311537742614746\n",
      "EPOCH 308:\n",
      "Average epoch loss: 0.7751029402017593\n",
      "Average valid loss 1.0948338508605957\n",
      "EPOCH 309:\n",
      "Average epoch loss: 0.667638772726059\n",
      "Average valid loss 0.42607563734054565\n",
      "EPOCH 310:\n",
      "Average epoch loss: 0.6757113099098205\n",
      "Average valid loss 2.215665817260742\n",
      "EPOCH 311:\n",
      "Average epoch loss: 0.7938455939292908\n",
      "Average valid loss 2.18007755279541\n",
      "EPOCH 312:\n",
      "Average epoch loss: 0.7302489131689072\n",
      "Average valid loss 3.855072498321533\n",
      "EPOCH 313:\n",
      "Average epoch loss: 0.6482392385601997\n",
      "Average valid loss 0.3975113034248352\n",
      "EPOCH 314:\n",
      "Average epoch loss: 0.727671954035759\n",
      "Average valid loss 3.7767980098724365\n",
      "EPOCH 315:\n",
      "Average epoch loss: 0.6855591028928757\n",
      "Average valid loss 0.3871656060218811\n",
      "EPOCH 316:\n",
      "Average epoch loss: 0.657226511836052\n",
      "Average valid loss 0.6976453065872192\n",
      "EPOCH 317:\n",
      "Average epoch loss: 0.6713939070701599\n",
      "Average valid loss 0.38389259576797485\n",
      "EPOCH 318:\n",
      "Average epoch loss: 0.6202832132577896\n",
      "Average valid loss 0.5521455407142639\n",
      "EPOCH 319:\n",
      "Average epoch loss: 0.5810726225376129\n",
      "Average valid loss 1.887399435043335\n",
      "EPOCH 320:\n",
      "Average epoch loss: 0.6974332690238952\n",
      "Average valid loss 0.8384624123573303\n",
      "EPOCH 321:\n",
      "Average epoch loss: 0.6834975093603134\n",
      "Average valid loss 0.45851507782936096\n",
      "EPOCH 322:\n",
      "Average epoch loss: 0.6346201062202453\n",
      "Average valid loss 0.3623155355453491\n",
      "EPOCH 323:\n",
      "Average epoch loss: 0.609310182929039\n",
      "Average valid loss 0.36967670917510986\n",
      "EPOCH 324:\n",
      "Average epoch loss: 0.597911411523819\n",
      "Average valid loss 0.5477455854415894\n",
      "EPOCH 325:\n",
      "Average epoch loss: 0.5181359976530076\n",
      "Average valid loss 1.7124589681625366\n",
      "EPOCH 326:\n",
      "Average epoch loss: 0.539118766784668\n",
      "Average valid loss 0.3513936996459961\n",
      "EPOCH 327:\n",
      "Average epoch loss: 0.582604905962944\n",
      "Average valid loss 1.2900197505950928\n",
      "EPOCH 328:\n",
      "Average epoch loss: 0.5174273788928986\n",
      "Average valid loss 0.32906582951545715\n",
      "EPOCH 329:\n",
      "Average epoch loss: 0.5610903099179267\n",
      "Average valid loss 1.6683037281036377\n",
      "EPOCH 330:\n",
      "Average epoch loss: 0.5362988963723183\n",
      "Average valid loss 3.337698221206665\n",
      "EPOCH 331:\n",
      "Average epoch loss: 0.5635332703590393\n",
      "Average valid loss 0.32129210233688354\n",
      "EPOCH 332:\n",
      "Average epoch loss: 0.4940154582262039\n",
      "Average valid loss 0.31393077969551086\n",
      "EPOCH 333:\n",
      "Average epoch loss: 0.4933424413204193\n",
      "Average valid loss 0.40226349234580994\n",
      "EPOCH 334:\n",
      "Average epoch loss: 0.49783357679843904\n",
      "Average valid loss 1.128316879272461\n",
      "EPOCH 335:\n",
      "Average epoch loss: 0.5459197998046875\n",
      "Average valid loss 0.31048470735549927\n",
      "EPOCH 336:\n",
      "Average epoch loss: 0.4884324610233307\n",
      "Average valid loss 0.30349820852279663\n",
      "EPOCH 337:\n",
      "Average epoch loss: 0.5270309060811996\n",
      "Average valid loss 0.2988836467266083\n",
      "EPOCH 338:\n",
      "Average epoch loss: 0.487721386551857\n",
      "Average valid loss 0.9470239281654358\n",
      "EPOCH 339:\n",
      "Average epoch loss: 0.46103357076644896\n",
      "Average valid loss 1.479777216911316\n",
      "EPOCH 340:\n",
      "Average epoch loss: 0.49118736386299133\n",
      "Average valid loss 0.2891353368759155\n",
      "EPOCH 341:\n",
      "Average epoch loss: 0.4913016200065613\n",
      "Average valid loss 0.28328683972358704\n",
      "EPOCH 342:\n",
      "Average epoch loss: 0.4450260728597641\n",
      "Average valid loss 0.28263863921165466\n",
      "EPOCH 343:\n",
      "Average epoch loss: 0.44064671993255616\n",
      "Average valid loss 0.28077825903892517\n",
      "EPOCH 344:\n",
      "Average epoch loss: 0.4379684716463089\n",
      "Average valid loss 0.27520227432250977\n",
      "EPOCH 345:\n",
      "Average epoch loss: 0.455151329934597\n",
      "Average valid loss 0.27184444665908813\n",
      "EPOCH 346:\n",
      "Average epoch loss: 0.38501404970884323\n",
      "Average valid loss 1.7400790452957153\n",
      "EPOCH 347:\n",
      "Average epoch loss: 0.41350287944078445\n",
      "Average valid loss 0.26387301087379456\n",
      "EPOCH 348:\n",
      "Average epoch loss: 0.4673126548528671\n",
      "Average valid loss 0.8945815563201904\n",
      "EPOCH 349:\n",
      "Average epoch loss: 0.3801914736628532\n",
      "Average valid loss 0.27608075737953186\n",
      "EPOCH 350:\n",
      "Average epoch loss: 0.448683363199234\n",
      "Average valid loss 0.2593088150024414\n",
      "EPOCH 351:\n",
      "Average epoch loss: 0.4278410837054253\n",
      "Average valid loss 1.6762334108352661\n",
      "EPOCH 352:\n",
      "Average epoch loss: 0.41487922668457033\n",
      "Average valid loss 0.2600535750389099\n",
      "EPOCH 353:\n",
      "Average epoch loss: 0.382354111969471\n",
      "Average valid loss 0.24644993245601654\n",
      "EPOCH 354:\n",
      "Average epoch loss: 0.3565897047519684\n",
      "Average valid loss 0.3222300708293915\n",
      "EPOCH 355:\n",
      "Average epoch loss: 0.41209532618522643\n",
      "Average valid loss 0.24459147453308105\n",
      "EPOCH 356:\n",
      "Average epoch loss: 0.38424114882946014\n",
      "Average valid loss 1.6840591430664062\n",
      "EPOCH 357:\n",
      "Average epoch loss: 0.405724535882473\n",
      "Average valid loss 0.3719218373298645\n",
      "EPOCH 358:\n",
      "Average epoch loss: 0.3570720136165619\n",
      "Average valid loss 0.23888233304023743\n",
      "EPOCH 359:\n",
      "Average epoch loss: 0.359342585504055\n",
      "Average valid loss 0.23052392899990082\n",
      "EPOCH 360:\n",
      "Average epoch loss: 0.3810606122016907\n",
      "Average valid loss 0.2260415256023407\n",
      "EPOCH 361:\n",
      "Average epoch loss: 0.3353294588625431\n",
      "Average valid loss 0.22633977234363556\n",
      "EPOCH 362:\n",
      "Average epoch loss: 0.35364075005054474\n",
      "Average valid loss 0.2194410264492035\n",
      "EPOCH 363:\n",
      "Average epoch loss: 0.31899669766426086\n",
      "Average valid loss 0.30970144271850586\n",
      "EPOCH 364:\n",
      "Average epoch loss: 0.3113381639122963\n",
      "Average valid loss 0.2361346036195755\n",
      "EPOCH 365:\n",
      "Average epoch loss: 0.30955263376235964\n",
      "Average valid loss 0.21749335527420044\n",
      "EPOCH 366:\n",
      "Average epoch loss: 0.36447418928146363\n",
      "Average valid loss 0.21265283226966858\n",
      "EPOCH 367:\n",
      "Average epoch loss: 0.33189681470394133\n",
      "Average valid loss 0.21197368204593658\n",
      "EPOCH 368:\n",
      "Average epoch loss: 0.3013653874397278\n",
      "Average valid loss 0.21119171380996704\n",
      "EPOCH 369:\n",
      "Average epoch loss: 0.3382554315030575\n",
      "Average valid loss 0.20765289664268494\n",
      "EPOCH 370:\n",
      "Average epoch loss: 0.30194118395447733\n",
      "Average valid loss 0.28487980365753174\n",
      "EPOCH 371:\n",
      "Average epoch loss: 0.3211023837327957\n",
      "Average valid loss 0.2000492811203003\n",
      "EPOCH 372:\n",
      "Average epoch loss: 0.3271145328879356\n",
      "Average valid loss 0.1932627111673355\n",
      "EPOCH 373:\n",
      "Average epoch loss: 0.3242827236652374\n",
      "Average valid loss 0.19447080790996552\n",
      "EPOCH 374:\n",
      "Average epoch loss: 0.29296137392520905\n",
      "Average valid loss 0.1926610916852951\n",
      "EPOCH 375:\n",
      "Average epoch loss: 0.32480024844408034\n",
      "Average valid loss 1.8159644603729248\n",
      "EPOCH 376:\n",
      "Average epoch loss: 0.319660322368145\n",
      "Average valid loss 0.1857122778892517\n",
      "EPOCH 377:\n",
      "Average epoch loss: 0.28786294981837274\n",
      "Average valid loss 0.18430806696414948\n",
      "EPOCH 378:\n",
      "Average epoch loss: 0.2848953053355217\n",
      "Average valid loss 0.18139420449733734\n",
      "EPOCH 379:\n",
      "Average epoch loss: 0.29370394051074983\n",
      "Average valid loss 0.18967781960964203\n",
      "EPOCH 380:\n",
      "Average epoch loss: 0.3220467954874039\n",
      "Average valid loss 0.17716291546821594\n",
      "EPOCH 381:\n",
      "Average epoch loss: 0.24258855804800988\n",
      "Average valid loss 0.18065527081489563\n",
      "EPOCH 382:\n",
      "Average epoch loss: 0.2787015102803707\n",
      "Average valid loss 0.18518824875354767\n",
      "EPOCH 383:\n",
      "Average epoch loss: 0.30861122608184816\n",
      "Average valid loss 0.486175537109375\n",
      "EPOCH 384:\n",
      "Average epoch loss: 0.30669920742511747\n",
      "Average valid loss 0.16761936247348785\n",
      "EPOCH 385:\n",
      "Average epoch loss: 0.26890844479203224\n",
      "Average valid loss 0.16600477695465088\n",
      "EPOCH 386:\n",
      "Average epoch loss: 0.22458581998944283\n",
      "Average valid loss 0.16161687672138214\n",
      "EPOCH 387:\n",
      "Average epoch loss: 0.24917471930384635\n",
      "Average valid loss 0.16109272837638855\n",
      "EPOCH 388:\n",
      "Average epoch loss: 0.25724023282527925\n",
      "Average valid loss 0.15838134288787842\n",
      "EPOCH 389:\n",
      "Average epoch loss: 0.2584270901978016\n",
      "Average valid loss 0.155641570687294\n",
      "EPOCH 390:\n",
      "Average epoch loss: 0.2536530740559101\n",
      "Average valid loss 0.15414832532405853\n",
      "EPOCH 391:\n",
      "Average epoch loss: 0.2352290630340576\n",
      "Average valid loss 0.1548067033290863\n",
      "EPOCH 392:\n",
      "Average epoch loss: 0.22992018461227418\n",
      "Average valid loss 1.239830732345581\n",
      "EPOCH 393:\n",
      "Average epoch loss: 0.21988786980509759\n",
      "Average valid loss 1.4022012948989868\n",
      "EPOCH 394:\n",
      "Average epoch loss: 0.18897510170936585\n",
      "Average valid loss 0.1737034022808075\n",
      "EPOCH 395:\n",
      "Average epoch loss: 0.1999669775366783\n",
      "Average valid loss 0.14326386153697968\n",
      "EPOCH 396:\n",
      "Average epoch loss: 0.29194261580705644\n",
      "Average valid loss 0.13878215849399567\n",
      "EPOCH 397:\n",
      "Average epoch loss: 0.2932586118578911\n",
      "Average valid loss 0.20134539902210236\n",
      "EPOCH 398:\n",
      "Average epoch loss: 0.21071765720844268\n",
      "Average valid loss 1.354512333869934\n",
      "EPOCH 399:\n",
      "Average epoch loss: 0.2062369741499424\n",
      "Average valid loss 0.13580913841724396\n",
      "EPOCH 400:\n",
      "Average epoch loss: 0.24347857460379602\n",
      "Average valid loss 0.13762089610099792\n",
      "EPOCH 401:\n",
      "Average epoch loss: 0.20881168469786643\n",
      "Average valid loss 0.13695694506168365\n",
      "EPOCH 402:\n",
      "Average epoch loss: 0.19342911392450332\n",
      "Average valid loss 0.4906710088253021\n",
      "EPOCH 403:\n",
      "Average epoch loss: 0.19762013480067253\n",
      "Average valid loss 0.12865711748600006\n",
      "EPOCH 404:\n",
      "Average epoch loss: 0.22997297644615172\n",
      "Average valid loss 0.12541915476322174\n",
      "EPOCH 405:\n",
      "Average epoch loss: 0.17822214290499688\n",
      "Average valid loss 0.18370047211647034\n",
      "EPOCH 406:\n",
      "Average epoch loss: 0.20586443021893502\n",
      "Average valid loss 0.11883524805307388\n",
      "EPOCH 407:\n",
      "Average epoch loss: 0.20543527603149414\n",
      "Average valid loss 0.11989588290452957\n",
      "EPOCH 408:\n",
      "Average epoch loss: 0.21373089253902436\n",
      "Average valid loss 0.11990769952535629\n",
      "EPOCH 409:\n",
      "Average epoch loss: 0.17707298770546914\n",
      "Average valid loss 0.1200091689825058\n",
      "EPOCH 410:\n",
      "Average epoch loss: 0.21009964495897293\n",
      "Average valid loss 0.11856037378311157\n",
      "EPOCH 411:\n",
      "Average epoch loss: 0.17931465096771718\n",
      "Average valid loss 0.11678452044725418\n",
      "EPOCH 412:\n",
      "Average epoch loss: 0.19693334251642228\n",
      "Average valid loss 1.3553791046142578\n",
      "EPOCH 413:\n",
      "Average epoch loss: 0.23449297845363617\n",
      "Average valid loss 0.12784124910831451\n",
      "EPOCH 414:\n",
      "Average epoch loss: 0.18631199672818183\n",
      "Average valid loss 0.11821932345628738\n",
      "EPOCH 415:\n",
      "Average epoch loss: 0.2179602138698101\n",
      "Average valid loss 0.3047062158584595\n",
      "EPOCH 416:\n",
      "Average epoch loss: 0.21531140953302383\n",
      "Average valid loss 0.11579768359661102\n",
      "EPOCH 417:\n",
      "Average epoch loss: 0.1903810277581215\n",
      "Average valid loss 0.11036175489425659\n",
      "EPOCH 418:\n",
      "Average epoch loss: 0.19300630167126656\n",
      "Average valid loss 0.10801485180854797\n",
      "EPOCH 419:\n",
      "Average epoch loss: 0.2032226525247097\n",
      "Average valid loss 0.1067834421992302\n",
      "EPOCH 420:\n",
      "Average epoch loss: 0.16151252537965774\n",
      "Average valid loss 0.10793403536081314\n",
      "EPOCH 421:\n",
      "Average epoch loss: 0.1920780435204506\n",
      "Average valid loss 0.11519190669059753\n",
      "EPOCH 422:\n",
      "Average epoch loss: 0.1825857415795326\n",
      "Average valid loss 0.10303571820259094\n",
      "EPOCH 423:\n",
      "Average epoch loss: 0.14242805391550065\n",
      "Average valid loss 0.2666943073272705\n",
      "EPOCH 424:\n",
      "Average epoch loss: 0.17844227999448775\n",
      "Average valid loss 0.30043506622314453\n",
      "EPOCH 425:\n",
      "Average epoch loss: 0.14146935380995274\n",
      "Average valid loss 0.10525452345609665\n",
      "EPOCH 426:\n",
      "Average epoch loss: 0.18594580367207528\n",
      "Average valid loss 0.10279769450426102\n",
      "EPOCH 427:\n",
      "Average epoch loss: 0.18993402868509293\n",
      "Average valid loss 0.10106322169303894\n",
      "EPOCH 428:\n",
      "Average epoch loss: 0.15365691781044005\n",
      "Average valid loss 0.1449528932571411\n",
      "EPOCH 429:\n",
      "Average epoch loss: 0.18742036260664463\n",
      "Average valid loss 0.09699724614620209\n",
      "EPOCH 430:\n",
      "Average epoch loss: 0.13534605503082275\n",
      "Average valid loss 0.09606894850730896\n",
      "EPOCH 431:\n",
      "Average epoch loss: 0.13513091951608658\n",
      "Average valid loss 0.13806456327438354\n",
      "EPOCH 432:\n",
      "Average epoch loss: 0.14574639201164247\n",
      "Average valid loss 0.09787558764219284\n",
      "EPOCH 433:\n",
      "Average epoch loss: 0.144978891313076\n",
      "Average valid loss 0.0981210246682167\n",
      "EPOCH 434:\n",
      "Average epoch loss: 0.14057187139987945\n",
      "Average valid loss 0.09622736275196075\n",
      "EPOCH 435:\n",
      "Average epoch loss: 0.14238037541508675\n",
      "Average valid loss 0.353701651096344\n",
      "EPOCH 436:\n",
      "Average epoch loss: 0.13928653448820114\n",
      "Average valid loss 0.09138955920934677\n",
      "EPOCH 437:\n",
      "Average epoch loss: 0.13463568165898324\n",
      "Average valid loss 0.2321571707725525\n",
      "EPOCH 438:\n",
      "Average epoch loss: 0.14741897210478783\n",
      "Average valid loss 0.0902433842420578\n",
      "EPOCH 439:\n",
      "Average epoch loss: 0.13909027129411697\n",
      "Average valid loss 0.1574612259864807\n",
      "EPOCH 440:\n",
      "Average epoch loss: 0.14651631712913513\n",
      "Average valid loss 0.09193716198205948\n",
      "EPOCH 441:\n",
      "Average epoch loss: 0.18154275976121426\n",
      "Average valid loss 0.09259136021137238\n",
      "EPOCH 442:\n",
      "Average epoch loss: 0.11822976917028427\n",
      "Average valid loss 0.12438632547855377\n",
      "EPOCH 443:\n",
      "Average epoch loss: 0.13918923772871494\n",
      "Average valid loss 0.08920399844646454\n",
      "EPOCH 444:\n",
      "Average epoch loss: 0.13114189840853213\n",
      "Average valid loss 0.0889461562037468\n",
      "EPOCH 445:\n",
      "Average epoch loss: 0.1394857969135046\n",
      "Average valid loss 0.23641949892044067\n",
      "EPOCH 446:\n",
      "Average epoch loss: 0.10894555188715457\n",
      "Average valid loss 0.09165643900632858\n",
      "EPOCH 447:\n",
      "Average epoch loss: 0.12633088789880276\n",
      "Average valid loss 0.09111328423023224\n",
      "EPOCH 448:\n",
      "Average epoch loss: 0.15714123360812665\n",
      "Average valid loss 0.12608914077281952\n",
      "EPOCH 449:\n",
      "Average epoch loss: 0.12273156493902207\n",
      "Average valid loss 0.12649448215961456\n",
      "EPOCH 450:\n",
      "Average epoch loss: 0.09976677559316158\n",
      "Average valid loss 0.09167424589395523\n",
      "EPOCH 451:\n",
      "Average epoch loss: 0.10588253103196621\n",
      "Average valid loss 0.20759640634059906\n",
      "EPOCH 452:\n",
      "Average epoch loss: 0.14887141436338425\n",
      "Average valid loss 0.09034021198749542\n",
      "EPOCH 453:\n",
      "Average epoch loss: 0.14529803544282913\n",
      "Average valid loss 0.08894900232553482\n",
      "EPOCH 454:\n",
      "Average epoch loss: 0.1202352423220873\n",
      "Average valid loss 1.3091695308685303\n",
      "EPOCH 455:\n",
      "Average epoch loss: 0.13551675602793695\n",
      "Average valid loss 0.10515005141496658\n",
      "EPOCH 456:\n",
      "Average epoch loss: 0.11941712945699692\n",
      "Average valid loss 0.08546172827482224\n",
      "EPOCH 457:\n",
      "Average epoch loss: 0.10188219845294952\n",
      "Average valid loss 0.08317260444164276\n",
      "EPOCH 458:\n",
      "Average epoch loss: 0.09551940597593785\n",
      "Average valid loss 0.0845799669623375\n",
      "EPOCH 459:\n",
      "Average epoch loss: 0.12336317971348762\n",
      "Average valid loss 0.08561746031045914\n",
      "EPOCH 460:\n",
      "Average epoch loss: 0.11388796716928482\n",
      "Average valid loss 0.08739761263132095\n",
      "EPOCH 461:\n",
      "Average epoch loss: 0.11981610991060734\n",
      "Average valid loss 0.139868825674057\n",
      "EPOCH 462:\n",
      "Average epoch loss: 0.10743814706802368\n",
      "Average valid loss 0.0978068932890892\n",
      "EPOCH 463:\n",
      "Average epoch loss: 0.12711079753935337\n",
      "Average valid loss 0.08674443513154984\n",
      "EPOCH 464:\n",
      "Average epoch loss: 0.11338281109929085\n",
      "Average valid loss 0.0869157686829567\n",
      "EPOCH 465:\n",
      "Average epoch loss: 0.12170058116316795\n",
      "Average valid loss 0.08699879795312881\n",
      "EPOCH 466:\n",
      "Average epoch loss: 0.12004944123327732\n",
      "Average valid loss 0.0863599181175232\n",
      "EPOCH 467:\n",
      "Average epoch loss: 0.1396675318479538\n",
      "Average valid loss 0.15457214415073395\n",
      "EPOCH 468:\n",
      "Average epoch loss: 0.13756891917437314\n",
      "Average valid loss 0.08339112251996994\n",
      "EPOCH 469:\n",
      "Average epoch loss: 0.1092359609901905\n",
      "Average valid loss 0.08131721615791321\n",
      "EPOCH 470:\n",
      "Average epoch loss: 0.10592257156968117\n",
      "Average valid loss 0.07981253415346146\n",
      "EPOCH 471:\n",
      "Average epoch loss: 0.10372131392359733\n",
      "Average valid loss 0.08345966041088104\n",
      "EPOCH 472:\n",
      "Average epoch loss: 0.11296088583767414\n",
      "Average valid loss 0.08399245142936707\n",
      "EPOCH 473:\n",
      "Average epoch loss: 0.1289501465857029\n",
      "Average valid loss 0.08494610339403152\n",
      "EPOCH 474:\n",
      "Average epoch loss: 0.10837729945778847\n",
      "Average valid loss 0.08474039286375046\n",
      "EPOCH 475:\n",
      "Average epoch loss: 0.13479098528623581\n",
      "Average valid loss 0.08570386469364166\n",
      "EPOCH 476:\n",
      "Average epoch loss: 0.12765860222280026\n",
      "Average valid loss 0.08610996603965759\n",
      "EPOCH 477:\n",
      "Average epoch loss: 0.08892202377319336\n",
      "Average valid loss 0.08840575814247131\n",
      "EPOCH 478:\n",
      "Average epoch loss: 0.09321999251842499\n",
      "Average valid loss 0.08715531975030899\n",
      "EPOCH 479:\n",
      "Average epoch loss: 0.1180666983127594\n",
      "Average valid loss 0.11760126054286957\n",
      "EPOCH 480:\n",
      "Average epoch loss: 0.11292769592255354\n",
      "Average valid loss 0.08730129897594452\n",
      "EPOCH 481:\n",
      "Average epoch loss: 0.09862886741757393\n",
      "Average valid loss 0.0875105932354927\n",
      "EPOCH 482:\n",
      "Average epoch loss: 0.09256971403956413\n",
      "Average valid loss 0.12230519950389862\n",
      "EPOCH 483:\n",
      "Average epoch loss: 0.1096656158566475\n",
      "Average valid loss 0.11691942811012268\n",
      "EPOCH 484:\n",
      "Average epoch loss: 0.09250515401363373\n",
      "Average valid loss 0.08510252833366394\n",
      "EPOCH 485:\n",
      "Average epoch loss: 0.09276567455381154\n",
      "Average valid loss 0.15810652077198029\n",
      "EPOCH 486:\n",
      "Average epoch loss: 0.0989727096632123\n",
      "Average valid loss 0.08224841952323914\n",
      "EPOCH 487:\n",
      "Average epoch loss: 0.07181641720235347\n",
      "Average valid loss 0.08237531781196594\n",
      "EPOCH 488:\n",
      "Average epoch loss: 0.0843781691044569\n",
      "Average valid loss 0.08030939847230911\n",
      "EPOCH 489:\n",
      "Average epoch loss: 0.09564678557217121\n",
      "Average valid loss 0.08024799078702927\n",
      "EPOCH 490:\n",
      "Average epoch loss: 0.08744862638413906\n",
      "Average valid loss 0.07945626974105835\n",
      "EPOCH 491:\n",
      "Average epoch loss: 0.1102737046778202\n",
      "Average valid loss 0.08120478689670563\n",
      "EPOCH 492:\n",
      "Average epoch loss: 0.10811722911894321\n",
      "Average valid loss 0.08308670669794083\n",
      "EPOCH 493:\n",
      "Average epoch loss: 0.11894243694841862\n",
      "Average valid loss 0.12497438490390778\n",
      "EPOCH 494:\n",
      "Average epoch loss: 0.1167800473049283\n",
      "Average valid loss 0.10526705533266068\n",
      "EPOCH 495:\n",
      "Average epoch loss: 0.12166752181947231\n",
      "Average valid loss 0.08728781342506409\n",
      "EPOCH 496:\n",
      "Average epoch loss: 0.07494966331869364\n",
      "Average valid loss 0.08636236190795898\n",
      "EPOCH 497:\n",
      "Average epoch loss: 0.09535922333598137\n",
      "Average valid loss 0.1407766342163086\n",
      "EPOCH 498:\n",
      "Average epoch loss: 0.07266704272478819\n",
      "Average valid loss 0.1112322136759758\n",
      "EPOCH 499:\n",
      "Average epoch loss: 0.05893212556838989\n",
      "Average valid loss 0.08366212248802185\n",
      "EPOCH 500:\n",
      "Average epoch loss: 0.0865092346444726\n",
      "Average valid loss 0.0835137739777565\n",
      "EPOCH 501:\n",
      "Average epoch loss: 0.0817488070577383\n",
      "Average valid loss 0.08433745801448822\n",
      "EPOCH 502:\n",
      "Average epoch loss: 0.09609660264104605\n",
      "Average valid loss 0.08671719580888748\n",
      "EPOCH 503:\n",
      "Average epoch loss: 0.09234317019581795\n",
      "Average valid loss 0.08734375238418579\n",
      "EPOCH 504:\n",
      "Average epoch loss: 0.1010117718949914\n",
      "Average valid loss 0.15020941197872162\n",
      "EPOCH 505:\n",
      "Average epoch loss: 0.07487362381070853\n",
      "Average valid loss 0.08316902071237564\n",
      "EPOCH 506:\n",
      "Average epoch loss: 0.08503729738295078\n",
      "Average valid loss 0.08211527019739151\n",
      "EPOCH 507:\n",
      "Average epoch loss: 0.07230130210518837\n",
      "Average valid loss 0.08219871670007706\n",
      "EPOCH 508:\n",
      "Average epoch loss: 0.06919875256717205\n",
      "Average valid loss 0.08289673924446106\n",
      "EPOCH 509:\n",
      "Average epoch loss: 0.06668513007462025\n",
      "Average valid loss 0.08378896117210388\n",
      "EPOCH 510:\n",
      "Average epoch loss: 0.09879884161055089\n",
      "Average valid loss 0.08470476418733597\n",
      "EPOCH 511:\n",
      "Average epoch loss: 0.07288294732570648\n",
      "Average valid loss 0.08800161629915237\n",
      "EPOCH 512:\n",
      "Average epoch loss: 0.06480486635118723\n",
      "Average valid loss 0.08658577501773834\n",
      "EPOCH 513:\n",
      "Average epoch loss: 0.08125847149640322\n",
      "Average valid loss 0.08737652748823166\n",
      "EPOCH 514:\n",
      "Average epoch loss: 0.0996750934049487\n",
      "Average valid loss 0.09229988604784012\n",
      "EPOCH 515:\n",
      "Average epoch loss: 0.0780470035970211\n",
      "Average valid loss 0.0931573212146759\n",
      "EPOCH 516:\n",
      "Average epoch loss: 0.08248936235904694\n",
      "Average valid loss 0.1335282325744629\n",
      "EPOCH 517:\n",
      "Average epoch loss: 0.0933208093047142\n",
      "Average valid loss 0.0884975865483284\n",
      "EPOCH 518:\n",
      "Average epoch loss: 0.08517213389277459\n",
      "Average valid loss 0.08689836412668228\n",
      "EPOCH 519:\n",
      "Average epoch loss: 0.05499466508626938\n",
      "Average valid loss 0.08588709682226181\n",
      "EPOCH 520:\n",
      "Average epoch loss: 0.07979796510189771\n",
      "Average valid loss 0.08655887097120285\n",
      "EPOCH 521:\n",
      "Average epoch loss: 0.08958331607282162\n",
      "Average valid loss 0.0875391885638237\n",
      "EPOCH 522:\n",
      "Average epoch loss: 0.06869468092918396\n",
      "Average valid loss 0.08283951133489609\n",
      "EPOCH 523:\n",
      "Average epoch loss: 0.06481207627803087\n",
      "Average valid loss 0.08287952840328217\n",
      "EPOCH 524:\n",
      "Average epoch loss: 0.07807368841022252\n",
      "Average valid loss 0.07913435250520706\n",
      "EPOCH 525:\n",
      "Average epoch loss: 0.08831977285444736\n",
      "Average valid loss 0.07710031419992447\n",
      "EPOCH 526:\n",
      "Average epoch loss: 0.062489009089767934\n",
      "Average valid loss 0.07709922641515732\n",
      "EPOCH 527:\n",
      "Average epoch loss: 0.06709889192134141\n",
      "Average valid loss 0.10103823244571686\n",
      "EPOCH 528:\n",
      "Average epoch loss: 0.0807331494987011\n",
      "Average valid loss 0.0814954936504364\n",
      "EPOCH 529:\n",
      "Average epoch loss: 0.07497104797512293\n",
      "Average valid loss 0.08226746320724487\n",
      "EPOCH 530:\n",
      "Average epoch loss: 0.07375173270702362\n",
      "Average valid loss 0.08165743201971054\n",
      "EPOCH 531:\n",
      "Average epoch loss: 0.05752082765102386\n",
      "Average valid loss 0.08125609904527664\n",
      "EPOCH 532:\n",
      "Average epoch loss: 0.07600862793624401\n",
      "Average valid loss 1.27601957321167\n",
      "EPOCH 533:\n",
      "Average epoch loss: 0.07405000627040863\n",
      "Average valid loss 0.09421060979366302\n",
      "EPOCH 534:\n",
      "Average epoch loss: 0.07742696180939675\n",
      "Average valid loss 0.0799921378493309\n",
      "EPOCH 535:\n",
      "Average epoch loss: 0.07220017965883016\n",
      "Average valid loss 0.0797608345746994\n",
      "EPOCH 536:\n",
      "Average epoch loss: 0.08451047465205193\n",
      "Average valid loss 0.08002647757530212\n",
      "EPOCH 537:\n",
      "Average epoch loss: 0.06401424240320921\n",
      "Average valid loss 0.08042731881141663\n",
      "EPOCH 538:\n",
      "Average epoch loss: 0.08286235630512237\n",
      "Average valid loss 0.07970164716243744\n",
      "EPOCH 539:\n",
      "Average epoch loss: 0.07964941710233689\n",
      "Average valid loss 1.2580115795135498\n",
      "EPOCH 540:\n",
      "Average epoch loss: 0.07211243193596602\n",
      "Average valid loss 0.08142664283514023\n",
      "EPOCH 541:\n",
      "Average epoch loss: 0.060277981124818324\n",
      "Average valid loss 0.08062861859798431\n",
      "EPOCH 542:\n",
      "Average epoch loss: 0.05877507925033569\n",
      "Average valid loss 0.08417429029941559\n",
      "EPOCH 543:\n",
      "Average epoch loss: 0.06283689979463816\n",
      "Average valid loss 0.08236382901668549\n",
      "EPOCH 544:\n",
      "Average epoch loss: 0.06802826896309852\n",
      "Average valid loss 0.07878725975751877\n",
      "EPOCH 545:\n",
      "Average epoch loss: 0.05189852435141802\n",
      "Average valid loss 0.08454445749521255\n",
      "EPOCH 546:\n",
      "Average epoch loss: 0.08405094295740127\n",
      "Average valid loss 0.08069239556789398\n",
      "EPOCH 547:\n",
      "Average epoch loss: 0.081479137763381\n",
      "Average valid loss 0.08219772577285767\n",
      "EPOCH 548:\n",
      "Average epoch loss: 0.06450193412601948\n",
      "Average valid loss 0.08140980452299118\n",
      "EPOCH 549:\n",
      "Average epoch loss: 0.08400944527238607\n",
      "Average valid loss 1.2865025997161865\n",
      "EPOCH 550:\n",
      "Average epoch loss: 0.08648215383291244\n",
      "Average valid loss 0.0838526040315628\n",
      "EPOCH 551:\n",
      "Average epoch loss: 0.08323945477604866\n",
      "Average valid loss 0.08318420499563217\n",
      "EPOCH 552:\n",
      "Average epoch loss: 0.06695253234356642\n",
      "Average valid loss 0.08190258592367172\n",
      "EPOCH 553:\n",
      "Average epoch loss: 0.07723672278225421\n",
      "Average valid loss 0.08297072350978851\n",
      "EPOCH 554:\n",
      "Average epoch loss: 0.06697789561003446\n",
      "Average valid loss 0.08306329697370529\n",
      "EPOCH 555:\n",
      "Average epoch loss: 0.05057135745882988\n",
      "Average valid loss 1.2846293449401855\n",
      "EPOCH 556:\n",
      "Average epoch loss: 0.0634918635711074\n",
      "Average valid loss 0.08220987766981125\n",
      "EPOCH 557:\n",
      "Average epoch loss: 0.06118628606200218\n",
      "Average valid loss 0.08190089464187622\n",
      "EPOCH 558:\n",
      "Average epoch loss: 0.07850969359278678\n",
      "Average valid loss 0.08179839700460434\n",
      "EPOCH 559:\n",
      "Average epoch loss: 0.07332485187798739\n",
      "Average valid loss 0.08222238719463348\n",
      "EPOCH 560:\n",
      "Average epoch loss: 0.06402181442826986\n",
      "Average valid loss 0.08006978034973145\n",
      "EPOCH 561:\n",
      "Average epoch loss: 0.06404975522309542\n",
      "Average valid loss 0.07991141825914383\n",
      "EPOCH 562:\n",
      "Average epoch loss: 0.059982471913099286\n",
      "Average valid loss 0.07754416763782501\n",
      "EPOCH 563:\n",
      "Average epoch loss: 0.06061938945204019\n",
      "Average valid loss 0.07711867243051529\n",
      "EPOCH 564:\n",
      "Average epoch loss: 0.04292714204639196\n",
      "Average valid loss 0.07799085974693298\n",
      "EPOCH 565:\n",
      "Average epoch loss: 0.06093917898833752\n",
      "Average valid loss 0.08996757864952087\n",
      "EPOCH 566:\n",
      "Average epoch loss: 0.07982691675424576\n",
      "Average valid loss 0.08085734397172928\n",
      "EPOCH 567:\n",
      "Average epoch loss: 0.05683524273335934\n",
      "Average valid loss 0.07999005168676376\n",
      "EPOCH 568:\n",
      "Average epoch loss: 0.058140579611063004\n",
      "Average valid loss 0.07953180372714996\n",
      "EPOCH 569:\n",
      "Average epoch loss: 0.044435752741992476\n",
      "Average valid loss 0.07935293018817902\n",
      "EPOCH 570:\n",
      "Average epoch loss: 0.07382013220340014\n",
      "Average valid loss 0.07967158406972885\n",
      "EPOCH 571:\n",
      "Average epoch loss: 0.05230479333549738\n",
      "Average valid loss 0.08068667352199554\n",
      "EPOCH 572:\n",
      "Average epoch loss: 0.06253023836761713\n",
      "Average valid loss 0.08082907646894455\n",
      "EPOCH 573:\n",
      "Average epoch loss: 0.0635559780523181\n",
      "Average valid loss 0.0781983956694603\n",
      "EPOCH 574:\n",
      "Average epoch loss: 0.05094514451920986\n",
      "Average valid loss 0.07745662331581116\n",
      "EPOCH 575:\n",
      "Average epoch loss: 0.05104186236858368\n",
      "Average valid loss 0.09414293617010117\n",
      "EPOCH 576:\n",
      "Average epoch loss: 0.05985299814492464\n",
      "Average valid loss 0.07854152470827103\n",
      "EPOCH 577:\n",
      "Average epoch loss: 0.06361353863030672\n",
      "Average valid loss 0.07948494702577591\n",
      "EPOCH 578:\n",
      "Average epoch loss: 0.04866949934512377\n",
      "Average valid loss 0.07647662609815598\n",
      "EPOCH 579:\n",
      "Average epoch loss: 0.06322611849755048\n",
      "Average valid loss 0.07750926911830902\n",
      "EPOCH 580:\n",
      "Average epoch loss: 0.06365350540727377\n",
      "Average valid loss 0.07412682473659515\n",
      "EPOCH 581:\n",
      "Average epoch loss: 0.061148989200592044\n",
      "Average valid loss 0.07683650404214859\n",
      "EPOCH 582:\n",
      "Average epoch loss: 0.0714622288942337\n",
      "Average valid loss 1.233768343925476\n",
      "EPOCH 583:\n",
      "Average epoch loss: 0.05050649093464017\n",
      "Average valid loss 0.07827180624008179\n",
      "EPOCH 584:\n",
      "Average epoch loss: 0.04505646228790283\n",
      "Average valid loss 0.0794685035943985\n",
      "EPOCH 585:\n",
      "Average epoch loss: 0.03528096098452806\n",
      "Average valid loss 0.07662945985794067\n",
      "EPOCH 586:\n",
      "Average epoch loss: 0.04960480174049735\n",
      "Average valid loss 0.07462204992771149\n",
      "EPOCH 587:\n",
      "Average epoch loss: 0.0651630500331521\n",
      "Average valid loss 0.07932145148515701\n",
      "EPOCH 588:\n",
      "Average epoch loss: 0.0457853733561933\n",
      "Average valid loss 0.0764036551117897\n",
      "EPOCH 589:\n",
      "Average epoch loss: 0.059273457154631615\n",
      "Average valid loss 0.08061310648918152\n",
      "EPOCH 590:\n",
      "Average epoch loss: 0.06696506571024656\n",
      "Average valid loss 0.07941439002752304\n",
      "EPOCH 591:\n",
      "Average epoch loss: 0.06832719072699547\n",
      "Average valid loss 0.08009728044271469\n",
      "EPOCH 592:\n",
      "Average epoch loss: 0.030634465673938394\n",
      "Average valid loss 0.07873465865850449\n",
      "EPOCH 593:\n",
      "Average epoch loss: 0.05057341223582625\n",
      "Average valid loss 0.07826921343803406\n",
      "EPOCH 594:\n",
      "Average epoch loss: 0.05707097621634603\n",
      "Average valid loss 0.0779077336192131\n",
      "EPOCH 595:\n",
      "Average epoch loss: 0.042117600329220295\n",
      "Average valid loss 0.07764873653650284\n",
      "EPOCH 596:\n",
      "Average epoch loss: 0.05910658370703459\n",
      "Average valid loss 0.08256635069847107\n",
      "EPOCH 597:\n",
      "Average epoch loss: 0.047359257470816374\n",
      "Average valid loss 0.07448187470436096\n",
      "EPOCH 598:\n",
      "Average epoch loss: 0.06764105493202806\n",
      "Average valid loss 0.08345276862382889\n",
      "EPOCH 599:\n",
      "Average epoch loss: 0.0638170599937439\n",
      "Average valid loss 0.072965607047081\n",
      "EPOCH 600:\n",
      "Average epoch loss: 0.060511990915983915\n",
      "Average valid loss 0.07340926676988602\n",
      "EPOCH 601:\n",
      "Average epoch loss: 0.059239936247468\n",
      "Average valid loss 0.07363135367631912\n",
      "EPOCH 602:\n",
      "Average epoch loss: 0.056598585285246375\n",
      "Average valid loss 0.07205243408679962\n",
      "EPOCH 603:\n",
      "Average epoch loss: 0.058221299387514594\n",
      "Average valid loss 0.07160965353250504\n",
      "EPOCH 604:\n",
      "Average epoch loss: 0.040034664794802666\n",
      "Average valid loss 0.07204603403806686\n",
      "EPOCH 605:\n",
      "Average epoch loss: 0.05281449947506189\n",
      "Average valid loss 1.1485164165496826\n",
      "EPOCH 606:\n",
      "Average epoch loss: 0.04164453931152821\n",
      "Average valid loss 0.0793004035949707\n",
      "EPOCH 607:\n",
      "Average epoch loss: 0.03687057513743639\n",
      "Average valid loss 0.0741824060678482\n",
      "EPOCH 608:\n",
      "Average epoch loss: 0.06824799738824368\n",
      "Average valid loss 0.07432762533426285\n",
      "EPOCH 609:\n",
      "Average epoch loss: 0.051571264676749703\n",
      "Average valid loss 0.07415634393692017\n",
      "EPOCH 610:\n",
      "Average epoch loss: 0.039431795477867126\n",
      "Average valid loss 0.08100351691246033\n",
      "EPOCH 611:\n",
      "Average epoch loss: 0.04914491511881351\n",
      "Average valid loss 0.07084310054779053\n",
      "EPOCH 612:\n",
      "Average epoch loss: 0.04594600023701787\n",
      "Average valid loss 0.07087544351816177\n",
      "EPOCH 613:\n",
      "Average epoch loss: 0.04531619567424059\n",
      "Average valid loss 0.07286841422319412\n",
      "EPOCH 614:\n",
      "Average epoch loss: 0.03905181949958205\n",
      "Average valid loss 0.07316607981920242\n",
      "EPOCH 615:\n",
      "Average epoch loss: 0.04287339188158512\n",
      "Average valid loss 0.0724540576338768\n",
      "EPOCH 616:\n",
      "Average epoch loss: 0.05051894690841437\n",
      "Average valid loss 0.07167037576436996\n",
      "EPOCH 617:\n",
      "Average epoch loss: 0.05357904452830553\n",
      "Average valid loss 0.07173627614974976\n",
      "EPOCH 618:\n",
      "Average epoch loss: 0.06618419270962476\n",
      "Average valid loss 0.07135716825723648\n",
      "EPOCH 619:\n",
      "Average epoch loss: 0.0533728064969182\n",
      "Average valid loss 0.07144149392843246\n",
      "EPOCH 620:\n",
      "Average epoch loss: 0.06472759591415525\n",
      "Average valid loss 0.07170858979225159\n",
      "EPOCH 621:\n",
      "Average epoch loss: 0.0464732401072979\n",
      "Average valid loss 0.07340352237224579\n",
      "EPOCH 622:\n",
      "Average epoch loss: 0.03643373590894043\n",
      "Average valid loss 0.08572188764810562\n",
      "EPOCH 623:\n",
      "Average epoch loss: 0.03640022352337837\n",
      "Average valid loss 0.073543019592762\n",
      "EPOCH 624:\n",
      "Average epoch loss: 0.025909683015197516\n",
      "Average valid loss 0.08577097952365875\n",
      "EPOCH 625:\n",
      "Average epoch loss: 0.055134727340191605\n",
      "Average valid loss 0.07414530217647552\n",
      "EPOCH 626:\n",
      "Average epoch loss: 0.05383508317172527\n",
      "Average valid loss 0.07358798384666443\n",
      "EPOCH 627:\n",
      "Average epoch loss: 0.05854877810925245\n",
      "Average valid loss 0.07301615178585052\n",
      "EPOCH 628:\n",
      "Average epoch loss: 0.05973356296308339\n",
      "Average valid loss 0.07278613746166229\n",
      "EPOCH 629:\n",
      "Average epoch loss: 0.042767367139458654\n",
      "Average valid loss 0.08040134608745575\n",
      "EPOCH 630:\n",
      "Average epoch loss: 0.03470552717335522\n",
      "Average valid loss 0.07351678609848022\n",
      "EPOCH 631:\n",
      "Average epoch loss: 0.0380598652176559\n",
      "Average valid loss 0.07289787381887436\n",
      "EPOCH 632:\n",
      "Average epoch loss: 0.053476741630584\n",
      "Average valid loss 0.07020595669746399\n",
      "EPOCH 633:\n",
      "Average epoch loss: 0.05214953729882836\n",
      "Average valid loss 0.0703766942024231\n",
      "EPOCH 634:\n",
      "Average epoch loss: 0.03905290206894278\n",
      "Average valid loss 0.07172200828790665\n",
      "EPOCH 635:\n",
      "Average epoch loss: 0.03602105230093002\n",
      "Average valid loss 0.07298585772514343\n",
      "EPOCH 636:\n",
      "Average epoch loss: 0.04711751993745565\n",
      "Average valid loss 1.1627087593078613\n",
      "EPOCH 637:\n",
      "Average epoch loss: 0.03082291460596025\n",
      "Average valid loss 0.07393287867307663\n",
      "EPOCH 638:\n",
      "Average epoch loss: 0.039035945665091276\n",
      "Average valid loss 1.1802399158477783\n",
      "EPOCH 639:\n",
      "Average epoch loss: 0.04219473134726286\n",
      "Average valid loss 0.0750470831990242\n",
      "EPOCH 640:\n",
      "Average epoch loss: 0.027559995045885445\n",
      "Average valid loss 0.08432161062955856\n",
      "EPOCH 641:\n",
      "Average epoch loss: 0.03790049375966191\n",
      "Average valid loss 0.07314106076955795\n",
      "EPOCH 642:\n",
      "Average epoch loss: 0.03270024433732033\n",
      "Average valid loss 1.143477201461792\n",
      "EPOCH 643:\n",
      "Average epoch loss: 0.04024698957800865\n",
      "Average valid loss 0.07445632666349411\n",
      "EPOCH 644:\n",
      "Average epoch loss: 0.050780687108635904\n",
      "Average valid loss 0.07537841796875\n",
      "EPOCH 645:\n",
      "Average epoch loss: 0.04526253305375576\n",
      "Average valid loss 0.07167372852563858\n",
      "EPOCH 646:\n",
      "Average epoch loss: 0.03174145491793752\n",
      "Average valid loss 0.06983446329832077\n",
      "EPOCH 647:\n",
      "Average epoch loss: 0.035670898482203485\n",
      "Average valid loss 0.0698244571685791\n",
      "EPOCH 648:\n",
      "Average epoch loss: 0.04006120171397924\n",
      "Average valid loss 0.07055764645338058\n",
      "EPOCH 649:\n",
      "Average epoch loss: 0.03245570408180356\n",
      "Average valid loss 0.07342470437288284\n",
      "EPOCH 650:\n",
      "Average epoch loss: 0.050442819762974975\n",
      "Average valid loss 0.07025643438100815\n",
      "EPOCH 651:\n",
      "Average epoch loss: 0.042277149856090546\n",
      "Average valid loss 0.06877214461565018\n",
      "EPOCH 652:\n",
      "Average epoch loss: 0.060880251973867414\n",
      "Average valid loss 1.0959447622299194\n",
      "EPOCH 653:\n",
      "Average epoch loss: 0.03286889316514134\n",
      "Average valid loss 0.06885622441768646\n",
      "EPOCH 654:\n",
      "Average epoch loss: 0.04978938400745392\n",
      "Average valid loss 0.06983289867639542\n",
      "EPOCH 655:\n",
      "Average epoch loss: 0.03354658950120211\n",
      "Average valid loss 0.07290926575660706\n",
      "EPOCH 656:\n",
      "Average epoch loss: 0.035559914214536546\n",
      "Average valid loss 0.07275597751140594\n",
      "EPOCH 657:\n",
      "Average epoch loss: 0.029446697793900967\n",
      "Average valid loss 0.07218602299690247\n",
      "EPOCH 658:\n",
      "Average epoch loss: 0.03874238068237901\n",
      "Average valid loss 0.07054160535335541\n",
      "EPOCH 659:\n",
      "Average epoch loss: 0.04298398331739008\n",
      "Average valid loss 0.07404013723134995\n",
      "EPOCH 660:\n",
      "Average epoch loss: 0.049567704647779466\n",
      "Average valid loss 0.06951206177473068\n",
      "EPOCH 661:\n",
      "Average epoch loss: 0.04030573293566704\n",
      "Average valid loss 0.06891513615846634\n",
      "EPOCH 662:\n",
      "Average epoch loss: 0.034481740277260545\n",
      "Average valid loss 0.07099584490060806\n",
      "EPOCH 663:\n",
      "Average epoch loss: 0.03572012474760413\n",
      "Average valid loss 0.07364922761917114\n",
      "EPOCH 664:\n",
      "Average epoch loss: 0.031916160508990286\n",
      "Average valid loss 0.07643164694309235\n",
      "EPOCH 665:\n",
      "Average epoch loss: 0.0454400110989809\n",
      "Average valid loss 0.07472969591617584\n",
      "EPOCH 666:\n",
      "Average epoch loss: 0.043476310651749374\n",
      "Average valid loss 0.07427041977643967\n",
      "EPOCH 667:\n",
      "Average epoch loss: 0.03835956631228328\n",
      "Average valid loss 0.07490236312150955\n",
      "EPOCH 668:\n",
      "Average epoch loss: 0.04317392921075225\n",
      "Average valid loss 0.0780775398015976\n",
      "EPOCH 669:\n",
      "Average epoch loss: 0.04537836257368326\n",
      "Average valid loss 0.07749510556459427\n",
      "EPOCH 670:\n",
      "Average epoch loss: 0.047122889943420884\n",
      "Average valid loss 0.08244486153125763\n",
      "EPOCH 671:\n",
      "Average epoch loss: 0.042338880337774755\n",
      "Average valid loss 1.220659852027893\n",
      "EPOCH 672:\n",
      "Average epoch loss: 0.03157275943085551\n",
      "Average valid loss 0.07573280483484268\n",
      "EPOCH 673:\n",
      "Average epoch loss: 0.029072944121435285\n",
      "Average valid loss 1.1980761289596558\n",
      "EPOCH 674:\n",
      "Average epoch loss: 0.03132242280989885\n",
      "Average valid loss 0.07521869242191315\n",
      "EPOCH 675:\n",
      "Average epoch loss: 0.02887162338010967\n",
      "Average valid loss 0.07499183714389801\n",
      "EPOCH 676:\n",
      "Average epoch loss: 0.04175150264054537\n",
      "Average valid loss 0.07360659539699554\n",
      "EPOCH 677:\n",
      "Average epoch loss: 0.03644114420749247\n",
      "Average valid loss 1.1774829626083374\n",
      "EPOCH 678:\n",
      "Average epoch loss: 0.02291661947965622\n",
      "Average valid loss 0.07632184773683548\n",
      "EPOCH 679:\n",
      "Average epoch loss: 0.03458217503502965\n",
      "Average valid loss 0.07840170711278915\n",
      "EPOCH 680:\n",
      "Average epoch loss: 0.03541211821138859\n",
      "Average valid loss 0.07949548214673996\n",
      "EPOCH 681:\n",
      "Average epoch loss: 0.03341399431228638\n",
      "Average valid loss 0.07800310105085373\n",
      "EPOCH 682:\n",
      "Average epoch loss: 0.01895485268905759\n",
      "Average valid loss 0.07802470773458481\n",
      "EPOCH 683:\n",
      "Average epoch loss: 0.024935988523066045\n",
      "Average valid loss 0.07876240462064743\n",
      "EPOCH 684:\n",
      "Average epoch loss: 0.04889073548838496\n",
      "Average valid loss 0.079071544110775\n",
      "EPOCH 685:\n",
      "Average epoch loss: 0.048287215270102025\n",
      "Average valid loss 0.07565987855195999\n",
      "EPOCH 686:\n",
      "Average epoch loss: 0.04227551268413663\n",
      "Average valid loss 0.07452861964702606\n",
      "EPOCH 687:\n",
      "Average epoch loss: 0.028331369627267122\n",
      "Average valid loss 0.07394473254680634\n",
      "EPOCH 688:\n",
      "Average epoch loss: 0.04425828903913498\n",
      "Average valid loss 0.07403523474931717\n",
      "EPOCH 689:\n",
      "Average epoch loss: 0.03788029169663787\n",
      "Average valid loss 0.07441610097885132\n",
      "EPOCH 690:\n",
      "Average epoch loss: 0.032483568880707026\n",
      "Average valid loss 0.07474397122859955\n",
      "EPOCH 691:\n",
      "Average epoch loss: 0.026961753983050583\n",
      "Average valid loss 1.17646062374115\n",
      "EPOCH 692:\n",
      "Average epoch loss: 0.05188485942780972\n",
      "Average valid loss 1.17716383934021\n",
      "EPOCH 693:\n",
      "Average epoch loss: 0.029296681564301253\n",
      "Average valid loss 0.07377517968416214\n",
      "EPOCH 694:\n",
      "Average epoch loss: 0.039534397330135106\n",
      "Average valid loss 0.07299208641052246\n",
      "EPOCH 695:\n",
      "Average epoch loss: 0.03807145981118083\n",
      "Average valid loss 0.07211363315582275\n",
      "EPOCH 696:\n",
      "Average epoch loss: 0.039365383051335814\n",
      "Average valid loss 0.07392706722021103\n",
      "EPOCH 697:\n",
      "Average epoch loss: 0.03044362929649651\n",
      "Average valid loss 0.07594522833824158\n",
      "EPOCH 698:\n",
      "Average epoch loss: 0.03945764619857073\n",
      "Average valid loss 0.07739635556936264\n",
      "EPOCH 699:\n",
      "Average epoch loss: 0.037225513439625504\n",
      "Average valid loss 0.07748700678348541\n",
      "EPOCH 700:\n",
      "Average epoch loss: 0.021970461960881947\n",
      "Average valid loss 1.286180853843689\n",
      "EPOCH 701:\n",
      "Average epoch loss: 0.019690053537487984\n",
      "Average valid loss 0.08032315224409103\n",
      "EPOCH 702:\n",
      "Average epoch loss: 0.03751517799682915\n",
      "Average valid loss 0.08002283424139023\n",
      "EPOCH 703:\n",
      "Average epoch loss: 0.022465716861188412\n",
      "Average valid loss 1.2605156898498535\n",
      "EPOCH 704:\n",
      "Average epoch loss: 0.03275234750472009\n",
      "Average valid loss 0.07782654464244843\n",
      "EPOCH 705:\n",
      "Average epoch loss: 0.021342383604496716\n",
      "Average valid loss 0.07707244157791138\n",
      "EPOCH 706:\n",
      "Average epoch loss: 0.04147350126877427\n",
      "Average valid loss 0.07654161006212234\n",
      "EPOCH 707:\n",
      "Average epoch loss: 0.03831716859713197\n",
      "Average valid loss 0.07644655555486679\n",
      "EPOCH 708:\n",
      "Average epoch loss: 0.02703217277303338\n",
      "Average valid loss 0.08302491158246994\n",
      "EPOCH 709:\n",
      "Average epoch loss: 0.0420463461894542\n",
      "Average valid loss 0.077686607837677\n",
      "EPOCH 710:\n",
      "Average epoch loss: 0.033594823395833376\n",
      "Average valid loss 0.07823865115642548\n",
      "EPOCH 711:\n",
      "Average epoch loss: 0.042381483828648925\n",
      "Average valid loss 0.07787571102380753\n",
      "EPOCH 712:\n",
      "Average epoch loss: 0.03883990831673145\n",
      "Average valid loss 0.07632803916931152\n",
      "EPOCH 713:\n",
      "Average epoch loss: 0.04159728530794382\n",
      "Average valid loss 0.07662203162908554\n",
      "EPOCH 714:\n",
      "Average epoch loss: 0.0269307833397761\n",
      "Average valid loss 0.07783554494380951\n",
      "EPOCH 715:\n",
      "Average epoch loss: 0.04103498854674399\n",
      "Average valid loss 0.07703438401222229\n",
      "EPOCH 716:\n",
      "Average epoch loss: 0.03020901596173644\n",
      "Average valid loss 0.07664411514997482\n",
      "EPOCH 717:\n",
      "Average epoch loss: 0.04231111495755613\n",
      "Average valid loss 0.07532937824726105\n",
      "EPOCH 718:\n",
      "Average epoch loss: 0.03476940123364329\n",
      "Average valid loss 0.0736776664853096\n",
      "EPOCH 719:\n",
      "Average epoch loss: 0.030228799674659967\n",
      "Average valid loss 0.07374266535043716\n",
      "EPOCH 720:\n",
      "Average epoch loss: 0.0299081286881119\n",
      "Average valid loss 0.07880435883998871\n",
      "EPOCH 721:\n",
      "Average epoch loss: 0.034180473536252975\n",
      "Average valid loss 0.0784226730465889\n",
      "EPOCH 722:\n",
      "Average epoch loss: 0.024789428524672984\n",
      "Average valid loss 0.07083044201135635\n",
      "EPOCH 723:\n",
      "Average epoch loss: 0.031786830816417935\n",
      "Average valid loss 0.07187522947788239\n",
      "EPOCH 724:\n",
      "Average epoch loss: 0.024137529963627457\n",
      "Average valid loss 1.1554657220840454\n",
      "EPOCH 725:\n",
      "Average epoch loss: 0.018066858127713203\n",
      "Average valid loss 0.07413093745708466\n",
      "EPOCH 726:\n",
      "Average epoch loss: 0.034725741995498535\n",
      "Average valid loss 0.07545919716358185\n",
      "EPOCH 727:\n",
      "Average epoch loss: 0.014195932308211923\n",
      "Average valid loss 0.0749681293964386\n",
      "EPOCH 728:\n",
      "Average epoch loss: 0.02787536457180977\n",
      "Average valid loss 1.1828655004501343\n",
      "EPOCH 729:\n",
      "Average epoch loss: 0.035224550683051345\n",
      "Average valid loss 0.08069462329149246\n",
      "EPOCH 730:\n",
      "Average epoch loss: 0.027090773452073336\n",
      "Average valid loss 0.08046403527259827\n",
      "EPOCH 731:\n",
      "Average epoch loss: 0.028399651497602464\n",
      "Average valid loss 0.07368229329586029\n",
      "EPOCH 732:\n",
      "Average epoch loss: 0.029466852266341447\n",
      "Average valid loss 0.07016487419605255\n",
      "EPOCH 733:\n",
      "Average epoch loss: 0.028821808006614446\n",
      "Average valid loss 0.06949824839830399\n",
      "EPOCH 734:\n",
      "Average epoch loss: 0.029807791672647\n",
      "Average valid loss 0.07039116322994232\n",
      "EPOCH 735:\n",
      "Average epoch loss: 0.04182689553126693\n",
      "Average valid loss 0.07062392681837082\n",
      "EPOCH 736:\n",
      "Average epoch loss: 0.04391926529351622\n",
      "Average valid loss 1.1389522552490234\n",
      "EPOCH 737:\n",
      "Average epoch loss: 0.02827887199819088\n",
      "Average valid loss 0.07063373923301697\n",
      "EPOCH 738:\n",
      "Average epoch loss: 0.019048550724983217\n",
      "Average valid loss 0.06839226186275482\n",
      "EPOCH 739:\n",
      "Average epoch loss: 0.03407876291312277\n",
      "Average valid loss 0.06739944219589233\n",
      "EPOCH 740:\n",
      "Average epoch loss: 0.015134615171700717\n",
      "Average valid loss 0.06747277826070786\n",
      "EPOCH 741:\n",
      "Average epoch loss: 0.025670253415592016\n",
      "Average valid loss 0.06806636601686478\n",
      "EPOCH 742:\n",
      "Average epoch loss: 0.03855438302271068\n",
      "Average valid loss 0.06859248876571655\n",
      "EPOCH 743:\n",
      "Average epoch loss: 0.020976674370467664\n",
      "Average valid loss 0.07172075659036636\n",
      "EPOCH 744:\n",
      "Average epoch loss: 0.03201793618500233\n",
      "Average valid loss 0.07319987565279007\n",
      "EPOCH 745:\n",
      "Average epoch loss: 0.04596092372667045\n",
      "Average valid loss 0.073513925075531\n",
      "EPOCH 746:\n",
      "Average epoch loss: 0.028894030768424273\n",
      "Average valid loss 0.07460924237966537\n",
      "EPOCH 747:\n",
      "Average epoch loss: 0.030587366968393325\n",
      "Average valid loss 0.07488450407981873\n",
      "EPOCH 748:\n",
      "Average epoch loss: 0.024580581672489644\n",
      "Average valid loss 0.07674535363912582\n",
      "EPOCH 749:\n",
      "Average epoch loss: 0.04863919038325548\n",
      "Average valid loss 0.07657741010189056\n",
      "EPOCH 750:\n",
      "Average epoch loss: 0.0435806947760284\n",
      "Average valid loss 0.07633554935455322\n",
      "EPOCH 751:\n",
      "Average epoch loss: 0.021111860405653715\n",
      "Average valid loss 0.07734865695238113\n",
      "EPOCH 752:\n",
      "Average epoch loss: 0.025265491008758544\n",
      "Average valid loss 0.07627789676189423\n",
      "EPOCH 753:\n",
      "Average epoch loss: 0.0244933164678514\n",
      "Average valid loss 0.07415376603603363\n",
      "EPOCH 754:\n",
      "Average epoch loss: 0.032813623920083045\n",
      "Average valid loss 0.07128618657588959\n",
      "EPOCH 755:\n",
      "Average epoch loss: 0.018223485024645925\n",
      "Average valid loss 1.1155637502670288\n",
      "EPOCH 756:\n",
      "Average epoch loss: 0.023044398752972484\n",
      "Average valid loss 0.07027833163738251\n",
      "EPOCH 757:\n",
      "Average epoch loss: 0.038098845072090624\n",
      "Average valid loss 0.07036616653203964\n",
      "EPOCH 758:\n",
      "Average epoch loss: 0.023261462431401016\n",
      "Average valid loss 0.07072199881076813\n",
      "EPOCH 759:\n",
      "Average epoch loss: 0.017150101996958256\n",
      "Average valid loss 1.1433407068252563\n",
      "EPOCH 760:\n",
      "Average epoch loss: 0.022583664348348974\n",
      "Average valid loss 0.07045353204011917\n",
      "EPOCH 761:\n",
      "Average epoch loss: 0.028854631911963226\n",
      "Average valid loss 0.07077258825302124\n",
      "EPOCH 762:\n",
      "Average epoch loss: 0.027455801237374546\n",
      "Average valid loss 0.07147882133722305\n",
      "EPOCH 763:\n",
      "Average epoch loss: 0.026952370582148433\n",
      "Average valid loss 0.07203453779220581\n",
      "EPOCH 764:\n",
      "Average epoch loss: 0.051844256930053234\n",
      "Average valid loss 0.0808352679014206\n",
      "EPOCH 765:\n",
      "Average epoch loss: 0.022847022954374553\n",
      "Average valid loss 0.07295170426368713\n",
      "EPOCH 766:\n",
      "Average epoch loss: 0.02921339003369212\n",
      "Average valid loss 0.07392623275518417\n",
      "EPOCH 767:\n",
      "Average epoch loss: 0.02739124959334731\n",
      "Average valid loss 0.0869370773434639\n",
      "EPOCH 768:\n",
      "Average epoch loss: 0.027451268490403892\n",
      "Average valid loss 0.07565911114215851\n",
      "EPOCH 769:\n",
      "Average epoch loss: 0.02732615969143808\n",
      "Average valid loss 0.07555701583623886\n",
      "EPOCH 770:\n",
      "Average epoch loss: 0.02604544279165566\n",
      "Average valid loss 0.0760812908411026\n",
      "EPOCH 771:\n",
      "Average epoch loss: 0.019159232173115014\n",
      "Average valid loss 0.07801363617181778\n",
      "EPOCH 772:\n",
      "Average epoch loss: 0.0303122085519135\n",
      "Average valid loss 0.07838340848684311\n",
      "EPOCH 773:\n",
      "Average epoch loss: 0.032398353796452284\n",
      "Average valid loss 0.07670699059963226\n",
      "EPOCH 774:\n",
      "Average epoch loss: 0.014916260121390223\n",
      "Average valid loss 0.08292482793331146\n",
      "EPOCH 775:\n",
      "Average epoch loss: 0.025416335090994835\n",
      "Average valid loss 0.07386040687561035\n",
      "EPOCH 776:\n",
      "Average epoch loss: 0.02371823969297111\n",
      "Average valid loss 0.0727519541978836\n",
      "EPOCH 777:\n",
      "Average epoch loss: 0.020962110348045827\n",
      "Average valid loss 0.07196652889251709\n",
      "EPOCH 778:\n",
      "Average epoch loss: 0.029360809409990907\n",
      "Average valid loss 0.07225444167852402\n",
      "EPOCH 779:\n",
      "Average epoch loss: 0.03270357088185847\n",
      "Average valid loss 0.0740063264966011\n",
      "EPOCH 780:\n",
      "Average epoch loss: 0.024596322025172413\n",
      "Average valid loss 0.0737701877951622\n",
      "EPOCH 781:\n",
      "Average epoch loss: 0.025619426881894468\n",
      "Average valid loss 0.07506553828716278\n",
      "EPOCH 782:\n",
      "Average epoch loss: 0.01652398188598454\n",
      "Average valid loss 0.08496071398258209\n",
      "EPOCH 783:\n",
      "Average epoch loss: 0.02808686955831945\n",
      "Average valid loss 0.08847648650407791\n",
      "EPOCH 784:\n",
      "Average epoch loss: 0.028320120042189957\n",
      "Average valid loss 0.07789722830057144\n",
      "EPOCH 785:\n",
      "Average epoch loss: 0.027638664515689016\n",
      "Average valid loss 1.2284015417099\n",
      "EPOCH 786:\n",
      "Average epoch loss: 0.02253092620521784\n",
      "Average valid loss 0.07646876573562622\n",
      "EPOCH 787:\n",
      "Average epoch loss: 0.015848584333434702\n",
      "Average valid loss 0.07462170720100403\n",
      "EPOCH 788:\n",
      "Average epoch loss: 0.023129216860979795\n",
      "Average valid loss 0.07383660227060318\n",
      "EPOCH 789:\n",
      "Average epoch loss: 0.04448276651091874\n",
      "Average valid loss 0.07437490671873093\n",
      "EPOCH 790:\n",
      "Average epoch loss: 0.02026746650226414\n",
      "Average valid loss 0.07943803071975708\n",
      "EPOCH 791:\n",
      "Average epoch loss: 0.01511638737283647\n",
      "Average valid loss 0.07511059194803238\n",
      "EPOCH 792:\n",
      "Average epoch loss: 0.014910439727827907\n",
      "Average valid loss 0.07329245656728745\n",
      "EPOCH 793:\n",
      "Average epoch loss: 0.02471278137527406\n",
      "Average valid loss 0.07210741192102432\n",
      "EPOCH 794:\n",
      "Average epoch loss: 0.0260308054741472\n",
      "Average valid loss 0.07283008843660355\n",
      "EPOCH 795:\n",
      "Average epoch loss: 0.016323615377768875\n",
      "Average valid loss 0.07381684333086014\n",
      "EPOCH 796:\n",
      "Average epoch loss: 0.03051738068461418\n",
      "Average valid loss 0.07555663585662842\n",
      "EPOCH 797:\n",
      "Average epoch loss: 0.03210078151896596\n",
      "Average valid loss 0.07646077126264572\n",
      "EPOCH 798:\n",
      "Average epoch loss: 0.0179408585652709\n",
      "Average valid loss 0.07690520584583282\n",
      "EPOCH 799:\n",
      "Average epoch loss: 0.031198417674750088\n",
      "Average valid loss 0.07575296610593796\n",
      "EPOCH 800:\n",
      "Average epoch loss: 0.017085808073170485\n",
      "Average valid loss 0.07679185271263123\n",
      "EPOCH 801:\n",
      "Average epoch loss: 0.025759668019600213\n",
      "Average valid loss 0.08041433244943619\n",
      "EPOCH 802:\n",
      "Average epoch loss: 0.02997276282403618\n",
      "Average valid loss 0.08359925448894501\n",
      "EPOCH 803:\n",
      "Average epoch loss: 0.019542454788461328\n",
      "Average valid loss 0.08261168003082275\n",
      "EPOCH 804:\n",
      "Average epoch loss: 0.02736697965301573\n",
      "Average valid loss 0.07976438850164413\n",
      "EPOCH 805:\n",
      "Average epoch loss: 0.017829623143188655\n",
      "Average valid loss 0.07867316901683807\n",
      "EPOCH 806:\n",
      "Average epoch loss: 0.029125174134969713\n",
      "Average valid loss 0.07782360911369324\n",
      "EPOCH 807:\n",
      "Average epoch loss: 0.016426021791994573\n",
      "Average valid loss 0.077296182513237\n",
      "EPOCH 808:\n",
      "Average epoch loss: 0.020728091755881906\n",
      "Average valid loss 1.2606128454208374\n",
      "EPOCH 809:\n",
      "Average epoch loss: 0.011144299060106277\n",
      "Average valid loss 0.08012605458498001\n",
      "EPOCH 810:\n",
      "Average epoch loss: 0.012305252905935048\n",
      "Average valid loss 0.0808972492814064\n",
      "EPOCH 811:\n",
      "Average epoch loss: 0.026572279632091522\n",
      "Average valid loss 0.0895615667104721\n",
      "EPOCH 812:\n",
      "Average epoch loss: 0.022515163174830377\n",
      "Average valid loss 0.08098474889993668\n",
      "EPOCH 813:\n",
      "Average epoch loss: 0.02552234851755202\n",
      "Average valid loss 0.08130192756652832\n",
      "EPOCH 814:\n",
      "Average epoch loss: 0.015152317611500621\n",
      "Average valid loss 0.08142738044261932\n",
      "EPOCH 815:\n",
      "Average epoch loss: 0.020817733299918473\n",
      "Average valid loss 0.0832572802901268\n",
      "EPOCH 816:\n",
      "Average epoch loss: 0.023655359307304023\n",
      "Average valid loss 0.08030521869659424\n",
      "EPOCH 817:\n",
      "Average epoch loss: 0.022246796172112226\n",
      "Average valid loss 1.2378101348876953\n",
      "EPOCH 818:\n",
      "Average epoch loss: 0.018269893201068043\n",
      "Average valid loss 0.07549463212490082\n",
      "EPOCH 819:\n",
      "Average epoch loss: 0.039722200576215984\n",
      "Average valid loss 0.0778607577085495\n",
      "EPOCH 820:\n",
      "Average epoch loss: 0.028021409548819064\n",
      "Average valid loss 0.07765708863735199\n",
      "EPOCH 821:\n",
      "Average epoch loss: 0.022534784534946083\n",
      "Average valid loss 0.07630825787782669\n",
      "EPOCH 822:\n",
      "Average epoch loss: 0.022074071783572437\n",
      "Average valid loss 0.07536086440086365\n",
      "EPOCH 823:\n",
      "Average epoch loss: 0.014908301876857877\n",
      "Average valid loss 0.07719332724809647\n",
      "EPOCH 824:\n",
      "Average epoch loss: 0.014437239710241556\n",
      "Average valid loss 0.078506238758564\n",
      "EPOCH 825:\n",
      "Average epoch loss: 0.02121830973774195\n",
      "Average valid loss 0.07725466042757034\n",
      "EPOCH 826:\n",
      "Average epoch loss: 0.03273880917113274\n",
      "Average valid loss 0.07791510224342346\n",
      "EPOCH 827:\n",
      "Average epoch loss: 0.020916606951504945\n",
      "Average valid loss 0.07684335857629776\n",
      "EPOCH 828:\n",
      "Average epoch loss: 0.027081669494509696\n",
      "Average valid loss 0.07638312876224518\n",
      "EPOCH 829:\n",
      "Average epoch loss: 0.023569327453151344\n",
      "Average valid loss 0.07475071400403976\n",
      "EPOCH 830:\n",
      "Average epoch loss: 0.02413861150853336\n",
      "Average valid loss 0.07390440255403519\n",
      "EPOCH 831:\n",
      "Average epoch loss: 0.020445204665884377\n",
      "Average valid loss 0.06933218985795975\n",
      "EPOCH 832:\n",
      "Average epoch loss: 0.024505718168802558\n",
      "Average valid loss 0.07038017362356186\n",
      "EPOCH 833:\n",
      "Average epoch loss: 0.023132319375872613\n",
      "Average valid loss 0.07226432859897614\n",
      "EPOCH 834:\n",
      "Average epoch loss: 0.03516744878143072\n",
      "Average valid loss 0.0777883231639862\n",
      "EPOCH 835:\n",
      "Average epoch loss: 0.018524414114654063\n",
      "Average valid loss 0.0757625475525856\n",
      "EPOCH 836:\n",
      "Average epoch loss: 0.017566556530073284\n",
      "Average valid loss 0.07685630768537521\n",
      "EPOCH 837:\n",
      "Average epoch loss: 0.013142766570672393\n",
      "Average valid loss 0.07712788134813309\n",
      "EPOCH 838:\n",
      "Average epoch loss: 0.016829834831878544\n",
      "Average valid loss 0.07633176445960999\n",
      "EPOCH 839:\n",
      "Average epoch loss: 0.017261315509676932\n",
      "Average valid loss 0.07656930387020111\n",
      "EPOCH 840:\n",
      "Average epoch loss: 0.01790769963990897\n",
      "Average valid loss 0.07757953554391861\n",
      "EPOCH 841:\n",
      "Average epoch loss: 0.01798338396474719\n",
      "Average valid loss 0.07778951525688171\n",
      "EPOCH 842:\n",
      "Average epoch loss: 0.019698705337941647\n",
      "Average valid loss 1.2493555545806885\n",
      "EPOCH 843:\n",
      "Average epoch loss: 0.022799186478368937\n",
      "Average valid loss 0.08092670887708664\n",
      "EPOCH 844:\n",
      "Average epoch loss: 0.03563083428889513\n",
      "Average valid loss 0.07975764572620392\n",
      "EPOCH 845:\n",
      "Average epoch loss: 0.032091021770611405\n",
      "Average valid loss 0.07778608798980713\n",
      "EPOCH 846:\n",
      "Average epoch loss: 0.016038276348263027\n",
      "Average valid loss 0.076730877161026\n",
      "EPOCH 847:\n",
      "Average epoch loss: 0.01389959892258048\n",
      "Average valid loss 0.07726597040891647\n",
      "EPOCH 848:\n",
      "Average epoch loss: 0.018063001334667206\n",
      "Average valid loss 0.07754530757665634\n",
      "EPOCH 849:\n",
      "Average epoch loss: 0.017213719617575406\n",
      "Average valid loss 0.07652173936367035\n",
      "EPOCH 850:\n",
      "Average epoch loss: 0.02359818769618869\n",
      "Average valid loss 0.07516838610172272\n",
      "EPOCH 851:\n",
      "Average epoch loss: 0.014696153276599944\n",
      "Average valid loss 0.07457894086837769\n",
      "EPOCH 852:\n",
      "Average epoch loss: 0.015284443460404872\n",
      "Average valid loss 0.0743042603135109\n",
      "EPOCH 853:\n",
      "Average epoch loss: 0.022321852459572255\n",
      "Average valid loss 0.07359076291322708\n",
      "EPOCH 854:\n",
      "Average epoch loss: 0.016058604745194317\n",
      "Average valid loss 0.07541108876466751\n",
      "EPOCH 855:\n",
      "Average epoch loss: 0.014917400991544127\n",
      "Average valid loss 0.07722207903862\n",
      "EPOCH 856:\n",
      "Average epoch loss: 0.01764305937103927\n",
      "Average valid loss 0.07818575203418732\n",
      "EPOCH 857:\n",
      "Average epoch loss: 0.015754340682178736\n",
      "Average valid loss 0.07879731059074402\n",
      "EPOCH 858:\n",
      "Average epoch loss: 0.017455863347277046\n",
      "Average valid loss 0.0777192935347557\n",
      "EPOCH 859:\n",
      "Average epoch loss: 0.027849807776510714\n",
      "Average valid loss 0.07713204622268677\n",
      "EPOCH 860:\n",
      "Average epoch loss: 0.009771574148908257\n",
      "Average valid loss 0.07679242640733719\n",
      "EPOCH 861:\n",
      "Average epoch loss: 0.03902581217698753\n",
      "Average valid loss 0.0757976844906807\n",
      "EPOCH 862:\n",
      "Average epoch loss: 0.024239653558470308\n",
      "Average valid loss 0.07638750970363617\n",
      "EPOCH 863:\n",
      "Average epoch loss: 0.029291095910593867\n",
      "Average valid loss 0.07543472200632095\n",
      "EPOCH 864:\n",
      "Average epoch loss: 0.017781250597909092\n",
      "Average valid loss 0.07056882977485657\n",
      "EPOCH 865:\n",
      "Average epoch loss: 0.032889605732634664\n",
      "Average valid loss 0.06637272238731384\n",
      "EPOCH 866:\n",
      "Average epoch loss: 0.02262183071579784\n",
      "Average valid loss 0.06827221065759659\n",
      "EPOCH 867:\n",
      "Average epoch loss: 0.03453563274815678\n",
      "Average valid loss 0.06933408975601196\n",
      "EPOCH 868:\n",
      "Average epoch loss: 0.028661898151040078\n",
      "Average valid loss 0.0685100257396698\n",
      "EPOCH 869:\n",
      "Average epoch loss: 0.01982545303180814\n",
      "Average valid loss 0.06813851743936539\n",
      "EPOCH 870:\n",
      "Average epoch loss: 0.017278084997087717\n",
      "Average valid loss 0.070909783244133\n",
      "EPOCH 871:\n",
      "Average epoch loss: 0.03266294645145536\n",
      "Average valid loss 0.0724962130188942\n",
      "EPOCH 872:\n",
      "Average epoch loss: 0.015576933277770877\n",
      "Average valid loss 0.07246434688568115\n",
      "EPOCH 873:\n",
      "Average epoch loss: 0.018135295109823348\n",
      "Average valid loss 1.1503965854644775\n",
      "EPOCH 874:\n",
      "Average epoch loss: 0.01744513907469809\n",
      "Average valid loss 0.07265415787696838\n",
      "EPOCH 875:\n",
      "Average epoch loss: 0.024360510939732193\n",
      "Average valid loss 0.07230407744646072\n",
      "EPOCH 876:\n",
      "Average epoch loss: 0.012761905370280147\n",
      "Average valid loss 0.07122305035591125\n",
      "EPOCH 877:\n",
      "Average epoch loss: 0.018543293094262482\n",
      "Average valid loss 0.07636434584856033\n",
      "EPOCH 878:\n",
      "Average epoch loss: 0.017252292041666804\n",
      "Average valid loss 0.07240310311317444\n",
      "EPOCH 879:\n",
      "Average epoch loss: 0.019434690568596126\n",
      "Average valid loss 0.07184658199548721\n",
      "EPOCH 880:\n",
      "Average epoch loss: 0.020012263022363187\n",
      "Average valid loss 0.07027173042297363\n",
      "EPOCH 881:\n",
      "Average epoch loss: 0.022427437361329794\n",
      "Average valid loss 0.07100716978311539\n",
      "EPOCH 882:\n",
      "Average epoch loss: 0.014860425377264619\n",
      "Average valid loss 0.06978775560855865\n",
      "EPOCH 883:\n",
      "Average epoch loss: 0.02790776379406452\n",
      "Average valid loss 0.0725531056523323\n",
      "EPOCH 884:\n",
      "Average epoch loss: 0.015697616478428246\n",
      "Average valid loss 0.07363972067832947\n",
      "EPOCH 885:\n",
      "Average epoch loss: 0.019556746212765576\n",
      "Average valid loss 0.07211581617593765\n",
      "EPOCH 886:\n",
      "Average epoch loss: 0.018490651226602494\n",
      "Average valid loss 0.07214205712080002\n",
      "EPOCH 887:\n",
      "Average epoch loss: 0.01813393128104508\n",
      "Average valid loss 0.07404065877199173\n",
      "EPOCH 888:\n",
      "Average epoch loss: 0.015324482461437583\n",
      "Average valid loss 0.0748380571603775\n",
      "EPOCH 889:\n",
      "Average epoch loss: 0.018814549618400633\n",
      "Average valid loss 0.07498610019683838\n",
      "EPOCH 890:\n",
      "Average epoch loss: 0.012788664642721414\n",
      "Average valid loss 0.07559756934642792\n",
      "EPOCH 891:\n",
      "Average epoch loss: 0.011473125428892672\n",
      "Average valid loss 0.07522321492433548\n",
      "EPOCH 892:\n",
      "Average epoch loss: 0.013263874687254429\n",
      "Average valid loss 0.07889807969331741\n",
      "EPOCH 893:\n",
      "Average epoch loss: 0.024035888025537133\n",
      "Average valid loss 0.07396639138460159\n",
      "EPOCH 894:\n",
      "Average epoch loss: 0.02033702824264765\n",
      "Average valid loss 0.07403934001922607\n",
      "EPOCH 895:\n",
      "Average epoch loss: 0.012913636397570372\n",
      "Average valid loss 0.07340886443853378\n",
      "EPOCH 896:\n",
      "Average epoch loss: 0.018377265543676913\n",
      "Average valid loss 0.07198844850063324\n",
      "EPOCH 897:\n",
      "Average epoch loss: 0.019596726447343827\n",
      "Average valid loss 0.07189740985631943\n",
      "EPOCH 898:\n",
      "Average epoch loss: 0.014658432314172387\n",
      "Average valid loss 0.06851840019226074\n",
      "EPOCH 899:\n",
      "Average epoch loss: 0.014423567010089755\n",
      "Average valid loss 0.06818734109401703\n",
      "EPOCH 900:\n",
      "Average epoch loss: 0.012657757755368948\n",
      "Average valid loss 1.0991716384887695\n",
      "EPOCH 901:\n",
      "Average epoch loss: 0.019591853115707637\n",
      "Average valid loss 0.07007177919149399\n",
      "EPOCH 902:\n",
      "Average epoch loss: 0.0208419987000525\n",
      "Average valid loss 0.07100797444581985\n",
      "EPOCH 903:\n",
      "Average epoch loss: 0.01127949943765998\n",
      "Average valid loss 0.07223071157932281\n",
      "EPOCH 904:\n",
      "Average epoch loss: 0.017058616783469916\n",
      "Average valid loss 0.07373720407485962\n",
      "EPOCH 905:\n",
      "Average epoch loss: 0.00929102732334286\n",
      "Average valid loss 0.07381436973810196\n",
      "EPOCH 906:\n",
      "Average epoch loss: 0.019818629929795863\n",
      "Average valid loss 0.07692115008831024\n",
      "EPOCH 907:\n",
      "Average epoch loss: 0.010849216324277221\n",
      "Average valid loss 0.07726175338029861\n",
      "EPOCH 908:\n",
      "Average epoch loss: 0.016142256325110793\n",
      "Average valid loss 0.07576125115156174\n",
      "EPOCH 909:\n",
      "Average epoch loss: 0.01663941335864365\n",
      "Average valid loss 0.07562380284070969\n",
      "EPOCH 910:\n",
      "Average epoch loss: 0.021805337630212307\n",
      "Average valid loss 0.07555820047855377\n",
      "EPOCH 911:\n",
      "Average epoch loss: 0.01837091308552772\n",
      "Average valid loss 0.07472074776887894\n",
      "EPOCH 912:\n",
      "Average epoch loss: 0.018256222363561392\n",
      "Average valid loss 0.073313869535923\n",
      "EPOCH 913:\n",
      "Average epoch loss: 0.02712651654146612\n",
      "Average valid loss 1.1390787363052368\n",
      "EPOCH 914:\n",
      "Average epoch loss: 0.016805665590800344\n",
      "Average valid loss 0.06601016968488693\n",
      "EPOCH 915:\n",
      "Average epoch loss: 0.014841605396941304\n",
      "Average valid loss 0.06009625270962715\n",
      "EPOCH 916:\n",
      "Average epoch loss: 0.029548834450542925\n",
      "Average valid loss 0.05970663204789162\n",
      "EPOCH 917:\n",
      "Average epoch loss: 0.017553796526044607\n",
      "Average valid loss 0.05710191652178764\n",
      "EPOCH 918:\n",
      "Average epoch loss: 0.033739316277205944\n",
      "Average valid loss 0.05673746392130852\n",
      "EPOCH 919:\n",
      "Average epoch loss: 0.034055697917938235\n",
      "Average valid loss 0.057085152715444565\n",
      "EPOCH 920:\n",
      "Average epoch loss: 0.017875823215581475\n",
      "Average valid loss 0.056733909994363785\n",
      "EPOCH 921:\n",
      "Average epoch loss: 0.015739838732406496\n",
      "Average valid loss 0.05640019103884697\n",
      "EPOCH 922:\n",
      "Average epoch loss: 0.014664134592749178\n",
      "Average valid loss 0.05525192245841026\n",
      "EPOCH 923:\n",
      "Average epoch loss: 0.011121037835255265\n",
      "Average valid loss 0.05675072222948074\n",
      "EPOCH 924:\n",
      "Average epoch loss: 0.021534745441749693\n",
      "Average valid loss 0.060407720506191254\n",
      "EPOCH 925:\n",
      "Average epoch loss: 0.024355020886287094\n",
      "Average valid loss 0.0601014718413353\n",
      "EPOCH 926:\n",
      "Average epoch loss: 0.01719446750357747\n",
      "Average valid loss 0.06016121059656143\n",
      "EPOCH 927:\n",
      "Average epoch loss: 0.0157304645748809\n",
      "Average valid loss 0.05817681923508644\n",
      "EPOCH 928:\n",
      "Average epoch loss: 0.010896674962714315\n",
      "Average valid loss 0.057253602892160416\n",
      "EPOCH 929:\n",
      "Average epoch loss: 0.01806456302292645\n",
      "Average valid loss 0.058469440788030624\n",
      "EPOCH 930:\n",
      "Average epoch loss: 0.016680203238502146\n",
      "Average valid loss 0.06321599334478378\n",
      "EPOCH 931:\n",
      "Average epoch loss: 0.024971243599429725\n",
      "Average valid loss 0.06329481303691864\n",
      "EPOCH 932:\n",
      "Average epoch loss: 0.013174122013151646\n",
      "Average valid loss 0.06366181373596191\n",
      "EPOCH 933:\n",
      "Average epoch loss: 0.011281372886151076\n",
      "Average valid loss 0.06410661339759827\n",
      "EPOCH 934:\n",
      "Average epoch loss: 0.014893448213115335\n",
      "Average valid loss 0.06495722383260727\n",
      "EPOCH 935:\n",
      "Average epoch loss: 0.019298592722043394\n",
      "Average valid loss 0.06345002353191376\n",
      "EPOCH 936:\n",
      "Average epoch loss: 0.0069390448275953535\n",
      "Average valid loss 0.06335978209972382\n",
      "EPOCH 937:\n",
      "Average epoch loss: 0.011652316199615598\n",
      "Average valid loss 0.06750622391700745\n",
      "EPOCH 938:\n",
      "Average epoch loss: 0.013593291700817644\n",
      "Average valid loss 0.06089004874229431\n",
      "EPOCH 939:\n",
      "Average epoch loss: 0.012132445257157088\n",
      "Average valid loss 0.06020784750580788\n",
      "EPOCH 940:\n",
      "Average epoch loss: 0.014389318274334073\n",
      "Average valid loss 0.060770586133003235\n",
      "EPOCH 941:\n",
      "Average epoch loss: 0.01904028549324721\n",
      "Average valid loss 0.0629088431596756\n",
      "EPOCH 942:\n",
      "Average epoch loss: 0.0227512723300606\n",
      "Average valid loss 0.05775084346532822\n",
      "EPOCH 943:\n",
      "Average epoch loss: 0.012177400523796678\n",
      "Average valid loss 0.0566592663526535\n",
      "EPOCH 944:\n",
      "Average epoch loss: 0.016604406153783204\n",
      "Average valid loss 0.056399766355752945\n",
      "EPOCH 945:\n",
      "Average epoch loss: 0.013480953546240926\n",
      "Average valid loss 0.056769613176584244\n",
      "EPOCH 946:\n",
      "Average epoch loss: 0.014450554130598902\n",
      "Average valid loss 0.05793650448322296\n",
      "EPOCH 947:\n",
      "Average epoch loss: 0.021404558327049017\n",
      "Average valid loss 0.0626693069934845\n",
      "EPOCH 948:\n",
      "Average epoch loss: 0.014308162778615952\n",
      "Average valid loss 0.06674898415803909\n",
      "EPOCH 949:\n",
      "Average epoch loss: 0.0139584188349545\n",
      "Average valid loss 0.06914892792701721\n",
      "EPOCH 950:\n",
      "Average epoch loss: 0.02396451928652823\n",
      "Average valid loss 0.06825026869773865\n",
      "EPOCH 951:\n",
      "Average epoch loss: 0.018527084356173874\n",
      "Average valid loss 0.06987314671278\n",
      "EPOCH 952:\n",
      "Average epoch loss: 0.017507699551060796\n",
      "Average valid loss 0.07051753252744675\n",
      "EPOCH 953:\n",
      "Average epoch loss: 0.030741632217541337\n",
      "Average valid loss 0.07094607502222061\n",
      "EPOCH 954:\n",
      "Average epoch loss: 0.015187746100127696\n",
      "Average valid loss 0.07027299702167511\n",
      "EPOCH 955:\n",
      "Average epoch loss: 0.01714205234311521\n",
      "Average valid loss 0.06924550235271454\n",
      "EPOCH 956:\n",
      "Average epoch loss: 0.012655368819832803\n",
      "Average valid loss 0.06919923424720764\n",
      "EPOCH 957:\n",
      "Average epoch loss: 0.016882336558774115\n",
      "Average valid loss 0.06941129267215729\n",
      "EPOCH 958:\n",
      "Average epoch loss: 0.028731238283216953\n",
      "Average valid loss 0.06842593103647232\n",
      "EPOCH 959:\n",
      "Average epoch loss: 0.03402405017986894\n",
      "Average valid loss 0.06582263112068176\n",
      "EPOCH 960:\n",
      "Average epoch loss: 0.020459998259320857\n",
      "Average valid loss 0.06492472440004349\n",
      "EPOCH 961:\n",
      "Average epoch loss: 0.019795037060976028\n",
      "Average valid loss 0.06450440734624863\n",
      "EPOCH 962:\n",
      "Average epoch loss: 0.016427944228053094\n",
      "Average valid loss 0.06146084889769554\n",
      "EPOCH 963:\n",
      "Average epoch loss: 0.02439473494887352\n",
      "Average valid loss 0.06164335086941719\n",
      "EPOCH 964:\n",
      "Average epoch loss: 0.03277360089123249\n",
      "Average valid loss 0.06669431179761887\n",
      "EPOCH 965:\n",
      "Average epoch loss: 0.009540394856594503\n",
      "Average valid loss 0.06917094439268112\n",
      "EPOCH 966:\n",
      "Average epoch loss: 0.008945523854345083\n",
      "Average valid loss 0.0715055987238884\n",
      "EPOCH 967:\n",
      "Average epoch loss: 0.02081548678688705\n",
      "Average valid loss 0.0737227201461792\n",
      "EPOCH 968:\n",
      "Average epoch loss: 0.01701796823181212\n",
      "Average valid loss 0.073709636926651\n",
      "EPOCH 969:\n",
      "Average epoch loss: 0.019653418916277587\n",
      "Average valid loss 0.07428533583879471\n",
      "EPOCH 970:\n",
      "Average epoch loss: 0.015201278985477984\n",
      "Average valid loss 0.07272632420063019\n",
      "EPOCH 971:\n",
      "Average epoch loss: 0.016402151703368874\n",
      "Average valid loss 0.06970758736133575\n",
      "EPOCH 972:\n",
      "Average epoch loss: 0.014527897909283638\n",
      "Average valid loss 0.06794983893632889\n",
      "EPOCH 973:\n",
      "Average epoch loss: 0.014371262677013875\n",
      "Average valid loss 0.06871544569730759\n",
      "EPOCH 974:\n",
      "Average epoch loss: 0.007091822405345738\n",
      "Average valid loss 0.06912201642990112\n",
      "EPOCH 975:\n",
      "Average epoch loss: 0.01991154404822737\n",
      "Average valid loss 0.06860063970088959\n",
      "EPOCH 976:\n",
      "Average epoch loss: 0.015289268153719605\n",
      "Average valid loss 0.06846983730792999\n",
      "EPOCH 977:\n",
      "Average epoch loss: 0.016575217130593956\n",
      "Average valid loss 0.06884317100048065\n",
      "EPOCH 978:\n",
      "Average epoch loss: 0.017038727225735784\n",
      "Average valid loss 0.06860142201185226\n",
      "EPOCH 979:\n",
      "Average epoch loss: 0.016891361796297133\n",
      "Average valid loss 0.06762179732322693\n",
      "EPOCH 980:\n",
      "Average epoch loss: 0.012498681619763374\n",
      "Average valid loss 0.06842291355133057\n",
      "EPOCH 981:\n",
      "Average epoch loss: 0.014762528450228274\n",
      "Average valid loss 0.06750833988189697\n",
      "EPOCH 982:\n",
      "Average epoch loss: 0.023731278232298793\n",
      "Average valid loss 0.06674890220165253\n",
      "EPOCH 983:\n",
      "Average epoch loss: 0.0263589343521744\n",
      "Average valid loss 1.036170482635498\n",
      "EPOCH 984:\n",
      "Average epoch loss: 0.02098929286003113\n",
      "Average valid loss 0.07056945562362671\n",
      "EPOCH 985:\n",
      "Average epoch loss: 0.011933550238609314\n",
      "Average valid loss 0.07592588663101196\n",
      "EPOCH 986:\n",
      "Average epoch loss: 0.01823048295918852\n",
      "Average valid loss 0.07482889294624329\n",
      "EPOCH 987:\n",
      "Average epoch loss: 0.006071915419306606\n",
      "Average valid loss 0.06942117214202881\n",
      "EPOCH 988:\n",
      "Average epoch loss: 0.016876697540283203\n",
      "Average valid loss 0.06945347785949707\n",
      "EPOCH 989:\n",
      "Average epoch loss: 0.013739611906930804\n",
      "Average valid loss 0.06790194660425186\n",
      "EPOCH 990:\n",
      "Average epoch loss: 0.010503591084852815\n",
      "Average valid loss 0.06700506061315536\n",
      "EPOCH 991:\n",
      "Average epoch loss: 0.013399553054478019\n",
      "Average valid loss 1.0879851579666138\n",
      "EPOCH 992:\n",
      "Average epoch loss: 0.025787379476241767\n",
      "Average valid loss 1.0340993404388428\n",
      "EPOCH 993:\n",
      "Average epoch loss: 0.010875819949433208\n",
      "Average valid loss 0.06377769261598587\n",
      "EPOCH 994:\n",
      "Average epoch loss: 0.011334051331505179\n",
      "Average valid loss 0.06469109654426575\n",
      "EPOCH 995:\n",
      "Average epoch loss: 0.019320616824552418\n",
      "Average valid loss 0.06744846701622009\n",
      "EPOCH 996:\n",
      "Average epoch loss: 0.01776018722448498\n",
      "Average valid loss 0.06679453700780869\n",
      "EPOCH 997:\n",
      "Average epoch loss: 0.008279070327989756\n",
      "Average valid loss 0.06662598997354507\n",
      "EPOCH 998:\n",
      "Average epoch loss: 0.015139852440916002\n",
      "Average valid loss 0.06634356826543808\n",
      "EPOCH 999:\n",
      "Average epoch loss: 0.010587988840416073\n",
      "Average valid loss 0.06861405074596405\n",
      "EPOCH 1000:\n",
      "Average epoch loss: 0.009407424321398138\n",
      "Average valid loss 0.07010120153427124\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import torch\n",
    "import math\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# torch.set_default_device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# batch = 16 if memory is not enough\n",
    "training_params = {\n",
    "    'collate_fn': collate,\n",
    "    'batch_size': 16,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0}\n",
    "max_epochs = 1000\n",
    "model_dir = \"../data/\"\n",
    "\n",
    "# Generators\n",
    "dataset = TokEnFrDataset(\"../data/eng_-french-nano.csv\", val_ratio=0.1)\n",
    "dataloader = DataLoader(dataset, **training_params)\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=2, d=64, h=4)\n",
    "print(f\"Number of model's params: {sum(p.numel() for p in transformer.parameters())}\")\n",
    "transformer = transformer.to(device)\n",
    "loss_fn = F.cross_entropy\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=3e-5, weight_decay=1e-5)\n",
    "\n",
    "def save_model(name):\n",
    "    model_path = model_dir + 'model_{name}.pt'.format(name)\n",
    "    torch.save(transformer.state_dict(), model_path)\n",
    "    \n",
    "\n",
    "def train_epoch(epoch):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    best_loss = math.inf\n",
    "\n",
    "    transformer.train(True)\n",
    "    dataset.partition = Partition.TRAIN\n",
    "\n",
    "    for i, data in enumerate(dataloader, 1):\n",
    "        enc_x, dec_x, label, enc_mask, dec_mask = data\n",
    "        enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "        # Clear grads\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "        loss = loss_fn(output.view(-1, encoding.n_vocab), label.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            last_loss = running_loss / 1000\n",
    "            print('Average batch loss: {}'.format(last_loss))\n",
    "            running_loss = 0.\n",
    "            if last_loss < best_loss:\n",
    "                save_model(\"intermediate-nano\")\n",
    "\n",
    "        if i % 5000 == 0:\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    print('Average epoch loss: {}'.format(last_loss if last_loss > 0 else running_loss / i))\n",
    "    return last_loss\n",
    "\n",
    "def validate_epoch():\n",
    "    running_vloss = 0.0\n",
    "    transformer.eval()\n",
    "    dataset.partition = Partition.VAL\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(dataloader):\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = vdata\n",
    "            enc_x, dec_x, label, enc_mask, dec_mask = enc_x.to(device), dec_x.to(device), label.to(device), enc_mask.to(device), dec_mask.to(device)\n",
    "            output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "            vloss = loss_fn(output.view(-1, encoding.n_vocab), label.view(-1))\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('Average valid loss {}'.format(avg_vloss))\n",
    "\n",
    "    return avg_vloss\n",
    "\n",
    "def train():\n",
    "    best_val_loss = math.inf\n",
    "    for epoch in range(max_epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        avg_train_loss = train_epoch(epoch)\n",
    "        \n",
    "        avg_val_loss = validate_epoch()\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_model(\"final-nano\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819338a-9df0-465b-b1cf-a0a6e8a59d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((2, 3, 32), requires_grad=True)\n",
    "print(input.view(-1, 32).shape)\n",
    "target = torch.empty((2, 3), dtype=torch.long).random_(32)\n",
    "print(target)\n",
    "loss = F.cross_entropy(input.view(-1, 32), target.view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05248576-1b7f-4994-80fe-c26d2dc1a81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef7bbe-c8a8-493e-8677-49d9c02dbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def build_train_sample(en_str: str, dec_str: str):\n",
    "    en_encoded = encoding.encode(en_str)\n",
    "    dec_encoded = encoding.encode(dec_str)\n",
    "    dec_encoded.append(Tokens.END_NUM.value)\n",
    "    en_sents = []\n",
    "    dec_sents = []\n",
    "    target_sents = []\n",
    "    \n",
    "    for i in range(1, len(dec_encoded)):\n",
    "        dec_sents.append(dec_encoded[:i])\n",
    "        target_sents.append(dec_encoded[1: i + 1])\n",
    "    en_sents.extend([en_encoded] * len(dec_sents))\n",
    "    return list(zip(en_sents, dec_sents, target_sents))\n",
    "\n",
    "build_train_sample('Hi.', 'START Salut!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfee8d-38b9-4abb-b127-5e620ea387f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# def collate_fn\n",
    "\n",
    "dataset.partition = Partition.TRAIN\n",
    "training_generator = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "for i, s in enumerate(training_generator):\n",
    "    print(s)\n",
    "    if i > 2:\n",
    "        break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d5995-4736-4fec-9e51-e3672a9dc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty((2, 3), dtype=torch.long).random_(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0f4c4-a05a-4353-b58d-f50a60157f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(transformer.encoder.layers[0].mhsa.wq.weight.device)\n",
    "print(next(transformer.parameters()).device)\n",
    "print(transformer.decoder.layers[0].mhsa.wq.weight.device)\n",
    "print(next(transformer.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd67961-4fe8-4c69-90ec-f351df169e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62b3c0-a20a-466d-b5fe-05b33071e963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0dbc9-7e82-48b4-9b90-0ddf28b10784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21175a-9751-4413-8cdb-579c52134778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207745b-3455-48f2-a6c1-d6546569344a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbfc3e-aca8-4edb-99e5-025c945d85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da787162-9955-4de5-a82d-60501d62ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partition(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df08f9da-be37-4a19-992e-11b2c6e1ce7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a2346-9e74-4eab-aab3-bd5bb0a0d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.3]]), dim=-1)\n",
    "torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]).argmax(dim=-1)\n",
    "# torch.max(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]), dim=-1)\n",
    "\n",
    "t1 = torch.tensor([[0.1, 0.2]])\n",
    "t2 = torch.tensor([[0.3]])\n",
    "torch.cat((t1, t2), dim=1).tolist()[-1][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683599c0-fd00-40b0-8bb1-4e477a2114a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45e55c-6d60-409b-b26e-0b218224483a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e70b0-a647-455b-bdfb-332d095aafc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082af878-9425-4445-b4b4-592cef79a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba170-623a-486b-8b67-58360bde104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d1a8-69aa-4c60-b3b4-ebb2e31f1c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
