{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d7f736-b9af-48e3-adb5-18ab3536abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmasked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkhokhlush/github/transformer-implementation/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 3])\n",
      "torch.Size([2, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(q, k, v):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh\n",
    "    # q = q.permute(0, 2, 1, 3)\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    print(softmaxed_prod.shape)\n",
    "    # print(softmaxed_prod)\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4, 5])\n",
    "self_attention(x, x, x)\n",
    "self_attention(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v):\n",
    "        # b, t, d\n",
    "        b, t, d = q.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(b, t, self.h, self.dh)\n",
    "        wk = wk.view(b, t, self.h, self.dh)\n",
    "        wv = wv.view(b, t, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention(wq, wk, wv)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(b, self.h, t, self.dh).transpose(1, 2).contiguous().view(b, t, d)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa = MHSA()\n",
    "x = torch.rand(2, 3, 512)\n",
    "mhsa(x, x, x).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abcca093-9e66-4b46-8ec3-aded63ece6f8",
   "metadata": {},
   "source": [
    "class PE1():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # -> d vector\n",
    "    def __call__(self, pos):\n",
    "        pow = torch.pow(10000, torch.arange(0, self.d) / self.d)\n",
    "        return torch.sin(torch.arange(0, self.d) / pow)\n",
    "\n",
    "print(PE1()(1).size()) # torch.Size([512])\n",
    "\n",
    "class PEScalar():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> d vector\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2)\n",
    "        # b = torch.arange(1, 12, 2)\n",
    "        # torch.stack((a, b), dim=1).view(-1)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1)\n",
    "\n",
    "print(PEScalar()(1).size()) # torch.Size([1, 512])\n",
    "\n",
    "class PEVector():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # 1 -> 1 d\n",
    "    # t 1 -> t d\n",
    "    def __call__(self, pos):\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d)\n",
    "\n",
    "print(PEVector()(1).size()) # torch.Size([1, 512])\n",
    "print(PEVector()(torch.arange(3).view(-1, 1)).size()) # torch.Size([3, 512])\n",
    "\n",
    "class PE():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        # a = torch.arange(0, 12, 2).view(-1, 2)\n",
    "        # b = torch.arange(1, 12, 2).view(-1, 2)\n",
    "        # torch.stack((a, b), dim=-1).view(-1, 4)\n",
    "        return torch.stack((sin_p, cos_p), dim=-1).view(-1, t, self.d)\n",
    "\n",
    "print(PE()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEAnotherImpl():\n",
    "    def __init__(self, d: int = 512):\n",
    "        self.d = d\n",
    "    # b t 1 -> b t d\n",
    "    def __call__(self, pos):\n",
    "        b, t, _ = pos.size()\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        max_len = 1024\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d)\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        return pe[:t, :].unsqueeze(0).repeat(b, 1, 1)\n",
    "\n",
    "print(PEAnotherImpl()(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])\n",
    "\n",
    "class PEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        pos = torch.arange(max_len).unsqueeze(1)\n",
    "        print(pos.size())\n",
    "        sin_p = torch.sin(pos / pow_)\n",
    "        cos_p = torch.cos(pos / pow_)\n",
    "        print(sin_p.size())\n",
    "        print(cos_p.size())\n",
    "        pe = torch.stack((sin_p, cos_p), dim=-1).view(-1, self.d) # downside sin, cos don't alternate\n",
    "        print(pe.size())\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size: b, t, d\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PEModule(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])\n",
    "\n",
    "class PositionalEncodingAnnotatedTransformerModule(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncodingAnnotatedTransformer, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        print(position.size())\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        print(div_term.size())\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        print(pe.size())\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.pe[:, : x.size(1)].size())\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(PositionalEncodingAnnotatedTransformerModule(512, 0.1)(torch.arange(6).view(-1, 3, 1)).size()) # torch.Size([2, 3, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a9e6b-8461-4417-ab07-e5fbd79c60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4413f3-ef3e-4d04-baad-f49354915c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PE(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        twoi = torch.arange(0, self.d, 2)\n",
    "        pow_ = torch.pow(10000, twoi / self.d)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        sin_p = torch.sin(position / pow_)\n",
    "        cos_p = torch.cos(position / pow_)\n",
    "        pe = torch.zeros(max_len, self.d, requires_grad=False) # Explicit, register buffer insures requires grad = False\n",
    "        pe[:, 0::2] = sin_p\n",
    "        pe[:, 1::2] = cos_p\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.pe[:, : t, :]\n",
    "        return self.dropout(x)\n",
    "print(PE(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd49c59-e4b6-4b7d-9719-79079f2f40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PEEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int = 512, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.pe = nn.Embedding(max_len, d)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        pos = self.pe(torch.arange(t))\n",
    "        x = x + pos\n",
    "        return self.dropout(x)\n",
    "print(PEEmbed(d=4)(torch.arange(24).view(-1, 3, 4)).size()) # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9c5bf9-3800-4b22-b7d8-8886d4f15b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder without mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d626059-fd88-4a03-8b20-ffbf0115fe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayerWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSA(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayerWithoutMask()\n",
    "x = torch.rand(2, 3, 512)\n",
    "encoder_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851eff3d-4816-4410-9184-f30c157e4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n",
      "torch.Size([2, 8, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderWithoutMask(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayerWithoutMask(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "encoder = EncoderWithoutMask()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddfbf32-dd33-4246-adad-0baa8b6ba5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With masks\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention_masked(q, k, v, mask=None):\n",
    "    # if 3 dim: b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    # or\n",
    "    # prod = torch.einsum(\"btd, bsd -> bts\", q, k)\n",
    "    # if 4 dim: b t h dh:\n",
    "    prod = torch.einsum(\"bthd, bshd -> bhts\", q, k)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "    print(f\"scaled_prod.shape: \\n {scaled_prod.shape}\")\n",
    "    # mask should be in shape to be broadcastable to bhts and lead to masked keys only (last s dim)\n",
    "    if mask is not None:\n",
    "        scaled_prod = scaled_prod.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    print(f\"scaled_prod: \\n {scaled_prod}\")\n",
    "    softmaxed_prod = F.softmax(scaled_prod, dim=-1)\n",
    "    # print(softmaxed_prod.shape)\n",
    "    print(f\"softmaxed_prod: \\n {softmaxed_prod}\")\n",
    "    # swap h and t in v\n",
    "    return softmaxed_prod @ v.permute(0, 2, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eac5e23-ea43-4295-ba24-a7edbe1a1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d78a45-be0c-4d97-9020-4b83e8c499f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.9266, 0.6322, 0.2052, 0.0140],\n",
      "          [0.6271, 0.9049, 0.9586, 0.7029]],\n",
      "\n",
      "         [[0.7098, 0.9483, 0.9307, 0.6353],\n",
      "          [0.6122, 0.3291, 0.2792, 0.1365]],\n",
      "\n",
      "         [[0.8932, 0.4379, 0.8952, 0.9232],\n",
      "          [0.8860, 0.3774, 0.0104, 0.1081]]],\n",
      "\n",
      "\n",
      "        [[[0.5713, 0.8709, 0.4200, 0.3787],\n",
      "          [0.4595, 0.4335, 0.4950, 0.8630]],\n",
      "\n",
      "         [[0.0826, 0.2884, 0.6462, 0.8452],\n",
      "          [0.7802, 0.9643, 0.4328, 0.7577]],\n",
      "\n",
      "         [[0.9636, 0.3908, 0.7333, 0.8799],\n",
      "          [0.6512, 0.2242, 0.6542, 0.6363]]]])\n",
      "mask: \n",
      " tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.]])\n",
      "wrong mask: \n",
      " tensor([[[1., 1., 0.]],\n",
      "\n",
      "        [[1., 0., 0.]]])\n",
      "wrong mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6502, 0.7285,   -inf],\n",
      "          [0.7285, 1.3364,   -inf],\n",
      "          [0.6505, 1.2344,   -inf]],\n",
      "\n",
      "         [[1.3126,   -inf,   -inf],\n",
      "          [0.5226,   -inf,   -inf],\n",
      "          [0.4915,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7023, 0.4449,   -inf],\n",
      "          [0.4449, 0.6109,   -inf],\n",
      "          [0.7660, 0.7049,   -inf]],\n",
      "\n",
      "         [[0.6945,   -inf,   -inf],\n",
      "          [0.8223,   -inf,   -inf],\n",
      "          [0.6347,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4804, 0.5196, 0.0000],\n",
      "          [0.3525, 0.6475, 0.0000],\n",
      "          [0.3580, 0.6420, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5640, 0.4360, 0.0000],\n",
      "          [0.4586, 0.5414, 0.0000],\n",
      "          [0.5153, 0.4847, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "wrong a: \n",
      " tensor([[[[0.8139, 0.7964, 0.5821, 0.3368],\n",
      "          [0.7862, 0.8368, 0.6750, 0.4162],\n",
      "          [0.7874, 0.8351, 0.6710, 0.4128]],\n",
      "\n",
      "         [[0.6271, 0.9049, 0.9586, 0.7029],\n",
      "          [0.6271, 0.9049, 0.9586, 0.7029],\n",
      "          [0.6271, 0.9049, 0.9586, 0.7029]]],\n",
      "\n",
      "\n",
      "        [[[0.3582, 0.6169, 0.5186, 0.5821],\n",
      "          [0.3067, 0.5555, 0.5425, 0.6313],\n",
      "          [0.3344, 0.5885, 0.5296, 0.6048]],\n",
      "\n",
      "         [[0.4595, 0.4335, 0.4950, 0.8630],\n",
      "          [0.4595, 0.4335, 0.4950, 0.8630],\n",
      "          [0.4595, 0.4335, 0.4950, 0.8630]]]])\n",
      "wrong a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "mask: \n",
      " tensor([[[[1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.]]]])\n",
      "mask broadcast: \n",
      " tensor([[[[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [1., 0., 0.],\n",
      "          [1., 0., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6502, 0.7285,   -inf],\n",
      "          [0.7285, 1.3364,   -inf],\n",
      "          [0.6505, 1.2344,   -inf]],\n",
      "\n",
      "         [[1.3126, 0.5226,   -inf],\n",
      "          [0.5226, 0.2898,   -inf],\n",
      "          [0.4915, 0.3421,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7023,   -inf,   -inf],\n",
      "          [0.4449,   -inf,   -inf],\n",
      "          [0.7660,   -inf,   -inf]],\n",
      "\n",
      "         [[0.6945,   -inf,   -inf],\n",
      "          [0.8223,   -inf,   -inf],\n",
      "          [0.6347,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4804, 0.5196, 0.0000],\n",
      "          [0.3525, 0.6475, 0.0000],\n",
      "          [0.3580, 0.6420, 0.0000]],\n",
      "\n",
      "         [[0.6878, 0.3122, 0.0000],\n",
      "          [0.5579, 0.4421, 0.0000],\n",
      "          [0.5373, 0.4627, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.8139, 0.7964, 0.5821, 0.3368],\n",
      "          [0.7862, 0.8368, 0.6750, 0.4162],\n",
      "          [0.7874, 0.8351, 0.6710, 0.4128]],\n",
      "\n",
      "         [[0.6224, 0.7252, 0.7465, 0.5261],\n",
      "          [0.6205, 0.6504, 0.6583, 0.4525],\n",
      "          [0.6202, 0.6385, 0.6442, 0.4408]]],\n",
      "\n",
      "\n",
      "        [[[0.5713, 0.8709, 0.4200, 0.3787],\n",
      "          [0.5713, 0.8709, 0.4200, 0.3787],\n",
      "          [0.5713, 0.8709, 0.4200, 0.3787]],\n",
      "\n",
      "         [[0.4595, 0.4335, 0.4950, 0.8630],\n",
      "          [0.4595, 0.4335, 0.4950, 0.8630],\n",
      "          [0.4595, 0.4335, 0.4950, 0.8630]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# play with mask\n",
    "\n",
    "x = torch.rand([2, 3, 2, 4])\n",
    "print(x)\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "print(f\"mask: \\n {mask}\")\n",
    "# add head dim to make mask broatcastable to q x k.T prod. mask shape 2, 1, 3\n",
    "mask = mask.unsqueeze(1)\n",
    "\n",
    "\n",
    "# mask = mask.permute(0, 2, 1)\n",
    "# is the mask that I need? keys are ignored?\n",
    "print(f\"wrong mask: \\n {mask}\")\n",
    "#  mask = 2 1 3 -> b prepended before broadcasting (1!!!) h (remains since already 2) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"wrong mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"wrong a: \\n {a}\" )\n",
    "print(f\"wrong a.shape: \\n {a.shape}\")\n",
    "# leads to wrong attention since the shape of mask is wrong 2 1 3 \n",
    "\n",
    "# correct mask\n",
    "# mask 2 batches 3 timeseries\n",
    "mask = torch.ones([2, 3])\n",
    "mask[0, 2] = 0\n",
    "mask[1, 2] = 0\n",
    "mask[1, 1] = 0\n",
    "mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "print(f\"mask: \\n {mask}\")\n",
    "#  mask = 2 1 1 3 -> b (remains already 2) h (broadcasted from 1) t (broadcasted from 1) d (remains since already 3) \n",
    "print(f\"mask broadcast: \\n {mask.broadcast_to([2, 2, 3, 3])}\") \n",
    "a = self_attention_masked(x, x, x, mask=mask)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c6a367-2547-4d72-be99-19bbc1023c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: \n",
      " tensor([[[[0.9266, 0.6322, 0.2052, 0.0140],\n",
      "          [0.6271, 0.9049, 0.9586, 0.7029]],\n",
      "\n",
      "         [[0.7098, 0.9483, 0.9307, 0.6353],\n",
      "          [0.6122, 0.3291, 0.2792, 0.1365]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.5713, 0.8709, 0.4200, 0.3787],\n",
      "          [0.4595, 0.4335, 0.4950, 0.8630]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]],\n",
      "\n",
      "         [[  -inf,   -inf,   -inf,   -inf],\n",
      "          [  -inf,   -inf,   -inf,   -inf]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[0.6502, 0.7285,   -inf],\n",
      "          [0.7285, 1.3364,   -inf],\n",
      "          [0.6505, 1.2344,   -inf]],\n",
      "\n",
      "         [[1.3126, 0.5226,   -inf],\n",
      "          [0.5226, 0.2898,   -inf],\n",
      "          [0.4915, 0.3421,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.7023,   -inf,   -inf],\n",
      "          [0.4449,   -inf,   -inf],\n",
      "          [0.7660,   -inf,   -inf]],\n",
      "\n",
      "         [[0.6945,   -inf,   -inf],\n",
      "          [0.8223,   -inf,   -inf],\n",
      "          [0.6347,   -inf,   -inf]]]])\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4804, 0.5196, 0.0000],\n",
      "          [0.3525, 0.6475, 0.0000],\n",
      "          [0.3580, 0.6420, 0.0000]],\n",
      "\n",
      "         [[0.6878, 0.3122, 0.0000],\n",
      "          [0.5579, 0.4421, 0.0000],\n",
      "          [0.5373, 0.4627, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]])\n",
      "a: \n",
      " tensor([[[[0.8139, 0.7964, 0.5821, 0.3368],\n",
      "          [0.7862, 0.8368, 0.6750, 0.4162],\n",
      "          [0.7874, 0.8351, 0.6710, 0.4128]],\n",
      "\n",
      "         [[0.6224, 0.7252, 0.7465, 0.5261],\n",
      "          [0.6205, 0.6504, 0.6583, 0.4525],\n",
      "          [0.6202, 0.6385, 0.6442, 0.4408]]],\n",
      "\n",
      "\n",
      "        [[[0.5713, 0.8709, 0.4200, 0.3787],\n",
      "          [0.5713, 0.8709, 0.4200, 0.3787],\n",
      "          [0.5713, 0.8709, 0.4200, 0.3787]],\n",
      "\n",
      "         [[0.4595, 0.4335, 0.4950, 0.8630],\n",
      "          [0.4595, 0.4335, 0.4950, 0.8630],\n",
      "          [0.4595, 0.4335, 0.4950, 0.8630]]]])\n",
      "a.shape: \n",
      " torch.Size([2, 2, 3, 4])\n",
      "test: \n",
      " tensor([[[0.1885, 0.7659, 0.9220, 0.7652],\n",
      "         [0.1699, 0.7652, 0.6038, 0.6833],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6691, 0.5115, 0.7830, 0.4485],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "test_v: \n",
      " tensor([[[[0.1885, 0.7659],\n",
      "          [0.9220, 0.7652]],\n",
      "\n",
      "         [[0.1699, 0.7652],\n",
      "          [0.6038, 0.6833]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6691, 0.5115],\n",
      "          [0.7830, 0.4485]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_perm: \n",
      " tensor([[[[0.1885, 0.7659],\n",
      "          [0.1699, 0.7652],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.9220, 0.7652],\n",
      "          [0.6038, 0.6833],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6691, 0.5115],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]],\n",
      "\n",
      "         [[0.7830, 0.4485],\n",
      "          [0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]])\n",
      "test_k: \n",
      " tensor([[[2.0438e-01, 7.2422e-01, 9.7887e-02, 8.2096e-02],\n",
      "         [1.2694e-01, 9.1482e-02, 8.6500e-01, 2.5700e-01],\n",
      "         [      -inf,       -inf,       -inf,       -inf]],\n",
      "\n",
      "        [[4.1550e-04, 6.8541e-01, 9.3566e-01, 4.5981e-01],\n",
      "         [      -inf,       -inf,       -inf,       -inf],\n",
      "         [      -inf,       -inf,       -inf,       -inf]]])\n",
      "test_k_view: \n",
      " tensor([[[[2.0438e-01, 7.2422e-01],\n",
      "          [9.7887e-02, 8.2096e-02]],\n",
      "\n",
      "         [[1.2694e-01, 9.1482e-02],\n",
      "          [8.6500e-01, 2.5700e-01]],\n",
      "\n",
      "         [[      -inf,       -inf],\n",
      "          [      -inf,       -inf]]],\n",
      "\n",
      "\n",
      "        [[[4.1550e-04, 6.8541e-01],\n",
      "          [9.3566e-01, 4.5981e-01]],\n",
      "\n",
      "         [[      -inf,       -inf],\n",
      "          [      -inf,       -inf]],\n",
      "\n",
      "         [[      -inf,       -inf],\n",
      "          [      -inf,       -inf]]]])\n",
      "test_k_perm: \n",
      " tensor([[[[2.0438e-01, 7.2422e-01],\n",
      "          [1.2694e-01, 9.1482e-02],\n",
      "          [      -inf,       -inf]],\n",
      "\n",
      "         [[9.7887e-02, 8.2096e-02],\n",
      "          [8.6500e-01, 2.5700e-01],\n",
      "          [      -inf,       -inf]]],\n",
      "\n",
      "\n",
      "        [[[4.1550e-04, 6.8541e-01],\n",
      "          [      -inf,       -inf],\n",
      "          [      -inf,       -inf]],\n",
      "\n",
      "         [[9.3566e-01, 4.5981e-01],\n",
      "          [      -inf,       -inf],\n",
      "          [      -inf,       -inf]]]])\n",
      "q * k: \n",
      " tensor([[[[0.5663, 0.0922,   -inf],\n",
      "          [0.0922, 0.0245,   -inf],\n",
      "          [0.2585, 0.1059,   -inf]],\n",
      "\n",
      "         [[0.0163, 0.1058,   -inf],\n",
      "          [0.1058, 0.8143,   -inf],\n",
      "          [0.1280, 0.9353,   -inf]]],\n",
      "\n",
      "\n",
      "        [[[0.4698,   -inf,   -inf],\n",
      "          [0.3368,   -inf,   -inf],\n",
      "          [0.2209,   -inf,   -inf]],\n",
      "\n",
      "         [[1.0869,   -inf,   -inf],\n",
      "          [0.2636,   -inf,   -inf],\n",
      "          [0.7572,   -inf,   -inf]]]])\n"
     ]
    }
   ],
   "source": [
    "# mask is equal to making keys on masked places 0:\n",
    "# the result in terms of masked symbols is the same\n",
    "k = x.clone()\n",
    "k[0, 2, 0, :] = float(\"-inf\")\n",
    "k[0, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 2, 0, :] = float(\"-inf\")\n",
    "k[1, 1, 0, :] = float(\"-inf\")\n",
    "k[1, 2, 1, :] = float(\"-inf\")\n",
    "k[1, 1, 1, :] = float(\"-inf\")\n",
    "print(f\"k: \\n {k}\")\n",
    "a = self_attention_masked(x, k, x)\n",
    "print(f\"a: \\n {a}\" )\n",
    "print(f\"a.shape: \\n {a.shape}\")\n",
    "# a is the same shape as if mask was applied in q * k:\n",
    "\n",
    "test = torch.rand([2, 3, 4])\n",
    "test[0, 2, :] = 0\n",
    "test[1, 1, :] = 0\n",
    "test[1, 2, :] = 0\n",
    "\n",
    "print(f\"test: \\n {test}\")\n",
    "test_v = test.view(2, 3, 2, 2)\n",
    "print(f\"test_v: \\n {test_v}\")\n",
    "test_perm = test_v.permute(0, 2, 1, 3)\n",
    "print(f\"test_perm: \\n {test_perm}\")\n",
    "\n",
    "# or like that:\n",
    "test_q = torch.rand([2, 3, 4])\n",
    "test_k = test_q.clone()\n",
    "test_k[0, 2, :] = float(\"-inf\")\n",
    "test_k[1, 1, :] = float(\"-inf\")\n",
    "test_k[1, 2, :] = float(\"-inf\")\n",
    "print(f\"test_k: \\n {test_k}\")\n",
    "\n",
    "test_q_view = test_q.view(2, 3, 2, 2)\n",
    "test_k_view = test_k.view(2, 3, 2, 2)\n",
    "print(f\"test_k_view: \\n {test_k_view}\")\n",
    "test_q_perm = test_q_view.permute(0, 2, 1, 3)\n",
    "test_k_perm = test_k_view.permute(0, 2, 1, 3)\n",
    "print(f\"test_k_perm: \\n {test_k_perm}\")\n",
    "print(f\"q * k: \\n {torch.einsum(\"bhtd, bhsd -> bhts\", test_q_perm, test_k_perm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90465ec8-787f-409e-977a-30c566450515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.8365e-01, 5.6209e-01, 7.4922e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [8.8498e-01, 8.9538e-01, 7.0381e-01, 8.4553e-01, 1.0000e+02, 1.0000e+02],\n",
      "        [4.4282e-02, 3.5147e-01, 6.3375e-01, 7.3284e-01, 4.5642e-01, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [3.1932e-01, 7.5472e-01, 7.7778e-01, 3.2418e-01, 6.0775e-01, 1.9553e-01]])\n",
      "tensor([[1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_padding_mask(x, pad_token):\n",
    "    # x: b t shape\n",
    "    mask = torch.ones_like(x)\n",
    "    return mask.masked_fill(x == pad_token, 0)\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -2:] = 100\n",
    "x[2, -1] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "print(build_padding_mask(x, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647bfc41-5717-4c39-92d5-6d1ba132f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def build_causal_mask(x):\n",
    "    # x: b t shape\n",
    "    m = torch.ones_like(x)\n",
    "    return torch.tril(m)\n",
    "x = torch.rand(5, 6)\n",
    "\n",
    "print(build_causal_mask(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6702b0c-11f5-4326-989e-d2b76a77dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.3466e-01, 2.7316e-01, 3.1052e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [7.7956e-01, 8.8344e-01, 7.4748e-01, 4.3664e-01, 9.3578e-01, 1.0000e+02],\n",
      "        [3.7147e-01, 4.3347e-01, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02, 1.0000e+02],\n",
      "        [9.0758e-01, 4.4891e-01, 8.7484e-02, 1.8720e-01, 2.3320e-01, 7.3618e-01]])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def merge_masks(m1, m2):\n",
    "    return m1 * m2\n",
    "\n",
    "x = torch.rand(5, 6)\n",
    "x[0, -3:] = 100\n",
    "x[1, -1] = 100\n",
    "x[2, -4:] = 100\n",
    "x[3, :] = 100\n",
    "print(x)\n",
    "m1 = build_padding_mask(x, 100)\n",
    "m2 = build_causal_mask(x)\n",
    "print(merge_masks(m1, m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c167748-72fb-40b3-9e6d-7755fa12bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def reshape_mask(mask):\n",
    "    # b t -> b 1 1 t (to be broadcastable to b h t t)\n",
    "    return mask.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(reshape_mask(build_causal_mask(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336c48b9-29d0-4f4e-9d67-a12ddc566634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 0.]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 2.3196e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.8277e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.7866e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.9791e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 3.8770e-02,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 2.5436e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.6556e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.2781e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.6666e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.9238e-01,        -inf,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0695e-02,  1.1151e-01,        -inf,        -inf,        -inf],\n",
      "          [-1.1042e-01, -7.2236e-02,        -inf,        -inf,        -inf],\n",
      "          [-4.2115e-02, -1.6024e-02,        -inf,        -inf,        -inf],\n",
      "          [ 4.9692e-02,  9.1426e-02,        -inf,        -inf,        -inf],\n",
      "          [-1.1466e-01, -1.1400e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.3806e-02,  6.2004e-02,        -inf,        -inf,        -inf],\n",
      "          [-5.1086e-03,  5.7176e-02,        -inf,        -inf,        -inf],\n",
      "          [-3.8459e-05,  7.7961e-02,        -inf,        -inf,        -inf],\n",
      "          [-3.7592e-04,  5.9148e-02,        -inf,        -inf,        -inf],\n",
      "          [ 2.9136e-02,  1.2053e-01,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-4.1757e-02, -4.5574e-02, -9.1555e-02,        -inf,        -inf],\n",
      "          [ 8.1953e-03, -3.8631e-03, -6.8211e-02,        -inf,        -inf],\n",
      "          [-1.3914e-01, -1.4208e-01, -2.3183e-01,        -inf,        -inf],\n",
      "          [ 1.0227e-01,  8.2792e-02,  7.4108e-02,        -inf,        -inf],\n",
      "          [ 2.0188e-01,  1.7201e-01,  1.6516e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 2.6396e-01,  1.4920e-01,  1.3611e-01,        -inf,        -inf],\n",
      "          [ 2.5116e-01,  1.5146e-01,  1.4816e-01,        -inf,        -inf],\n",
      "          [ 3.3582e-01,  2.0532e-01,  1.7274e-01,        -inf,        -inf],\n",
      "          [ 2.1188e-01,  1.1516e-01,  1.1386e-01,        -inf,        -inf],\n",
      "          [ 1.6265e-01,  8.4929e-02,  1.1039e-01,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-2.4661e-03, -1.7075e-02, -3.8359e-02,  6.2555e-02,        -inf],\n",
      "          [ 5.4010e-02,  4.9785e-02,  4.6177e-02,  9.0752e-02,        -inf],\n",
      "          [-4.5257e-03, -1.3945e-02, -2.7839e-02,  3.6656e-02,        -inf],\n",
      "          [ 5.2558e-02,  4.1116e-02,  2.8012e-02,  1.1491e-01,        -inf],\n",
      "          [-1.9600e-02, -3.4406e-02, -5.6475e-02,  3.8836e-02,        -inf]],\n",
      "\n",
      "         [[ 8.4964e-02,  1.5961e-01,  1.9770e-01,  1.3397e-01,        -inf],\n",
      "          [ 4.5474e-02,  1.2817e-01,  1.7273e-01,  7.8198e-02,        -inf],\n",
      "          [ 4.8447e-02,  1.4581e-01,  1.9882e-01,  8.4575e-02,        -inf],\n",
      "          [ 6.3631e-02,  1.4594e-01,  1.8375e-01,  1.0706e-01,        -inf],\n",
      "          [ 9.4115e-02,  1.9474e-01,  2.4309e-01,  1.5304e-01,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4848, 0.5152, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4905, 0.5095, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4935, 0.5065, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4896, 0.5104, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4998, 0.5002, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4880, 0.5120, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4844, 0.5156, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4805, 0.5195, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4851, 0.5149, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4772, 0.5228, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3393, 0.3380, 0.3228, 0.0000, 0.0000],\n",
      "          [0.3431, 0.3390, 0.3179, 0.0000, 0.0000],\n",
      "          [0.3438, 0.3428, 0.3134, 0.0000, 0.0000],\n",
      "          [0.3386, 0.3321, 0.3292, 0.0000, 0.0000],\n",
      "          [0.3408, 0.3307, 0.3285, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3608, 0.3217, 0.3175, 0.0000, 0.0000],\n",
      "          [0.3562, 0.3224, 0.3214, 0.0000, 0.0000],\n",
      "          [0.3667, 0.3218, 0.3115, 0.0000, 0.0000],\n",
      "          [0.3553, 0.3226, 0.3221, 0.0000, 0.0000],\n",
      "          [0.3479, 0.3219, 0.3302, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2489, 0.2453, 0.2401, 0.2656, 0.0000],\n",
      "          [0.2484, 0.2474, 0.2465, 0.2577, 0.0000],\n",
      "          [0.2494, 0.2471, 0.2437, 0.2599, 0.0000],\n",
      "          [0.2482, 0.2454, 0.2422, 0.2642, 0.0000],\n",
      "          [0.2494, 0.2458, 0.2404, 0.2644, 0.0000]],\n",
      "\n",
      "         [[0.2355, 0.2537, 0.2636, 0.2473, 0.0000],\n",
      "          [0.2350, 0.2553, 0.2669, 0.2428, 0.0000],\n",
      "          [0.2325, 0.2563, 0.2702, 0.2410, 0.0000],\n",
      "          [0.2349, 0.2550, 0.2648, 0.2453, 0.0000],\n",
      "          [0.2311, 0.2556, 0.2682, 0.2451, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[ 0.3373,  0.4448,  0.6477, -0.5983,  0.5145,  0.1131],\n",
      "         [ 0.3373,  0.4448,  0.6477, -0.5983,  0.5145,  0.1131],\n",
      "         [ 0.3373,  0.4448,  0.6477, -0.5983,  0.5145,  0.1131],\n",
      "         [ 0.3373,  0.4448,  0.6477, -0.5983,  0.5145,  0.1131],\n",
      "         [ 0.3373,  0.4448,  0.6477, -0.5983,  0.5145,  0.1131]],\n",
      "\n",
      "        [[ 0.4350,  0.1820,  0.1951, -0.1092,  0.5913, -0.3516],\n",
      "         [ 0.4358,  0.1824,  0.1952, -0.1098,  0.5914, -0.3522],\n",
      "         [ 0.4366,  0.1828,  0.1952, -0.1102,  0.5916, -0.3526],\n",
      "         [ 0.4357,  0.1823,  0.1952, -0.1097,  0.5913, -0.3521],\n",
      "         [ 0.4375,  0.1832,  0.1953, -0.1109,  0.5916, -0.3533]],\n",
      "\n",
      "        [[ 0.3771,  0.2905,  0.4565, -0.4088,  0.5069, -0.0067],\n",
      "         [ 0.3764,  0.2895,  0.4553, -0.4072,  0.5077, -0.0065],\n",
      "         [ 0.3760,  0.2916,  0.4570, -0.4089,  0.5079, -0.0059],\n",
      "         [ 0.3780,  0.2897,  0.4562, -0.4087,  0.5065, -0.0071],\n",
      "         [ 0.3775,  0.2879,  0.4545, -0.4068,  0.5068, -0.0072]],\n",
      "\n",
      "        [[ 0.3888,  0.2888,  0.3119, -0.2383,  0.6282, -0.1339],\n",
      "         [ 0.3874,  0.2887,  0.3110, -0.2377,  0.6296, -0.1335],\n",
      "         [ 0.3877,  0.2891,  0.3118, -0.2384,  0.6288, -0.1335],\n",
      "         [ 0.3884,  0.2889,  0.3118, -0.2383,  0.6285, -0.1338],\n",
      "         [ 0.3884,  0.2891,  0.3123, -0.2388,  0.6279, -0.1337]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([4, 2, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 2.3196e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.8277e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.7866e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.9791e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 3.8770e-02,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 2.5436e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.6556e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.2781e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.6666e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 2.9238e-01,        -inf,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0695e-02,  1.1151e-01,        -inf,        -inf,        -inf],\n",
      "          [-1.1042e-01, -7.2236e-02,        -inf,        -inf,        -inf],\n",
      "          [-4.2115e-02, -1.6024e-02,        -inf,        -inf,        -inf],\n",
      "          [ 4.9692e-02,  9.1426e-02,        -inf,        -inf,        -inf],\n",
      "          [-1.1466e-01, -1.1400e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.3806e-02,  6.2004e-02,        -inf,        -inf,        -inf],\n",
      "          [-5.1086e-03,  5.7176e-02,        -inf,        -inf,        -inf],\n",
      "          [-3.8459e-05,  7.7961e-02,        -inf,        -inf,        -inf],\n",
      "          [-3.7592e-04,  5.9148e-02,        -inf,        -inf,        -inf],\n",
      "          [ 2.9136e-02,  1.2053e-01,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-4.1757e-02, -4.5574e-02, -9.1555e-02,        -inf,        -inf],\n",
      "          [ 8.1953e-03, -3.8631e-03, -6.8211e-02,        -inf,        -inf],\n",
      "          [-1.3914e-01, -1.4208e-01, -2.3183e-01,        -inf,        -inf],\n",
      "          [ 1.0227e-01,  8.2792e-02,  7.4108e-02,        -inf,        -inf],\n",
      "          [ 2.0188e-01,  1.7201e-01,  1.6516e-01,        -inf,        -inf]],\n",
      "\n",
      "         [[ 2.6396e-01,  1.4920e-01,  1.3611e-01,        -inf,        -inf],\n",
      "          [ 2.5116e-01,  1.5146e-01,  1.4816e-01,        -inf,        -inf],\n",
      "          [ 3.3582e-01,  2.0532e-01,  1.7274e-01,        -inf,        -inf],\n",
      "          [ 2.1188e-01,  1.1516e-01,  1.1386e-01,        -inf,        -inf],\n",
      "          [ 1.6265e-01,  8.4929e-02,  1.1039e-01,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[-2.4661e-03, -1.7075e-02, -3.8359e-02,  6.2555e-02,        -inf],\n",
      "          [ 5.4010e-02,  4.9785e-02,  4.6177e-02,  9.0752e-02,        -inf],\n",
      "          [-4.5257e-03, -1.3945e-02, -2.7839e-02,  3.6656e-02,        -inf],\n",
      "          [ 5.2558e-02,  4.1116e-02,  2.8012e-02,  1.1491e-01,        -inf],\n",
      "          [-1.9600e-02, -3.4406e-02, -5.6475e-02,  3.8836e-02,        -inf]],\n",
      "\n",
      "         [[ 8.4964e-02,  1.5961e-01,  1.9770e-01,  1.3397e-01,        -inf],\n",
      "          [ 4.5474e-02,  1.2817e-01,  1.7273e-01,  7.8198e-02,        -inf],\n",
      "          [ 4.8447e-02,  1.4581e-01,  1.9882e-01,  8.4575e-02,        -inf],\n",
      "          [ 6.3631e-02,  1.4594e-01,  1.8375e-01,  1.0706e-01,        -inf],\n",
      "          [ 9.4115e-02,  1.9474e-01,  2.4309e-01,  1.5304e-01,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4848, 0.5152, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4905, 0.5095, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4935, 0.5065, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4896, 0.5104, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4998, 0.5002, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4880, 0.5120, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4844, 0.5156, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4805, 0.5195, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4851, 0.5149, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4772, 0.5228, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.3393, 0.3380, 0.3228, 0.0000, 0.0000],\n",
      "          [0.3431, 0.3390, 0.3179, 0.0000, 0.0000],\n",
      "          [0.3438, 0.3428, 0.3134, 0.0000, 0.0000],\n",
      "          [0.3386, 0.3321, 0.3292, 0.0000, 0.0000],\n",
      "          [0.3408, 0.3307, 0.3285, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3608, 0.3217, 0.3175, 0.0000, 0.0000],\n",
      "          [0.3562, 0.3224, 0.3214, 0.0000, 0.0000],\n",
      "          [0.3667, 0.3218, 0.3115, 0.0000, 0.0000],\n",
      "          [0.3553, 0.3226, 0.3221, 0.0000, 0.0000],\n",
      "          [0.3479, 0.3219, 0.3302, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2489, 0.2453, 0.2401, 0.2656, 0.0000],\n",
      "          [0.2484, 0.2474, 0.2465, 0.2577, 0.0000],\n",
      "          [0.2494, 0.2471, 0.2437, 0.2599, 0.0000],\n",
      "          [0.2482, 0.2454, 0.2422, 0.2642, 0.0000],\n",
      "          [0.2494, 0.2458, 0.2404, 0.2644, 0.0000]],\n",
      "\n",
      "         [[0.2355, 0.2537, 0.2636, 0.2473, 0.0000],\n",
      "          [0.2350, 0.2553, 0.2669, 0.2428, 0.0000],\n",
      "          [0.2325, 0.2563, 0.2702, 0.2410, 0.0000],\n",
      "          [0.2349, 0.2550, 0.2648, 0.2453, 0.0000],\n",
      "          [0.2311, 0.2556, 0.2682, 0.2451, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MHSAMasked(nn.Module):\n",
    "    def __init__(self, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        assert d % h == 0\n",
    "        self.d = d\n",
    "        self.dh = d // h\n",
    "        self.h = h\n",
    "        self.wq = nn.Linear(self.d, self.d)\n",
    "        self.wk = nn.Linear(self.d, self.d)\n",
    "        self.wv = nn.Linear(self.d, self.d)\n",
    "        self.wo = nn.Linear(self.d, self.d)\n",
    " \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        # q and k/v might be of different sizes if lengths of decoder and encoders inputs are different\n",
    "        bq, tq, dq = q.size()\n",
    "        bk, tk, dk = k.size()\n",
    "        wq = self.wq(q)\n",
    "        wk = self.wk(k)\n",
    "        wv = self.wv(v)\n",
    "        wq = wq.view(bq, tq, self.h, self.dh)\n",
    "        wk = wk.view(bk, tk, self.h, self.dh)\n",
    "        wv = wv.view(bk, tk, self.h, self.dh)\n",
    "        # b, t, h, dh\n",
    "        # if changing from 4 dim -> 3 dim: b*h, t, dh\n",
    "        # wq = wq.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wk = wk.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # wv = wv.permute(0, 2, 1, 3).reshape(b * self.h, t, self.dh)\n",
    "        # another option 4 dim -> 3 dim\n",
    "        # wq = wq.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wk = wk.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # wv = wv.transpose(1, 2).contiguous().view(b * self.h, t, self.dh)\n",
    "        # changing the number of dims is not necessary as @ supports 4 dims\n",
    "        attn = self_attention_masked(wq, wk, wv, mask=mask)\n",
    "        # b * h, t, dh\n",
    "        # attn = attn.view(b, self.h, t, self.dh).permute(0, 2, 1, 3).reshape(b, t, d)\n",
    "        attn = attn.view(bq, self.h, tq, self.dh).transpose(1, 2).contiguous().view(bq, tq, dq)\n",
    "        wo = self.wo(attn)\n",
    "        return wo\n",
    "        # # 1 2 3 4\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # return F.relu(self.conv2(x))\n",
    "\n",
    "mhsa_masked = MHSAMasked(h = 2, d = 6)\n",
    "x = torch.rand(4, 5)\n",
    "mask = reshape_mask(build_causal_mask(x))\n",
    "print(mask)\n",
    "x = torch.rand(4, 5, 6)\n",
    "print(mhsa_masked(x, x, x, mask=mask))\n",
    "print(mhsa_masked(x, x, x, mask=mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729ae6f8-a3f8-4386-837d-f1e67f022b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d0cd6d-12f4-44f0-b8b9-62a18a6d2f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0504,  0.0414,    -inf],\n",
      "          [ 0.0441,  0.1145,    -inf],\n",
      "          [-0.0344, -0.0269,    -inf]],\n",
      "\n",
      "         [[ 0.0800,  0.0021,    -inf],\n",
      "          [ 0.0334, -0.0432,    -inf],\n",
      "          [ 0.0105,  0.0133,    -inf]],\n",
      "\n",
      "         [[ 0.0526,  0.1589,    -inf],\n",
      "          [ 0.0852,  0.2007,    -inf],\n",
      "          [ 0.1412,  0.1630,    -inf]],\n",
      "\n",
      "         [[-0.0495, -0.0794,    -inf],\n",
      "          [-0.0549, -0.0811,    -inf],\n",
      "          [-0.0506, -0.0862,    -inf]],\n",
      "\n",
      "         [[ 0.0612,  0.1193,    -inf],\n",
      "          [ 0.0893,  0.1829,    -inf],\n",
      "          [ 0.0536,  0.1287,    -inf]],\n",
      "\n",
      "         [[ 0.0352,  0.0386,    -inf],\n",
      "          [ 0.0127, -0.1007,    -inf],\n",
      "          [-0.0564, -0.0129,    -inf]],\n",
      "\n",
      "         [[ 0.0632,  0.0636,    -inf],\n",
      "          [ 0.0371,  0.0845,    -inf],\n",
      "          [-0.0301, -0.0152,    -inf]],\n",
      "\n",
      "         [[-0.0849, -0.1491,    -inf],\n",
      "          [-0.1742, -0.1696,    -inf],\n",
      "          [-0.1034, -0.1082,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0917,    -inf,    -inf],\n",
      "          [ 0.1686,    -inf,    -inf],\n",
      "          [ 0.1778,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0732,    -inf,    -inf],\n",
      "          [ 0.0480,    -inf,    -inf],\n",
      "          [ 0.0758,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0369,    -inf,    -inf],\n",
      "          [-0.0379,    -inf,    -inf],\n",
      "          [ 0.0398,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1408,    -inf,    -inf],\n",
      "          [-0.0922,    -inf,    -inf],\n",
      "          [-0.2138,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0060,    -inf,    -inf],\n",
      "          [ 0.0255,    -inf,    -inf],\n",
      "          [-0.1006,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0108,    -inf,    -inf],\n",
      "          [-0.0673,    -inf,    -inf],\n",
      "          [-0.0958,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1604,    -inf,    -inf],\n",
      "          [ 0.1166,    -inf,    -inf],\n",
      "          [ 0.0508,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1511,    -inf,    -inf],\n",
      "          [-0.2369,    -inf,    -inf],\n",
      "          [-0.1918,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5022, 0.4978, 0.0000],\n",
      "          [0.4824, 0.5176, 0.0000],\n",
      "          [0.4981, 0.5019, 0.0000]],\n",
      "\n",
      "         [[0.5195, 0.4805, 0.0000],\n",
      "          [0.5191, 0.4809, 0.0000],\n",
      "          [0.4993, 0.5007, 0.0000]],\n",
      "\n",
      "         [[0.4735, 0.5265, 0.0000],\n",
      "          [0.4712, 0.5288, 0.0000],\n",
      "          [0.4945, 0.5055, 0.0000]],\n",
      "\n",
      "         [[0.5075, 0.4925, 0.0000],\n",
      "          [0.5066, 0.4934, 0.0000],\n",
      "          [0.5089, 0.4911, 0.0000]],\n",
      "\n",
      "         [[0.4855, 0.5145, 0.0000],\n",
      "          [0.4766, 0.5234, 0.0000],\n",
      "          [0.4812, 0.5188, 0.0000]],\n",
      "\n",
      "         [[0.4992, 0.5008, 0.0000],\n",
      "          [0.5283, 0.4717, 0.0000],\n",
      "          [0.4891, 0.5109, 0.0000]],\n",
      "\n",
      "         [[0.4999, 0.5001, 0.0000],\n",
      "          [0.4881, 0.5119, 0.0000],\n",
      "          [0.4963, 0.5037, 0.0000]],\n",
      "\n",
      "         [[0.5160, 0.4840, 0.0000],\n",
      "          [0.4988, 0.5012, 0.0000],\n",
      "          [0.5012, 0.4988, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d, h)\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, self_mask=None):\n",
    "        b, t, d = x.size()\n",
    "        x = x + self.attn_dropout(self.mhsa(x, x, x, mask=self_mask))\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer()\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "x = torch.rand(2, 3, 512)\n",
    "\n",
    "encoder_layer(x, self_mask=self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e48412b-a00c-45e5-829f-441a7df2318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2019,  0.3826,    -inf],\n",
      "          [ 0.4544,  0.1454,    -inf],\n",
      "          [-0.0198, -0.0634,    -inf]],\n",
      "\n",
      "         [[ 0.7710,  1.3631,    -inf],\n",
      "          [ 0.7813,  0.8880,    -inf],\n",
      "          [ 0.8045,  1.1054,    -inf]],\n",
      "\n",
      "         [[-1.3008, -0.0356,    -inf],\n",
      "          [-0.2252, -0.5723,    -inf],\n",
      "          [-0.3166,  0.1743,    -inf]],\n",
      "\n",
      "         [[ 0.5107, -0.9960,    -inf],\n",
      "          [-0.3594, -1.0873,    -inf],\n",
      "          [-0.0424, -0.3940,    -inf]],\n",
      "\n",
      "         [[ 0.1553,  0.4612,    -inf],\n",
      "          [-0.8120,  0.4822,    -inf],\n",
      "          [ 0.6522,  0.2152,    -inf]],\n",
      "\n",
      "         [[ 0.5118, -0.2565,    -inf],\n",
      "          [ 0.7400,  0.4533,    -inf],\n",
      "          [ 1.5635, -0.4713,    -inf]],\n",
      "\n",
      "         [[-0.7378, -0.7039,    -inf],\n",
      "          [-0.1546,  0.4256,    -inf],\n",
      "          [-0.3132, -0.7823,    -inf]],\n",
      "\n",
      "         [[-1.1126, -0.9435,    -inf],\n",
      "          [-0.4985, -0.7442,    -inf],\n",
      "          [-1.2370, -0.3892,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2163,    -inf,    -inf],\n",
      "          [ 0.3293,    -inf,    -inf],\n",
      "          [-0.1809,    -inf,    -inf]],\n",
      "\n",
      "         [[ 1.1983,    -inf,    -inf],\n",
      "          [ 0.3609,    -inf,    -inf],\n",
      "          [ 0.7957,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.7216,    -inf,    -inf],\n",
      "          [ 1.2650,    -inf,    -inf],\n",
      "          [ 0.6302,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6493,    -inf,    -inf],\n",
      "          [ 1.2011,    -inf,    -inf],\n",
      "          [ 0.1679,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5265,    -inf,    -inf],\n",
      "          [ 0.1052,    -inf,    -inf],\n",
      "          [-0.9346,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2810,    -inf,    -inf],\n",
      "          [ 0.1716,    -inf,    -inf],\n",
      "          [ 0.2616,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.6370,    -inf,    -inf],\n",
      "          [-0.0036,    -inf,    -inf],\n",
      "          [ 0.2034,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2090,    -inf,    -inf],\n",
      "          [ 0.0603,    -inf,    -inf],\n",
      "          [-0.9202,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3579, 0.6421, 0.0000],\n",
      "          [0.5766, 0.4234, 0.0000],\n",
      "          [0.5109, 0.4891, 0.0000]],\n",
      "\n",
      "         [[0.3562, 0.6438, 0.0000],\n",
      "          [0.4734, 0.5266, 0.0000],\n",
      "          [0.4253, 0.5747, 0.0000]],\n",
      "\n",
      "         [[0.2201, 0.7799, 0.0000],\n",
      "          [0.5859, 0.4141, 0.0000],\n",
      "          [0.3797, 0.6203, 0.0000]],\n",
      "\n",
      "         [[0.8186, 0.1814, 0.0000],\n",
      "          [0.6744, 0.3256, 0.0000],\n",
      "          [0.5870, 0.4130, 0.0000]],\n",
      "\n",
      "         [[0.4241, 0.5759, 0.0000],\n",
      "          [0.2151, 0.7849, 0.0000],\n",
      "          [0.6075, 0.3925, 0.0000]],\n",
      "\n",
      "         [[0.6832, 0.3168, 0.0000],\n",
      "          [0.5712, 0.4288, 0.0000],\n",
      "          [0.8844, 0.1156, 0.0000]],\n",
      "\n",
      "         [[0.4915, 0.5085, 0.0000],\n",
      "          [0.3589, 0.6411, 0.0000],\n",
      "          [0.6152, 0.3848, 0.0000]],\n",
      "\n",
      "         [[0.4578, 0.5422, 0.0000],\n",
      "          [0.5611, 0.4389, 0.0000],\n",
      "          [0.2999, 0.7001, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.5746, -0.1458,    -inf],\n",
      "          [-0.1839, -0.1588,    -inf],\n",
      "          [ 0.1248,  0.2455,    -inf]],\n",
      "\n",
      "         [[ 0.1352, -0.3349,    -inf],\n",
      "          [-0.0051, -0.0277,    -inf],\n",
      "          [-0.0806,  0.5468,    -inf]],\n",
      "\n",
      "         [[-0.0956, -0.2365,    -inf],\n",
      "          [-0.5474, -0.0350,    -inf],\n",
      "          [ 0.2811, -0.0427,    -inf]],\n",
      "\n",
      "         [[-0.2526, -0.2984,    -inf],\n",
      "          [-0.0576,  0.0613,    -inf],\n",
      "          [-0.0712, -0.3157,    -inf]],\n",
      "\n",
      "         [[ 0.0822,  0.2862,    -inf],\n",
      "          [-0.0750, -0.3955,    -inf],\n",
      "          [-0.0853,  0.4279,    -inf]],\n",
      "\n",
      "         [[ 0.5326,  0.1775,    -inf],\n",
      "          [ 0.4115, -0.3872,    -inf],\n",
      "          [-0.1296, -0.1555,    -inf]],\n",
      "\n",
      "         [[-0.3211, -0.1514,    -inf],\n",
      "          [-0.5768, -0.3286,    -inf],\n",
      "          [-0.1767,  0.6385,    -inf]],\n",
      "\n",
      "         [[-0.1739, -0.1822,    -inf],\n",
      "          [ 0.4505,  0.3724,    -inf],\n",
      "          [-0.0924, -0.0320,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3368,    -inf,    -inf],\n",
      "          [ 0.5383,    -inf,    -inf],\n",
      "          [ 0.3373,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3178,    -inf,    -inf],\n",
      "          [-0.4764,    -inf,    -inf],\n",
      "          [-0.5255,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1507,    -inf,    -inf],\n",
      "          [-0.1339,    -inf,    -inf],\n",
      "          [-0.1973,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0816,    -inf,    -inf],\n",
      "          [ 0.1039,    -inf,    -inf],\n",
      "          [ 0.0344,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0278,    -inf,    -inf],\n",
      "          [ 0.0092,    -inf,    -inf],\n",
      "          [ 0.3177,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2873,    -inf,    -inf],\n",
      "          [-0.1672,    -inf,    -inf],\n",
      "          [-0.0609,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3833,    -inf,    -inf],\n",
      "          [ 0.2146,    -inf,    -inf],\n",
      "          [-0.3774,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2429,    -inf,    -inf],\n",
      "          [ 0.6106,    -inf,    -inf],\n",
      "          [ 0.1237,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3944, 0.6056, 0.0000],\n",
      "          [0.4937, 0.5063, 0.0000],\n",
      "          [0.4699, 0.5301, 0.0000]],\n",
      "\n",
      "         [[0.6154, 0.3846, 0.0000],\n",
      "          [0.5057, 0.4943, 0.0000],\n",
      "          [0.3481, 0.6519, 0.0000]],\n",
      "\n",
      "         [[0.5352, 0.4648, 0.0000],\n",
      "          [0.3746, 0.6254, 0.0000],\n",
      "          [0.5802, 0.4198, 0.0000]],\n",
      "\n",
      "         [[0.5115, 0.4885, 0.0000],\n",
      "          [0.4703, 0.5297, 0.0000],\n",
      "          [0.5608, 0.4392, 0.0000]],\n",
      "\n",
      "         [[0.4492, 0.5508, 0.0000],\n",
      "          [0.5794, 0.4206, 0.0000],\n",
      "          [0.3744, 0.6256, 0.0000]],\n",
      "\n",
      "         [[0.5879, 0.4121, 0.0000],\n",
      "          [0.6897, 0.3103, 0.0000],\n",
      "          [0.5065, 0.4935, 0.0000]],\n",
      "\n",
      "         [[0.4577, 0.5423, 0.0000],\n",
      "          [0.4383, 0.5617, 0.0000],\n",
      "          [0.3068, 0.6932, 0.0000]],\n",
      "\n",
      "         [[0.5021, 0.4979, 0.0000],\n",
      "          [0.5195, 0.4805, 0.0000],\n",
      "          [0.4849, 0.5151, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1921, -0.4116,    -inf],\n",
      "          [-0.2474, -0.6393,    -inf],\n",
      "          [-0.8048, -0.9524,    -inf]],\n",
      "\n",
      "         [[ 0.0747, -0.0420,    -inf],\n",
      "          [ 0.3555, -0.4994,    -inf],\n",
      "          [-0.3964, -0.2539,    -inf]],\n",
      "\n",
      "         [[ 0.0051,  0.1035,    -inf],\n",
      "          [ 0.0700,  0.3051,    -inf],\n",
      "          [ 0.2421,  0.3376,    -inf]],\n",
      "\n",
      "         [[ 0.0393,  0.2824,    -inf],\n",
      "          [-0.3413,  0.1431,    -inf],\n",
      "          [-0.4817, -0.1087,    -inf]],\n",
      "\n",
      "         [[ 0.0714,  0.0967,    -inf],\n",
      "          [-0.2021, -0.0167,    -inf],\n",
      "          [-0.2562, -0.2307,    -inf]],\n",
      "\n",
      "         [[ 0.1639,  0.5496,    -inf],\n",
      "          [-0.2158,  0.2875,    -inf],\n",
      "          [ 0.1223,  0.4170,    -inf]],\n",
      "\n",
      "         [[ 0.1333,  0.0566,    -inf],\n",
      "          [-0.0694, -0.1945,    -inf],\n",
      "          [-0.2085,  0.0172,    -inf]],\n",
      "\n",
      "         [[-0.1372,  0.0295,    -inf],\n",
      "          [-0.1203, -0.2652,    -inf],\n",
      "          [-0.0795, -0.1254,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0167,    -inf,    -inf],\n",
      "          [-0.0416,    -inf,    -inf],\n",
      "          [ 0.0079,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2223,    -inf,    -inf],\n",
      "          [-0.5157,    -inf,    -inf],\n",
      "          [ 0.0612,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0689,    -inf,    -inf],\n",
      "          [-0.3188,    -inf,    -inf],\n",
      "          [ 0.1038,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2826,    -inf,    -inf],\n",
      "          [ 0.1698,    -inf,    -inf],\n",
      "          [-0.1962,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6253,    -inf,    -inf],\n",
      "          [-0.1021,    -inf,    -inf],\n",
      "          [-0.1702,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2047,    -inf,    -inf],\n",
      "          [ 0.4945,    -inf,    -inf],\n",
      "          [ 0.4113,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3807,    -inf,    -inf],\n",
      "          [ 0.3474,    -inf,    -inf],\n",
      "          [ 0.2469,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3912,    -inf,    -inf],\n",
      "          [-0.1421,    -inf,    -inf],\n",
      "          [-0.2375,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5546, 0.4454, 0.0000],\n",
      "          [0.5967, 0.4033, 0.0000],\n",
      "          [0.5368, 0.4632, 0.0000]],\n",
      "\n",
      "         [[0.5291, 0.4709, 0.0000],\n",
      "          [0.7016, 0.2984, 0.0000],\n",
      "          [0.4644, 0.5356, 0.0000]],\n",
      "\n",
      "         [[0.4754, 0.5246, 0.0000],\n",
      "          [0.4415, 0.5585, 0.0000],\n",
      "          [0.4761, 0.5239, 0.0000]],\n",
      "\n",
      "         [[0.4395, 0.5605, 0.0000],\n",
      "          [0.3812, 0.6188, 0.0000],\n",
      "          [0.4078, 0.5922, 0.0000]],\n",
      "\n",
      "         [[0.4937, 0.5063, 0.0000],\n",
      "          [0.4538, 0.5462, 0.0000],\n",
      "          [0.4936, 0.5064, 0.0000]],\n",
      "\n",
      "         [[0.4047, 0.5953, 0.0000],\n",
      "          [0.3768, 0.6232, 0.0000],\n",
      "          [0.4268, 0.5732, 0.0000]],\n",
      "\n",
      "         [[0.5192, 0.4808, 0.0000],\n",
      "          [0.5312, 0.4688, 0.0000],\n",
      "          [0.4438, 0.5562, 0.0000]],\n",
      "\n",
      "         [[0.4584, 0.5416, 0.0000],\n",
      "          [0.5362, 0.4638, 0.0000],\n",
      "          [0.5115, 0.4885, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0937, -0.1985,    -inf],\n",
      "          [ 0.2170,  0.1226,    -inf],\n",
      "          [-0.4370, -0.0987,    -inf]],\n",
      "\n",
      "         [[-0.0165, -0.0577,    -inf],\n",
      "          [ 0.1758,  0.0504,    -inf],\n",
      "          [ 0.4146, -0.0978,    -inf]],\n",
      "\n",
      "         [[-0.1300, -0.1049,    -inf],\n",
      "          [-0.2218,  0.1012,    -inf],\n",
      "          [ 0.1209,  0.0181,    -inf]],\n",
      "\n",
      "         [[ 0.6073,  0.0821,    -inf],\n",
      "          [-0.0784,  0.0733,    -inf],\n",
      "          [ 0.0322,  0.4209,    -inf]],\n",
      "\n",
      "         [[-0.3912, -0.3183,    -inf],\n",
      "          [ 0.4182,  0.1718,    -inf],\n",
      "          [-0.3378, -0.4484,    -inf]],\n",
      "\n",
      "         [[-0.2917, -0.4556,    -inf],\n",
      "          [-0.6167, -0.3380,    -inf],\n",
      "          [-0.3572, -0.2897,    -inf]],\n",
      "\n",
      "         [[-0.3138, -0.2134,    -inf],\n",
      "          [-0.2933, -0.0762,    -inf],\n",
      "          [ 0.1274,  0.2086,    -inf]],\n",
      "\n",
      "         [[ 0.0118,  0.3605,    -inf],\n",
      "          [-0.3083, -0.1196,    -inf],\n",
      "          [ 0.0573,  0.0994,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0312,    -inf,    -inf],\n",
      "          [ 0.1037,    -inf,    -inf],\n",
      "          [-0.0680,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1336,    -inf,    -inf],\n",
      "          [-0.8579,    -inf,    -inf],\n",
      "          [-0.0086,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1449,    -inf,    -inf],\n",
      "          [-0.1084,    -inf,    -inf],\n",
      "          [-0.2626,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1463,    -inf,    -inf],\n",
      "          [-0.1453,    -inf,    -inf],\n",
      "          [ 0.2031,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1257,    -inf,    -inf],\n",
      "          [-0.1117,    -inf,    -inf],\n",
      "          [-0.2236,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0498,    -inf,    -inf],\n",
      "          [-0.0227,    -inf,    -inf],\n",
      "          [-0.2951,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2357,    -inf,    -inf],\n",
      "          [-0.4094,    -inf,    -inf],\n",
      "          [ 0.4611,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0822,    -inf,    -inf],\n",
      "          [-0.1641,    -inf,    -inf],\n",
      "          [ 0.2418,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5725, 0.4275, 0.0000],\n",
      "          [0.5236, 0.4764, 0.0000],\n",
      "          [0.4162, 0.5838, 0.0000]],\n",
      "\n",
      "         [[0.5103, 0.4897, 0.0000],\n",
      "          [0.5313, 0.4687, 0.0000],\n",
      "          [0.6254, 0.3746, 0.0000]],\n",
      "\n",
      "         [[0.4937, 0.5063, 0.0000],\n",
      "          [0.4199, 0.5801, 0.0000],\n",
      "          [0.5257, 0.4743, 0.0000]],\n",
      "\n",
      "         [[0.6284, 0.3716, 0.0000],\n",
      "          [0.4621, 0.5379, 0.0000],\n",
      "          [0.4040, 0.5960, 0.0000]],\n",
      "\n",
      "         [[0.4818, 0.5182, 0.0000],\n",
      "          [0.5613, 0.4387, 0.0000],\n",
      "          [0.5276, 0.4724, 0.0000]],\n",
      "\n",
      "         [[0.5409, 0.4591, 0.0000],\n",
      "          [0.4308, 0.5692, 0.0000],\n",
      "          [0.4831, 0.5169, 0.0000]],\n",
      "\n",
      "         [[0.4749, 0.5251, 0.0000],\n",
      "          [0.4459, 0.5541, 0.0000],\n",
      "          [0.4797, 0.5203, 0.0000]],\n",
      "\n",
      "         [[0.4137, 0.5863, 0.0000],\n",
      "          [0.4530, 0.5470, 0.0000],\n",
      "          [0.4895, 0.5105, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.4717, -0.1153,    -inf],\n",
      "          [ 0.1561,  0.2509,    -inf],\n",
      "          [-0.2640, -0.0278,    -inf]],\n",
      "\n",
      "         [[ 0.6078,  0.4941,    -inf],\n",
      "          [ 1.0131, -0.0628,    -inf],\n",
      "          [ 0.9457, -0.1439,    -inf]],\n",
      "\n",
      "         [[-0.0641, -0.1169,    -inf],\n",
      "          [ 0.0509,  0.0593,    -inf],\n",
      "          [-0.2443, -0.3581,    -inf]],\n",
      "\n",
      "         [[-0.2738, -0.6587,    -inf],\n",
      "          [-0.0433,  0.2026,    -inf],\n",
      "          [-0.0738, -0.1895,    -inf]],\n",
      "\n",
      "         [[-0.1917, -0.7943,    -inf],\n",
      "          [ 0.0643, -0.1242,    -inf],\n",
      "          [ 0.2376,  0.0680,    -inf]],\n",
      "\n",
      "         [[ 0.3345,  0.0764,    -inf],\n",
      "          [ 1.0234,  0.3807,    -inf],\n",
      "          [ 0.4712,  0.3525,    -inf]],\n",
      "\n",
      "         [[-0.1224, -0.2679,    -inf],\n",
      "          [-0.2945, -0.4994,    -inf],\n",
      "          [ 0.2156,  0.1811,    -inf]],\n",
      "\n",
      "         [[ 0.4848,  0.8308,    -inf],\n",
      "          [ 0.1999,  0.5013,    -inf],\n",
      "          [ 0.1543, -0.2100,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1335,    -inf,    -inf],\n",
      "          [-0.1428,    -inf,    -inf],\n",
      "          [-0.1142,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0910,    -inf,    -inf],\n",
      "          [ 0.1431,    -inf,    -inf],\n",
      "          [-0.1866,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6649,    -inf,    -inf],\n",
      "          [-0.1225,    -inf,    -inf],\n",
      "          [-0.1180,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2529,    -inf,    -inf],\n",
      "          [ 0.0355,    -inf,    -inf],\n",
      "          [ 0.6603,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1775,    -inf,    -inf],\n",
      "          [-0.3202,    -inf,    -inf],\n",
      "          [ 0.3287,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1583,    -inf,    -inf],\n",
      "          [ 0.0275,    -inf,    -inf],\n",
      "          [ 0.2837,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3270,    -inf,    -inf],\n",
      "          [ 0.1375,    -inf,    -inf],\n",
      "          [ 0.1083,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1959,    -inf,    -inf],\n",
      "          [ 0.4293,    -inf,    -inf],\n",
      "          [ 0.0826,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4118, 0.5882, 0.0000],\n",
      "          [0.4763, 0.5237, 0.0000],\n",
      "          [0.4412, 0.5588, 0.0000]],\n",
      "\n",
      "         [[0.5284, 0.4716, 0.0000],\n",
      "          [0.7457, 0.2543, 0.0000],\n",
      "          [0.7483, 0.2517, 0.0000]],\n",
      "\n",
      "         [[0.5132, 0.4868, 0.0000],\n",
      "          [0.4979, 0.5021, 0.0000],\n",
      "          [0.5284, 0.4716, 0.0000]],\n",
      "\n",
      "         [[0.5950, 0.4050, 0.0000],\n",
      "          [0.4388, 0.5612, 0.0000],\n",
      "          [0.5289, 0.4711, 0.0000]],\n",
      "\n",
      "         [[0.6462, 0.3538, 0.0000],\n",
      "          [0.5470, 0.4530, 0.0000],\n",
      "          [0.5423, 0.4577, 0.0000]],\n",
      "\n",
      "         [[0.5642, 0.4358, 0.0000],\n",
      "          [0.6554, 0.3446, 0.0000],\n",
      "          [0.5296, 0.4704, 0.0000]],\n",
      "\n",
      "         [[0.5363, 0.4637, 0.0000],\n",
      "          [0.5510, 0.4490, 0.0000],\n",
      "          [0.5086, 0.4914, 0.0000]],\n",
      "\n",
      "         [[0.4144, 0.5856, 0.0000],\n",
      "          [0.4252, 0.5748, 0.0000],\n",
      "          [0.5901, 0.4099, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([2, 8, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0848, -0.3515,    -inf],\n",
      "          [ 0.0335,  0.0278,    -inf],\n",
      "          [-0.0805, -0.2964,    -inf]],\n",
      "\n",
      "         [[ 0.4463, -0.1463,    -inf],\n",
      "          [ 0.6748,  0.2399,    -inf],\n",
      "          [ 0.3416,  0.1127,    -inf]],\n",
      "\n",
      "         [[ 0.2417, -0.0382,    -inf],\n",
      "          [ 0.1796, -0.0881,    -inf],\n",
      "          [ 0.1172, -0.0285,    -inf]],\n",
      "\n",
      "         [[ 0.4076,  0.5155,    -inf],\n",
      "          [ 0.1382,  0.5591,    -inf],\n",
      "          [ 0.0685,  0.1636,    -inf]],\n",
      "\n",
      "         [[-0.1583, -0.1822,    -inf],\n",
      "          [-0.2826, -0.1181,    -inf],\n",
      "          [-0.3337, -0.2415,    -inf]],\n",
      "\n",
      "         [[ 0.2560,  0.2098,    -inf],\n",
      "          [ 0.0472,  0.3891,    -inf],\n",
      "          [ 0.3935,  0.1819,    -inf]],\n",
      "\n",
      "         [[ 0.2628, -0.4117,    -inf],\n",
      "          [ 0.4883,  0.0554,    -inf],\n",
      "          [ 0.5569,  0.2220,    -inf]],\n",
      "\n",
      "         [[-0.3950,  0.1499,    -inf],\n",
      "          [ 0.3084,  0.2523,    -inf],\n",
      "          [ 0.0588,  0.0436,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3476,    -inf,    -inf],\n",
      "          [-0.0349,    -inf,    -inf],\n",
      "          [-0.0360,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3026,    -inf,    -inf],\n",
      "          [-0.3041,    -inf,    -inf],\n",
      "          [-0.1949,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1472,    -inf,    -inf],\n",
      "          [-0.5242,    -inf,    -inf],\n",
      "          [ 0.3283,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0952,    -inf,    -inf],\n",
      "          [ 0.1391,    -inf,    -inf],\n",
      "          [ 0.0542,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1067,    -inf,    -inf],\n",
      "          [-0.1553,    -inf,    -inf],\n",
      "          [-0.0791,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1103,    -inf,    -inf],\n",
      "          [-0.1863,    -inf,    -inf],\n",
      "          [-0.2771,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3470,    -inf,    -inf],\n",
      "          [-0.0991,    -inf,    -inf],\n",
      "          [-0.2265,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1507,    -inf,    -inf],\n",
      "          [ 0.0867,    -inf,    -inf],\n",
      "          [ 0.2670,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5663, 0.4337, 0.0000],\n",
      "          [0.5014, 0.4986, 0.0000],\n",
      "          [0.5537, 0.4463, 0.0000]],\n",
      "\n",
      "         [[0.6440, 0.3560, 0.0000],\n",
      "          [0.6070, 0.3930, 0.0000],\n",
      "          [0.5570, 0.4430, 0.0000]],\n",
      "\n",
      "         [[0.5695, 0.4305, 0.0000],\n",
      "          [0.5665, 0.4335, 0.0000],\n",
      "          [0.5364, 0.4636, 0.0000]],\n",
      "\n",
      "         [[0.4731, 0.5269, 0.0000],\n",
      "          [0.3963, 0.6037, 0.0000],\n",
      "          [0.4762, 0.5238, 0.0000]],\n",
      "\n",
      "         [[0.5060, 0.4940, 0.0000],\n",
      "          [0.4590, 0.5410, 0.0000],\n",
      "          [0.4770, 0.5230, 0.0000]],\n",
      "\n",
      "         [[0.5116, 0.4884, 0.0000],\n",
      "          [0.4153, 0.5847, 0.0000],\n",
      "          [0.5527, 0.4473, 0.0000]],\n",
      "\n",
      "         [[0.6625, 0.3375, 0.0000],\n",
      "          [0.6066, 0.3934, 0.0000],\n",
      "          [0.5829, 0.4171, 0.0000]],\n",
      "\n",
      "         [[0.3671, 0.6329, 0.0000],\n",
      "          [0.5140, 0.4860, 0.0000],\n",
      "          [0.5038, 0.4962, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 512])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [EncoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, x, self_mask = None):\n",
    "        b, t = x.size()\n",
    "        x = self.embed(x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask=self_mask)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "x = torch.randint(0, 2**13, (2, 3))\n",
    "self_mask = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0]]), pad_token=0)\n",
    "self_mask = reshape_mask(self_mask)\n",
    "encoder(x, self_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7785e51c-c88e-4ad2-ad66-4117fbb1a52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0403,    -inf,    -inf],\n",
      "          [ 0.0247,    -inf,    -inf],\n",
      "          [-0.0854,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0748,    -inf,    -inf],\n",
      "          [-0.0820,    -inf,    -inf],\n",
      "          [ 0.1526,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1438,    -inf,    -inf],\n",
      "          [-0.1833,    -inf,    -inf],\n",
      "          [-0.2247,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0661,    -inf,    -inf],\n",
      "          [-0.0514,    -inf,    -inf],\n",
      "          [ 0.0755,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0095, -0.0082,    -inf],\n",
      "          [ 0.0211, -0.0427,    -inf],\n",
      "          [-0.0479, -0.0822,    -inf]],\n",
      "\n",
      "         [[-0.0482, -0.1462,    -inf],\n",
      "          [ 0.1053,  0.0211,    -inf],\n",
      "          [ 0.0307, -0.1188,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5044, 0.4956, 0.0000],\n",
      "          [0.5160, 0.4840, 0.0000],\n",
      "          [0.5086, 0.4914, 0.0000]],\n",
      "\n",
      "         [[0.5245, 0.4755, 0.0000],\n",
      "          [0.5210, 0.4790, 0.0000],\n",
      "          [0.5373, 0.4627, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0513, -0.1447, -0.0351],\n",
      "          [-0.0014,  0.0277,  0.0256],\n",
      "          [-0.0589, -0.1027, -0.0622]],\n",
      "\n",
      "         [[-0.0621,  0.0493,  0.0319],\n",
      "          [-0.2952, -0.2512, -0.0004],\n",
      "          [-0.0148,  0.0786,  0.0472]]],\n",
      "\n",
      "\n",
      "        [[[-0.0554,    -inf,    -inf],\n",
      "          [ 0.2250,    -inf,    -inf],\n",
      "          [-0.1311,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0342,    -inf,    -inf],\n",
      "          [ 0.0711,    -inf,    -inf],\n",
      "          [-0.0527,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0749,  0.0771,    -inf],\n",
      "          [-0.0011,  0.0065,    -inf],\n",
      "          [-0.0379, -0.1287,    -inf]],\n",
      "\n",
      "         [[ 0.1490,  0.1842,    -inf],\n",
      "          [ 0.1390,  0.2376,    -inf],\n",
      "          [ 0.4009,  0.3253,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3416, 0.3112, 0.3472],\n",
      "          [0.3271, 0.3368, 0.3361],\n",
      "          [0.3385, 0.3240, 0.3374]],\n",
      "\n",
      "         [[0.3109, 0.3475, 0.3415],\n",
      "          [0.2952, 0.3085, 0.3964],\n",
      "          [0.3163, 0.3472, 0.3365]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4995, 0.5005, 0.0000],\n",
      "          [0.4981, 0.5019, 0.0000],\n",
      "          [0.5227, 0.4773, 0.0000]],\n",
      "\n",
      "         [[0.4912, 0.5088, 0.0000],\n",
      "          [0.4754, 0.5246, 0.0000],\n",
      "          [0.5189, 0.4811, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self, d: int = 512, h: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mhsa = MHSAMasked(d=d, h=h)\n",
    "        self.attn_norm = nn.LayerNorm(d)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mhca = MHSAMasked(d=d, h=h)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d)\n",
    "        self.cross_attn_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ff1 = nn.Linear(d, d * 4)\n",
    "        self.ff2 = nn.Linear(d * 4, d)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        \n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=None, cross_mask=None):\n",
    "        # self_mask is merged decoders padding and causal masks\n",
    "        # cross_mask is equal to endcoders padding mask because we don't want to attend to encoded padded tokens\n",
    "        b, t, d = dec_x.size()\n",
    "        x = dec_x + self.attn_dropout(self.mhsa(dec_x, dec_x, dec_x, mask=self_mask))\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        x = x + self.cross_attn_dropout(self.mhca(x, enc_x, enc_x, mask=cross_mask))\n",
    "        x = self.cross_attn_norm(x)\n",
    "        \n",
    "        x = x + self.resid_dropout(self.ff2(F.relu(self.ff1(x))))\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "decoder_layer = DecoderLayer(h=2, d=16)\n",
    "x = torch.rand(3, 3, 16)\n",
    "y = torch.rand(3, 3, 16)\n",
    "self_mask1 = build_padding_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "self_mask2 = build_causal_mask(torch.tensor([[2, 2, 0], [2, 0, 0], [2, 2, 0]]))\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "decoder_layer(x, y, self_mask=self_mask, cross_mask=cross_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826a55e4-da2a-42e0-847d-14d0c7774cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_mask: \n",
      " tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "cross_mask: \n",
      " tensor([[[[1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0]]]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-1.0958,    -inf,    -inf],\n",
      "          [-0.4361,    -inf,    -inf],\n",
      "          [-0.8061,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0029,    -inf,    -inf],\n",
      "          [-0.4389,    -inf,    -inf],\n",
      "          [ 0.6587,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0073,    -inf,    -inf],\n",
      "          [-1.2627,    -inf,    -inf],\n",
      "          [-0.5950,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.8364,    -inf,    -inf],\n",
      "          [-0.0556,    -inf,    -inf],\n",
      "          [-0.1172,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.7968, -1.0391,    -inf],\n",
      "          [-0.0377, -1.3665,    -inf],\n",
      "          [-0.2926, -0.8001,    -inf]],\n",
      "\n",
      "         [[-0.0125, -0.9608,    -inf],\n",
      "          [-0.6266, -0.1671,    -inf],\n",
      "          [-1.2908,  0.1687,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5603, 0.4397, 0.0000],\n",
      "          [0.7906, 0.2094, 0.0000],\n",
      "          [0.6242, 0.3758, 0.0000]],\n",
      "\n",
      "         [[0.7208, 0.2792, 0.0000],\n",
      "          [0.3871, 0.6129, 0.0000],\n",
      "          [0.1885, 0.8115, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1087,  0.0006, -0.1106],\n",
      "          [-0.0416, -0.1323, -0.1543],\n",
      "          [ 0.2677,  0.0690, -0.0239]],\n",
      "\n",
      "         [[ 0.0685, -0.0843,  0.0399],\n",
      "          [-0.1796, -0.5381, -0.0098],\n",
      "          [-0.0090, -0.1433,  0.1606]]],\n",
      "\n",
      "\n",
      "        [[[-0.4528,    -inf,    -inf],\n",
      "          [ 0.0524,    -inf,    -inf],\n",
      "          [ 0.1624,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4803,    -inf,    -inf],\n",
      "          [-0.1184,    -inf,    -inf],\n",
      "          [-0.1042,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1592, -0.2249,    -inf],\n",
      "          [ 0.0043, -0.1768,    -inf],\n",
      "          [ 0.2630,  0.1390,    -inf]],\n",
      "\n",
      "         [[-0.1808, -0.3588,    -inf],\n",
      "          [ 0.1147,  0.0145,    -inf],\n",
      "          [-0.1257, -0.1369,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3703, 0.3323, 0.2974],\n",
      "          [0.3563, 0.3254, 0.3183],\n",
      "          [0.3896, 0.3194, 0.2910]],\n",
      "\n",
      "         [[0.3533, 0.3033, 0.3434],\n",
      "          [0.3468, 0.2423, 0.4110],\n",
      "          [0.3269, 0.2858, 0.3873]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5164, 0.4836, 0.0000],\n",
      "          [0.5451, 0.4549, 0.0000],\n",
      "          [0.5310, 0.4690, 0.0000]],\n",
      "\n",
      "         [[0.5444, 0.4556, 0.0000],\n",
      "          [0.5250, 0.4750, 0.0000],\n",
      "          [0.5028, 0.4972, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2397,    -inf,    -inf],\n",
      "          [-0.0507,    -inf,    -inf],\n",
      "          [ 0.1113,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0031,    -inf,    -inf],\n",
      "          [-0.1103,    -inf,    -inf],\n",
      "          [-0.1171,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5051,    -inf,    -inf],\n",
      "          [ 0.1653,    -inf,    -inf],\n",
      "          [ 0.4288,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0169,    -inf,    -inf],\n",
      "          [ 0.3144,    -inf,    -inf],\n",
      "          [ 0.0094,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.1359,  0.1465,    -inf],\n",
      "          [ 0.1754,  0.1240,    -inf],\n",
      "          [-0.0791, -0.0373,    -inf]],\n",
      "\n",
      "         [[-0.6115, -0.4192,    -inf],\n",
      "          [-0.2383, -0.4841,    -inf],\n",
      "          [-0.8294,  0.5797,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4299, 0.5701, 0.0000],\n",
      "          [0.5129, 0.4871, 0.0000],\n",
      "          [0.4895, 0.5105, 0.0000]],\n",
      "\n",
      "         [[0.4521, 0.5479, 0.0000],\n",
      "          [0.5611, 0.4389, 0.0000],\n",
      "          [0.1964, 0.8036, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0311,  0.0031,  0.0405],\n",
      "          [ 0.1390,  0.0411,  0.0991],\n",
      "          [-0.1032, -0.2048, -0.1391]],\n",
      "\n",
      "         [[ 0.0294,  0.0761, -0.1760],\n",
      "          [ 0.0712,  0.1734,  0.1492],\n",
      "          [ 0.0081,  0.0353,  0.1380]]],\n",
      "\n",
      "\n",
      "        [[[-0.0867,    -inf,    -inf],\n",
      "          [-0.3511,    -inf,    -inf],\n",
      "          [-0.4289,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1682,    -inf,    -inf],\n",
      "          [ 0.0546,    -inf,    -inf],\n",
      "          [-0.2336,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0159,  0.1009,    -inf],\n",
      "          [ 0.0428,  0.0587,    -inf],\n",
      "          [-0.1283, -0.0231,    -inf]],\n",
      "\n",
      "         [[-0.2256, -0.0508,    -inf],\n",
      "          [ 0.4281,  0.2221,    -inf],\n",
      "          [-0.0015, -0.0728,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3354, 0.3261, 0.3385],\n",
      "          [0.3487, 0.3162, 0.3351],\n",
      "          [0.3486, 0.3150, 0.3364]],\n",
      "\n",
      "         [[0.3494, 0.3661, 0.2845],\n",
      "          [0.3136, 0.3473, 0.3390],\n",
      "          [0.3158, 0.3245, 0.3596]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4708, 0.5292, 0.0000],\n",
      "          [0.4960, 0.5040, 0.0000],\n",
      "          [0.4737, 0.5263, 0.0000]],\n",
      "\n",
      "         [[0.4564, 0.5436, 0.0000],\n",
      "          [0.5513, 0.4487, 0.0000],\n",
      "          [0.5178, 0.4822, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module): \n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d)\n",
    "        self.pe = PE(d=d)\n",
    "        self.layers = [DecoderLayer(d, h) for _ in range(n)]\n",
    "\n",
    "    def forward(self, dec_x, enc_x, self_mask=self_mask, cross_mask=cross_mask):\n",
    "        b, t = dec_x.size()\n",
    "        x = self.embed(dec_x)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_x, self_mask=self_mask, cross_mask=cross_mask)\n",
    "        return x\n",
    "\n",
    "    def get_embed_weights(self):\n",
    "        return self.embed.weight\n",
    "\n",
    "decoder = Decoder(vocab_size=32, n=2, d=16, h=2)\n",
    "# x = torch.randint(0, 32, (2, 3))\n",
    "x = torch.tensor([[15, 7, 0], [10, 0, 0], [1, 3, 0]])\n",
    "y = torch.rand(3, 3, 16)\n",
    "\n",
    "self_mask1 = build_padding_mask(x, pad_token=0)\n",
    "self_mask2 = build_causal_mask(x)\n",
    "self_mask = merge_masks(self_mask1, self_mask2)\n",
    "print(f\"self_mask: \\n {self_mask}\")\n",
    "self_mask = reshape_mask(self_mask)\n",
    "\n",
    "cross_mask = build_padding_mask(torch.tensor([[2, 2, 2], [2, 0, 0], [2, 2, 0]]), pad_token=0)\n",
    "cross_mask = reshape_mask(cross_mask)\n",
    "print(f\"cross_mask: \\n {cross_mask}\")\n",
    "print(decoder(x, y, self_mask=self_mask, cross_mask=cross_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b5aba42-4209-4fbb-8cc9-a9e8a0236e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Output(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int = 2**13, d: int = 512, ff_weight = None):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(d, vocab_size)\n",
    "        # weight tying with the decoder embedding\n",
    "        if ff_weight is not None:\n",
    "            self.ff.weight = ff_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2477aec-7287-498b-bf31-cbb0bdfe154d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: \n",
      " tensor([[1, 1, 1],\n",
      "        [1, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 1, 1, 0]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.6455,  0.9651, -0.1231],\n",
      "          [ 0.4055,  0.7949,  0.0177],\n",
      "          [-0.2684, -0.1053, -1.1613]],\n",
      "\n",
      "         [[ 0.2812,  0.0461,  0.5877],\n",
      "          [ 0.8348,  0.4874,  0.3025],\n",
      "          [ 0.8979,  0.0184,  0.5143]]],\n",
      "\n",
      "\n",
      "        [[[-1.2137, -1.1024,    -inf],\n",
      "          [-1.0251, -0.8619,    -inf],\n",
      "          [-0.6701, -0.9384,    -inf]],\n",
      "\n",
      "         [[-0.3879, -0.4674,    -inf],\n",
      "          [-0.7635, -0.9031,    -inf],\n",
      "          [-0.8312, -0.8951,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0186,    -inf,    -inf],\n",
      "          [ 0.0660,    -inf,    -inf],\n",
      "          [ 0.2920,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5750,    -inf,    -inf],\n",
      "          [ 0.4815,    -inf,    -inf],\n",
      "          [ 0.2293,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.1300, 0.6508, 0.2192],\n",
      "          [0.3170, 0.4679, 0.2151],\n",
      "          [0.3866, 0.4551, 0.1583]],\n",
      "\n",
      "         [[0.3176, 0.2510, 0.4314],\n",
      "          [0.4360, 0.3080, 0.2560],\n",
      "          [0.4770, 0.1980, 0.3250]]],\n",
      "\n",
      "\n",
      "        [[[0.4722, 0.5278, 0.0000],\n",
      "          [0.4593, 0.5407, 0.0000],\n",
      "          [0.5667, 0.4333, 0.0000]],\n",
      "\n",
      "         [[0.5199, 0.4801, 0.0000],\n",
      "          [0.5348, 0.4652, 0.0000],\n",
      "          [0.5160, 0.4840, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 3, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1233, -0.3125, -0.4898],\n",
      "          [ 0.1035, -0.0753, -0.1098],\n",
      "          [-0.6225, -0.7469, -0.2186]],\n",
      "\n",
      "         [[-0.6091, -0.7185, -0.0508],\n",
      "          [-0.6905, -0.8988, -0.1730],\n",
      "          [-0.3015, -0.3738,  0.2810]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4412,  0.6039,    -inf],\n",
      "          [ 0.4187,  0.4545,    -inf],\n",
      "          [ 0.0335,  0.0439,    -inf]],\n",
      "\n",
      "         [[-0.0407, -0.0838,    -inf],\n",
      "          [ 0.2593,  0.2082,    -inf],\n",
      "          [-0.1498, -0.3180,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3844,    -inf,    -inf],\n",
      "          [ 0.3913,    -inf,    -inf],\n",
      "          [ 0.2692,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5791,    -inf,    -inf],\n",
      "          [-0.5642,    -inf,    -inf],\n",
      "          [-0.4602,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3967, 0.3283, 0.2750],\n",
      "          [0.3782, 0.3163, 0.3056],\n",
      "          [0.2958, 0.2612, 0.4430]],\n",
      "\n",
      "         [[0.2744, 0.2460, 0.4796],\n",
      "          [0.2865, 0.2327, 0.4808],\n",
      "          [0.2688, 0.2500, 0.4812]]],\n",
      "\n",
      "\n",
      "        [[[0.4594, 0.5406, 0.0000],\n",
      "          [0.4910, 0.5090, 0.0000],\n",
      "          [0.4974, 0.5026, 0.0000]],\n",
      "\n",
      "         [[0.5108, 0.4892, 0.0000],\n",
      "          [0.5128, 0.4872, 0.0000],\n",
      "          [0.5420, 0.4580, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.2038,    -inf,    -inf,    -inf],\n",
      "          [ 0.1971,    -inf,    -inf,    -inf],\n",
      "          [-1.2194,    -inf,    -inf,    -inf],\n",
      "          [-1.2211,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0569,    -inf,    -inf,    -inf],\n",
      "          [ 0.1163,    -inf,    -inf,    -inf],\n",
      "          [ 0.8290,    -inf,    -inf,    -inf],\n",
      "          [ 0.8638,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0190,    -inf,    -inf,    -inf],\n",
      "          [ 0.2882,    -inf,    -inf,    -inf],\n",
      "          [ 0.5685,    -inf,    -inf,    -inf],\n",
      "          [ 0.7979,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1731,    -inf,    -inf,    -inf],\n",
      "          [ 0.0739,    -inf,    -inf,    -inf],\n",
      "          [ 0.3439,    -inf,    -inf,    -inf],\n",
      "          [ 0.0665,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-1.0249, -0.4825, -0.9043,    -inf],\n",
      "          [ 0.2843,  0.5472,  0.8511,    -inf],\n",
      "          [ 0.1943,  0.3716,  0.0667,    -inf],\n",
      "          [ 1.0224, -0.7853,  0.0435,    -inf]],\n",
      "\n",
      "         [[ 0.0490, -0.9391,  0.2189,    -inf],\n",
      "          [-0.4544,  0.1581, -0.0226,    -inf],\n",
      "          [ 0.5130,  1.1205, -0.3845,    -inf],\n",
      "          [-1.2629, -0.1677,  0.8966,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2599, 0.4470, 0.2932, 0.0000],\n",
      "          [0.2461, 0.3201, 0.4338, 0.0000],\n",
      "          [0.3253, 0.3884, 0.2863, 0.0000],\n",
      "          [0.6495, 0.1065, 0.2440, 0.0000]],\n",
      "\n",
      "         [[0.3910, 0.1456, 0.4634, 0.0000],\n",
      "          [0.2281, 0.4208, 0.3512, 0.0000],\n",
      "          [0.3083, 0.5660, 0.1257, 0.0000],\n",
      "          [0.0790, 0.2362, 0.6848, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 4, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1547, -0.0316,  0.3296],\n",
      "          [ 0.3662,  0.5437,  0.3767],\n",
      "          [ 0.0050,  0.2398, -0.0119],\n",
      "          [ 0.0168,  0.2554,  0.0357]],\n",
      "\n",
      "         [[-0.1046,  0.2314,  0.1206],\n",
      "          [ 0.3404,  0.3534,  0.2635],\n",
      "          [ 0.2357,  0.2826, -0.1746],\n",
      "          [ 0.2509,  0.3418, -0.0426]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1618,  0.0902,    -inf],\n",
      "          [ 0.6232,  0.6482,    -inf],\n",
      "          [ 0.6468,  0.7036,    -inf],\n",
      "          [ 0.3707,  0.4689,    -inf]],\n",
      "\n",
      "         [[ 0.0072,  0.3508,    -inf],\n",
      "          [ 0.0258,  0.2843,    -inf],\n",
      "          [ 0.1377,  0.2895,    -inf],\n",
      "          [ 0.2692,  0.2853,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8993,    -inf,    -inf],\n",
      "          [-0.0642,    -inf,    -inf],\n",
      "          [ 0.2805,    -inf,    -inf],\n",
      "          [ 0.0323,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.7237,    -inf,    -inf],\n",
      "          [-0.2828,    -inf,    -inf],\n",
      "          [-0.1615,    -inf,    -inf],\n",
      "          [ 0.4706,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.3310, 0.2747, 0.3943],\n",
      "          [0.3120, 0.3726, 0.3153],\n",
      "          [0.3079, 0.3894, 0.3027],\n",
      "          [0.3041, 0.3860, 0.3099]],\n",
      "\n",
      "         [[0.2738, 0.3832, 0.3430],\n",
      "          [0.3402, 0.3447, 0.3151],\n",
      "          [0.3688, 0.3865, 0.2447],\n",
      "          [0.3520, 0.3855, 0.2625]]],\n",
      "\n",
      "\n",
      "        [[[0.5179, 0.4821, 0.0000],\n",
      "          [0.4938, 0.5062, 0.0000],\n",
      "          [0.4858, 0.5142, 0.0000],\n",
      "          [0.4755, 0.5245, 0.0000]],\n",
      "\n",
      "         [[0.4149, 0.5851, 0.0000],\n",
      "          [0.4357, 0.5643, 0.0000],\n",
      "          [0.4621, 0.5379, 0.0000],\n",
      "          [0.4960, 0.5040, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1921,    -inf,    -inf,    -inf],\n",
      "          [ 0.2127,    -inf,    -inf,    -inf],\n",
      "          [ 0.1451,    -inf,    -inf,    -inf],\n",
      "          [ 0.2360,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3606,    -inf,    -inf,    -inf],\n",
      "          [ 0.1135,    -inf,    -inf,    -inf],\n",
      "          [ 0.4429,    -inf,    -inf,    -inf],\n",
      "          [ 0.4445,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1661,    -inf,    -inf,    -inf],\n",
      "          [ 0.0751,    -inf,    -inf,    -inf],\n",
      "          [ 0.3175,    -inf,    -inf,    -inf],\n",
      "          [ 0.2733,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3394,    -inf,    -inf,    -inf],\n",
      "          [ 0.2698,    -inf,    -inf,    -inf],\n",
      "          [ 0.3144,    -inf,    -inf,    -inf],\n",
      "          [ 0.5281,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.6745, -0.3437, -0.2542,    -inf],\n",
      "          [-0.2149,  0.1332, -0.0512,    -inf],\n",
      "          [ 0.5743, -0.1734,  0.7508,    -inf],\n",
      "          [ 0.3241, -0.3172,  0.6303,    -inf]],\n",
      "\n",
      "         [[ 0.5470, -0.0795, -0.0395,    -inf],\n",
      "          [ 0.1455,  0.0565,  0.2281,    -inf],\n",
      "          [-0.4087, -0.2406, -0.1876,    -inf],\n",
      "          [-0.2908, -0.1761, -0.3875,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2555, 0.3556, 0.3889, 0.0000],\n",
      "          [0.2782, 0.3941, 0.3277, 0.0000],\n",
      "          [0.3750, 0.1775, 0.4474, 0.0000],\n",
      "          [0.3466, 0.1825, 0.4708, 0.0000]],\n",
      "\n",
      "         [[0.4783, 0.2556, 0.2661, 0.0000],\n",
      "          [0.3332, 0.3049, 0.3619, 0.0000],\n",
      "          [0.2915, 0.3449, 0.3636, 0.0000],\n",
      "          [0.3301, 0.3702, 0.2997, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 2, 4, 3])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1897, -0.9224, -0.2390],\n",
      "          [-0.2247, -0.3062, -0.1863],\n",
      "          [-0.0108, -0.5095, -0.3570],\n",
      "          [ 0.2440, -0.4030, -0.3339]],\n",
      "\n",
      "         [[ 0.1818,  0.2365, -0.1585],\n",
      "          [-0.0208, -0.3062, -0.2851],\n",
      "          [ 0.0157,  0.2249, -0.0856],\n",
      "          [ 0.0384,  0.1719, -0.0791]]],\n",
      "\n",
      "\n",
      "        [[[-0.0156, -0.2382,    -inf],\n",
      "          [ 0.1764,  0.5618,    -inf],\n",
      "          [ 0.0951,  0.4899,    -inf],\n",
      "          [-0.1251,  0.2148,    -inf]],\n",
      "\n",
      "         [[ 0.4519,  0.1201,    -inf],\n",
      "          [ 0.3709,  0.0733,    -inf],\n",
      "          [ 0.5700,  0.2947,    -inf],\n",
      "          [ 0.2730,  0.2076,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3387,    -inf,    -inf],\n",
      "          [-0.6171,    -inf,    -inf],\n",
      "          [-0.4286,    -inf,    -inf],\n",
      "          [ 0.0869,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.7056,    -inf,    -inf],\n",
      "          [-0.1239,    -inf,    -inf],\n",
      "          [-0.3943,    -inf,    -inf],\n",
      "          [ 0.5024,    -inf,    -inf]]]], grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4111, 0.1976, 0.3913],\n",
      "          [0.3377, 0.3113, 0.3510],\n",
      "          [0.4320, 0.2624, 0.3056],\n",
      "          [0.4797, 0.2512, 0.2691]],\n",
      "\n",
      "         [[0.3613, 0.3816, 0.2571],\n",
      "          [0.3969, 0.2984, 0.3047],\n",
      "          [0.3188, 0.3930, 0.2881],\n",
      "          [0.3298, 0.3769, 0.2933]]],\n",
      "\n",
      "\n",
      "        [[[0.5554, 0.4446, 0.0000],\n",
      "          [0.4048, 0.5952, 0.0000],\n",
      "          [0.4026, 0.5974, 0.0000],\n",
      "          [0.4158, 0.5842, 0.0000]],\n",
      "\n",
      "         [[0.5822, 0.4178, 0.0000],\n",
      "          [0.5739, 0.4261, 0.0000],\n",
      "          [0.5684, 0.4316, 0.0000],\n",
      "          [0.5164, 0.4836, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([3, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 2**13, n: int = 6, d: int = 512, h: int = 8, embed_tying=True):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, n=n, d=d, h=h)\n",
    "        if embed_tying:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d, ff_weight = self.decoder.get_embed_weights())\n",
    "        else:\n",
    "            self.output = Output(vocab_size=vocab_size, d=d)\n",
    "\n",
    "    def forward(self, enc_x, dec_x, enc_mask, dec_mask):\n",
    "        encoded = self.encoder(enc_x, enc_mask)\n",
    "        decoded = self.decoder(dec_x=dec_x, enc_x=encoded, self_mask=dec_mask, cross_mask=enc_mask)\n",
    "        return self.output(decoded)\n",
    "\n",
    "transformer = Transformer(vocab_size=32, n=2, d=16, h=2, embed_tying=False)\n",
    "enc_x = torch.tensor([[15, 7, 3], [10, 10, 0], [1, 0, 0]])\n",
    "dec_x = torch.tensor([[21, 8, 0, 0], [25, 0, 0, 0], [8, 1, 2, 3]])\n",
    "# dec_x = torch.tensor([[21, 8], [25, 0], [8, 1]])\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=0)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=0)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "print(transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5f69bab-e9a5-44a7-af1d-f294f62d3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a06c43eb-c348-4905-9384-1b28378435b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  9906,   4435, 100257, 100257, 100257],\n",
      "        [  2028,    374,    264,   4382,  11914],\n",
      "        [  7979, 100257, 100257, 100257, 100257]])\n",
      "tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "enc_mask: \n",
      " tensor([[1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "dec_mask: \n",
      " tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 0, 0, 0]])\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.1590,  0.2998,    -inf,    -inf,    -inf],\n",
      "          [ 0.0604,  0.5770,    -inf,    -inf,    -inf],\n",
      "          [-0.6017, -0.3135,    -inf,    -inf,    -inf],\n",
      "          [-0.1898, -0.1289,    -inf,    -inf,    -inf],\n",
      "          [-0.6386, -0.3220,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4319, -0.7827,    -inf,    -inf,    -inf],\n",
      "          [ 0.7172, -0.4852,    -inf,    -inf,    -inf],\n",
      "          [-0.2203, -0.3441,    -inf,    -inf,    -inf],\n",
      "          [-0.1797, -0.0738,    -inf,    -inf,    -inf],\n",
      "          [-0.1749, -0.2210,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2723, -0.9000,    -inf,    -inf,    -inf],\n",
      "          [ 0.6739, -0.8510,    -inf,    -inf,    -inf],\n",
      "          [ 0.5669, -0.3341,    -inf,    -inf,    -inf],\n",
      "          [ 0.1414, -0.3725,    -inf,    -inf,    -inf],\n",
      "          [ 0.1359, -0.2045,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2738, -0.0448,    -inf,    -inf,    -inf],\n",
      "          [-0.9779,  0.1545,    -inf,    -inf,    -inf],\n",
      "          [-0.1938,  0.2540,    -inf,    -inf,    -inf],\n",
      "          [ 0.0259,  0.5044,    -inf,    -inf,    -inf],\n",
      "          [ 0.0728,  0.0514,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0993, -0.1519,  0.1201,  0.3743, -0.1144],\n",
      "          [ 0.1836,  0.3844,  0.5131,  0.2374,  1.0382],\n",
      "          [-0.1075, -0.2549, -0.1495, -0.0732,  0.6778],\n",
      "          [-0.0353, -1.2126, -0.5901, -0.5787, -0.2609],\n",
      "          [ 0.0886, -0.7827, -0.7385, -0.2380, -0.6643]],\n",
      "\n",
      "         [[-0.4907,  0.2432, -0.2939, -0.9702, -0.9253],\n",
      "          [-0.1634, -0.3074, -0.7581, -0.1327, -0.8355],\n",
      "          [-0.5655, -0.2562,  0.2263, -0.4327,  0.1723],\n",
      "          [-0.1793, -0.8599, -0.3490, -0.2653,  0.4232],\n",
      "          [-0.1308, -0.4942,  0.2259,  0.1054, -0.2816]],\n",
      "\n",
      "         [[ 0.3362,  0.1447,  0.1152, -0.5745,  0.4620],\n",
      "          [ 0.5432, -0.0727,  0.6608, -0.8954,  0.7044],\n",
      "          [ 0.3737, -0.3794,  1.2687,  0.2697, -0.0018],\n",
      "          [ 0.5936,  0.8690, -0.2030,  0.2326,  0.4743],\n",
      "          [ 0.0156,  0.9497,  0.3593, -0.0533,  0.2534]],\n",
      "\n",
      "         [[-0.4294,  0.6973, -0.3672,  0.2399,  0.0471],\n",
      "          [-0.7681,  0.6810, -0.9113,  0.1942, -0.5965],\n",
      "          [-0.5304, -0.4001, -0.1964, -0.1037,  0.1046],\n",
      "          [-0.3704,  0.6349, -0.6607, -0.0225, -0.2580],\n",
      "          [-0.2715, -0.1630, -0.2629, -0.7852, -0.6096]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0643,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0205,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3718,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.4441,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.5184,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3126,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.7176,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5865,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5428,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5778,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0094,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1022,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2946,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.6137,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2146,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.8287,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.9941,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 1.1466,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 1.1283,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.9110,    -inf,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.4648, 0.5352, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3737, 0.6263, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4285, 0.5715, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4848, 0.5152, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4215, 0.5785, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5868, 0.4132, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7690, 0.2310, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5309, 0.4691, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4735, 0.5265, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5115, 0.4885, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.7636, 0.2364, 0.0000, 0.0000, 0.0000],\n",
      "          [0.8213, 0.1787, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7112, 0.2888, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6257, 0.3743, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5843, 0.4157, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4430, 0.5570, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2437, 0.7563, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3899, 0.6101, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3826, 0.6174, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5054, 0.4946, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1729, 0.1640, 0.2153, 0.2776, 0.1703],\n",
      "          [0.1425, 0.1742, 0.1981, 0.1504, 0.3349],\n",
      "          [0.1653, 0.1426, 0.1585, 0.1711, 0.3625],\n",
      "          [0.3066, 0.0945, 0.1761, 0.1781, 0.2447],\n",
      "          [0.3281, 0.1373, 0.1435, 0.2367, 0.1545]],\n",
      "\n",
      "         [[0.1796, 0.3742, 0.2187, 0.1112, 0.1163],\n",
      "          [0.2526, 0.2187, 0.1393, 0.2604, 0.1290],\n",
      "          [0.1282, 0.1746, 0.2829, 0.1463, 0.2680],\n",
      "          [0.1963, 0.0994, 0.1657, 0.1801, 0.3586],\n",
      "          [0.1905, 0.1324, 0.2721, 0.2412, 0.1638]],\n",
      "\n",
      "         [[0.2402, 0.1983, 0.1925, 0.0966, 0.2724],\n",
      "          [0.2453, 0.1325, 0.2759, 0.0582, 0.2882],\n",
      "          [0.1816, 0.0855, 0.4445, 0.1637, 0.1248],\n",
      "          [0.2297, 0.3026, 0.1036, 0.1601, 0.2039],\n",
      "          [0.1397, 0.3556, 0.1970, 0.1304, 0.1772]],\n",
      "\n",
      "         [[0.1148, 0.3541, 0.1221, 0.2241, 0.1848],\n",
      "          [0.1007, 0.4289, 0.0873, 0.2636, 0.1196],\n",
      "          [0.1438, 0.1638, 0.2008, 0.2203, 0.2713],\n",
      "          [0.1425, 0.3895, 0.1066, 0.2019, 0.1595],\n",
      "          [0.2255, 0.2513, 0.2274, 0.1349, 0.1608]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2453, -0.3805,    -inf,    -inf,    -inf],\n",
      "          [-0.1271,  0.3070,    -inf,    -inf,    -inf],\n",
      "          [-0.5547,  0.1370,    -inf,    -inf,    -inf],\n",
      "          [-0.5449,  0.0238,    -inf,    -inf,    -inf],\n",
      "          [-0.4845, -0.0356,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2859, -0.2461,    -inf,    -inf,    -inf],\n",
      "          [-0.1512, -0.4313,    -inf,    -inf,    -inf],\n",
      "          [-0.6013, -0.5157,    -inf,    -inf,    -inf],\n",
      "          [-0.6971, -0.2699,    -inf,    -inf,    -inf],\n",
      "          [-0.4720, -0.3197,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2961,  0.3166,    -inf,    -inf,    -inf],\n",
      "          [-0.8140,  0.1354,    -inf,    -inf,    -inf],\n",
      "          [-0.0799, -0.2429,    -inf,    -inf,    -inf],\n",
      "          [-0.1004, -0.3481,    -inf,    -inf,    -inf],\n",
      "          [-0.0836, -0.2131,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4823,  0.3749,    -inf,    -inf,    -inf],\n",
      "          [-0.0017, -0.5624,    -inf,    -inf,    -inf],\n",
      "          [ 0.1178,  0.2983,    -inf,    -inf,    -inf],\n",
      "          [ 0.3266,  0.3175,    -inf,    -inf,    -inf],\n",
      "          [ 0.1659,  0.1902,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3768, -0.0868,  0.9074,  0.1638, -0.1522],\n",
      "          [-0.2760, -0.3206,  0.6946, -0.1308, -0.1295],\n",
      "          [-0.3876,  0.0954,  0.1946, -0.2105,  0.0411],\n",
      "          [-0.2649, -0.5369,  0.5064, -0.3429, -0.2055],\n",
      "          [-0.1448, -0.1905,  0.3166,  0.2361, -0.3218]],\n",
      "\n",
      "         [[ 0.0386,  0.0711,  0.1056,  0.2884, -0.5360],\n",
      "          [-0.4202,  0.2399,  0.2781, -0.2441, -0.6730],\n",
      "          [-0.5790, -0.1489, -0.2786, -0.3391, -0.5620],\n",
      "          [ 0.2974,  0.0518, -0.2838,  0.0385,  0.7197],\n",
      "          [ 0.1301,  0.0339,  0.5462,  0.2681,  0.8067]],\n",
      "\n",
      "         [[ 0.1114,  0.1341, -0.6726, -0.4139, -0.7538],\n",
      "          [ 0.0024,  0.3410, -0.0014, -0.0948, -0.1542],\n",
      "          [ 0.4054,  0.0287,  0.1537,  0.1555, -0.0841],\n",
      "          [ 0.2873,  0.1772, -0.0407,  0.4969, -0.3444],\n",
      "          [-0.1201,  0.1489, -0.3565,  0.2185, -0.2857]],\n",
      "\n",
      "         [[ 0.5024, -0.1741,  0.1379,  0.1265, -0.1555],\n",
      "          [-0.0644, -0.0021,  0.3322, -0.2156, -0.0911],\n",
      "          [ 0.0215,  0.0440, -0.3948, -0.0325, -0.0117],\n",
      "          [ 0.0891, -0.2941,  0.1003,  0.1324,  0.3265],\n",
      "          [ 0.2109, -0.5203,  0.4533,  0.7132,  1.0092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3162,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3369,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1286,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0780,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1064,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5398,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5234,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.7659,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5287,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.6076,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0832,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5492,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.6782,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.6910,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.6208,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4082,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1166,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3542,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3277,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1022,    -inf,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5338, 0.4662, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3932, 0.6068, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3336, 0.6664, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3615, 0.6385, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3896, 0.6104, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4900, 0.5100, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5696, 0.4304, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4786, 0.5214, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3948, 0.6052, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4620, 0.5380, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4949, 0.5051, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2790, 0.7210, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5407, 0.4593, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5616, 0.4384, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5323, 0.4677, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5268, 0.4732, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6366, 0.3634, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4550, 0.5450, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5023, 0.4977, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4939, 0.5061, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2116, 0.1331, 0.3597, 0.1710, 0.1247],\n",
      "          [0.1447, 0.1384, 0.3820, 0.1673, 0.1676],\n",
      "          [0.1401, 0.2270, 0.2507, 0.1672, 0.2150],\n",
      "          [0.1692, 0.1289, 0.3659, 0.1565, 0.1795],\n",
      "          [0.1711, 0.1635, 0.2715, 0.2505, 0.1434]],\n",
      "\n",
      "         [[0.2021, 0.2087, 0.2161, 0.2594, 0.1137],\n",
      "          [0.1446, 0.2798, 0.2908, 0.1725, 0.1123],\n",
      "          [0.1619, 0.2489, 0.2187, 0.2058, 0.1647],\n",
      "          [0.2156, 0.1686, 0.1206, 0.1664, 0.3289],\n",
      "          [0.1529, 0.1389, 0.2318, 0.1755, 0.3008]],\n",
      "\n",
      "         [[0.2864, 0.2930, 0.1308, 0.1694, 0.1206],\n",
      "          [0.1937, 0.2718, 0.1930, 0.1758, 0.1656],\n",
      "          [0.2594, 0.1780, 0.2017, 0.2020, 0.1590],\n",
      "          [0.2283, 0.2045, 0.1644, 0.2815, 0.1214],\n",
      "          [0.1870, 0.2447, 0.1476, 0.2623, 0.1584]],\n",
      "\n",
      "         [[0.2935, 0.1492, 0.2038, 0.2015, 0.1520],\n",
      "          [0.1857, 0.1977, 0.2761, 0.1597, 0.1808],\n",
      "          [0.2175, 0.2225, 0.1435, 0.2061, 0.2104],\n",
      "          [0.1998, 0.1362, 0.2020, 0.2086, 0.2533],\n",
      "          [0.1508, 0.0726, 0.1922, 0.2493, 0.3351]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 5, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0525, -0.3796,    -inf,    -inf,    -inf],\n",
      "          [-0.0160, -0.1944,    -inf,    -inf,    -inf],\n",
      "          [-0.3030,  0.1424,    -inf,    -inf,    -inf],\n",
      "          [-0.1029, -0.1155,    -inf,    -inf,    -inf],\n",
      "          [-0.1036, -0.3414,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.6678,  0.2993,    -inf,    -inf,    -inf],\n",
      "          [ 0.6037,  0.4226,    -inf,    -inf,    -inf],\n",
      "          [ 0.4227, -0.2722,    -inf,    -inf,    -inf],\n",
      "          [ 0.2140, -0.3746,    -inf,    -inf,    -inf],\n",
      "          [ 0.2626, -0.4295,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2426,  0.0454,    -inf,    -inf,    -inf],\n",
      "          [-0.3402, -0.2067,    -inf,    -inf,    -inf],\n",
      "          [ 0.2294,  0.1503,    -inf,    -inf,    -inf],\n",
      "          [-0.0103, -0.0759,    -inf,    -inf,    -inf],\n",
      "          [ 0.0723, -0.1226,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1434, -0.5777,    -inf,    -inf,    -inf],\n",
      "          [ 0.3800,  0.0122,    -inf,    -inf,    -inf],\n",
      "          [ 0.7079, -0.0491,    -inf,    -inf,    -inf],\n",
      "          [ 0.5073,  0.1261,    -inf,    -inf,    -inf],\n",
      "          [ 0.4686, -0.0653,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.3463, -0.6793, -0.0934, -0.5858, -0.0506],\n",
      "          [-0.1089, -0.3902,  0.0797,  0.1618, -0.1034],\n",
      "          [-0.5612, -0.1054, -0.4180, -0.3757, -0.4972],\n",
      "          [-0.1355, -0.4412,  0.3395, -0.2521, -0.1585],\n",
      "          [ 0.3420, -0.0336, -0.5154,  0.2961,  0.3242]],\n",
      "\n",
      "         [[ 0.0099, -0.5918,  0.5612,  0.2329, -0.0069],\n",
      "          [-0.3153, -0.1234, -0.2260,  0.1998,  0.0409],\n",
      "          [ 0.1399,  0.2135, -0.6143, -0.2883,  0.2624],\n",
      "          [-0.2820,  0.1620, -0.3803,  0.0165,  0.2414],\n",
      "          [-0.3413, -0.7005, -0.1349,  0.3717,  0.1677]],\n",
      "\n",
      "         [[-0.0904, -0.1936, -0.3140, -0.0833, -0.2063],\n",
      "          [-0.0983, -0.3881,  0.0424, -0.4207,  0.1953],\n",
      "          [ 0.3641, -0.1377,  0.4663,  0.3102,  0.0729],\n",
      "          [ 0.5671, -0.1364,  0.2647,  0.1635,  0.4523],\n",
      "          [-0.0619,  0.4492,  0.0829, -0.0750,  0.3678]],\n",
      "\n",
      "         [[ 0.1031, -0.6267, -0.6966, -0.2375, -0.0408],\n",
      "          [ 0.0569,  0.2967, -0.0829,  0.3276, -0.3128],\n",
      "          [-0.0175, -0.2881, -0.2838, -0.1924, -0.0036],\n",
      "          [ 0.0144,  0.0206, -0.4219, -0.0924, -0.0088],\n",
      "          [-0.6411, -0.4770, -0.4548, -0.4182, -0.2830]]],\n",
      "\n",
      "\n",
      "        [[[-0.0552,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3948,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3980,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3588,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3073,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0856,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3570,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0988,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0912,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1674,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.1987,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0083,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3213,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3199,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1034,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4533,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.5140,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1858,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.2519,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.4645,    -inf,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5811, 0.4189, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5445, 0.4555, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3905, 0.6095, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5031, 0.4969, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5592, 0.4408, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5911, 0.4089, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5452, 0.4548, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6671, 0.3329, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6430, 0.3570, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6664, 0.3336, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4285, 0.5715, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4667, 0.5333, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5198, 0.4802, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5164, 0.4836, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5486, 0.4514, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.6728, 0.3272, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5909, 0.4091, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6807, 0.3193, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5941, 0.4059, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6304, 0.3696, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1947, 0.1396, 0.2507, 0.1533, 0.2617],\n",
      "          [0.1894, 0.1430, 0.2288, 0.2483, 0.1905],\n",
      "          [0.1666, 0.2629, 0.1923, 0.2006, 0.1776],\n",
      "          [0.1919, 0.1413, 0.3085, 0.1708, 0.1875],\n",
      "          [0.2470, 0.1697, 0.1048, 0.2359, 0.2426]],\n",
      "\n",
      "         [[0.1813, 0.0993, 0.3146, 0.2266, 0.1783],\n",
      "          [0.1561, 0.1891, 0.1707, 0.2613, 0.2229],\n",
      "          [0.2310, 0.2487, 0.1087, 0.1506, 0.2611],\n",
      "          [0.1538, 0.2398, 0.1394, 0.2073, 0.2596],\n",
      "          [0.1508, 0.1053, 0.1854, 0.3077, 0.2509]],\n",
      "\n",
      "         [[0.2174, 0.1961, 0.1739, 0.2190, 0.1936],\n",
      "          [0.2014, 0.1507, 0.2318, 0.1459, 0.2701],\n",
      "          [0.2269, 0.1373, 0.2513, 0.2150, 0.1695],\n",
      "          [0.2636, 0.1304, 0.1948, 0.1761, 0.2350],\n",
      "          [0.1576, 0.2627, 0.1821, 0.1555, 0.2421]],\n",
      "\n",
      "         [[0.2850, 0.1374, 0.1281, 0.2027, 0.2468],\n",
      "          [0.1945, 0.2472, 0.1691, 0.2549, 0.1344],\n",
      "          [0.2282, 0.1741, 0.1748, 0.1916, 0.2314],\n",
      "          [0.2208, 0.2222, 0.1427, 0.1985, 0.2158],\n",
      "          [0.1649, 0.1943, 0.1987, 0.2061, 0.2359]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.2844,    -inf,    -inf,    -inf],\n",
      "          [-0.1609,    -inf,    -inf,    -inf],\n",
      "          [ 0.0895,    -inf,    -inf,    -inf],\n",
      "          [ 0.0415,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.7959,    -inf,    -inf,    -inf],\n",
      "          [ 0.2344,    -inf,    -inf,    -inf],\n",
      "          [ 0.3779,    -inf,    -inf,    -inf],\n",
      "          [ 0.2718,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0798,    -inf,    -inf,    -inf],\n",
      "          [ 0.6039,    -inf,    -inf,    -inf],\n",
      "          [ 0.1698,    -inf,    -inf,    -inf],\n",
      "          [ 0.6288,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2089,    -inf,    -inf,    -inf],\n",
      "          [-0.1707,    -inf,    -inf,    -inf],\n",
      "          [-0.4007,    -inf,    -inf,    -inf],\n",
      "          [-0.2450,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0230, -0.2666,    -inf,    -inf],\n",
      "          [-0.1189,  0.1219,    -inf,    -inf],\n",
      "          [-2.0225, -0.9699,    -inf,    -inf],\n",
      "          [-0.0852, -0.4328,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3069, -0.3836,    -inf,    -inf],\n",
      "          [ 0.5220,  0.3501,    -inf,    -inf],\n",
      "          [-0.1506,  0.3059,    -inf,    -inf],\n",
      "          [ 0.5693,  0.9302,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2737, -0.1528,    -inf,    -inf],\n",
      "          [ 0.2508, -0.0769,    -inf,    -inf],\n",
      "          [ 0.1719,  0.4520,    -inf,    -inf],\n",
      "          [-0.1864, -0.5105,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5201, -0.2395,    -inf,    -inf],\n",
      "          [-0.8551, -0.0869,    -inf,    -inf],\n",
      "          [-0.3804, -0.3183,    -inf,    -inf],\n",
      "          [ 0.1226,  0.2595,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4702,    -inf,    -inf,    -inf],\n",
      "          [ 1.2188,    -inf,    -inf,    -inf],\n",
      "          [ 0.4990,    -inf,    -inf,    -inf],\n",
      "          [ 0.6337,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4388,    -inf,    -inf,    -inf],\n",
      "          [-0.0036,    -inf,    -inf,    -inf],\n",
      "          [ 0.0824,    -inf,    -inf,    -inf],\n",
      "          [ 0.2752,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0912,    -inf,    -inf,    -inf],\n",
      "          [-0.4444,    -inf,    -inf,    -inf],\n",
      "          [-0.4027,    -inf,    -inf,    -inf],\n",
      "          [-0.5396,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4848,    -inf,    -inf,    -inf],\n",
      "          [-0.2260,    -inf,    -inf,    -inf],\n",
      "          [-0.5358,    -inf,    -inf,    -inf],\n",
      "          [-0.2831,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5719, 0.4281, 0.0000, 0.0000],\n",
      "          [0.4401, 0.5599, 0.0000, 0.0000],\n",
      "          [0.2587, 0.7413, 0.0000, 0.0000],\n",
      "          [0.5860, 0.4140, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5192, 0.4808, 0.0000, 0.0000],\n",
      "          [0.5429, 0.4571, 0.0000, 0.0000],\n",
      "          [0.3878, 0.6122, 0.0000, 0.0000],\n",
      "          [0.4107, 0.5893, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.6050, 0.3950, 0.0000, 0.0000],\n",
      "          [0.5812, 0.4188, 0.0000, 0.0000],\n",
      "          [0.4304, 0.5696, 0.0000, 0.0000],\n",
      "          [0.5803, 0.4197, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4303, 0.5697, 0.0000, 0.0000],\n",
      "          [0.3169, 0.6831, 0.0000, 0.0000],\n",
      "          [0.4845, 0.5155, 0.0000, 0.0000],\n",
      "          [0.4658, 0.5342, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0086, -0.2113,    -inf,    -inf,    -inf],\n",
      "          [-0.2311, -0.4956,    -inf,    -inf,    -inf],\n",
      "          [-0.0187, -0.3958,    -inf,    -inf,    -inf],\n",
      "          [-0.0741, -0.3267,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0596,  0.1792,    -inf,    -inf,    -inf],\n",
      "          [ 0.2341,  0.2107,    -inf,    -inf,    -inf],\n",
      "          [-0.2207,  0.0090,    -inf,    -inf,    -inf],\n",
      "          [-0.1373,  0.2538,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2528,  0.2200,    -inf,    -inf,    -inf],\n",
      "          [ 0.1518,  0.1431,    -inf,    -inf,    -inf],\n",
      "          [-0.0138,  0.2653,    -inf,    -inf,    -inf],\n",
      "          [ 0.0631,  0.4485,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0180,  0.0569,    -inf,    -inf,    -inf],\n",
      "          [ 0.1150, -0.3960,    -inf,    -inf,    -inf],\n",
      "          [ 0.0578, -0.4070,    -inf,    -inf,    -inf],\n",
      "          [ 0.2237, -0.2635,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1756,  0.7946,  0.2425, -0.3822,  0.2364],\n",
      "          [ 0.4218,  0.2028,  0.3669,  0.1922,  0.8316],\n",
      "          [ 0.3260, -0.3259, -0.0287, -0.1592,  0.1792],\n",
      "          [-0.0063, -0.2105,  0.0867, -0.3426, -0.2621]],\n",
      "\n",
      "         [[ 0.0825, -0.1558, -0.1606,  0.1604,  0.2188],\n",
      "          [-0.0770,  0.0846, -0.3645,  0.3545, -0.0955],\n",
      "          [-0.2520, -0.6104, -0.0562,  0.1584,  0.2606],\n",
      "          [ 0.0777,  0.1712, -0.3225, -0.1828,  0.7666]],\n",
      "\n",
      "         [[-0.0523,  0.0142, -0.3539, -0.0417,  0.0038],\n",
      "          [-0.2436, -0.2847, -0.9509, -0.4938, -0.4558],\n",
      "          [ 0.4517,  0.4166,  0.0287,  0.8619,  0.5762],\n",
      "          [ 0.2406, -0.1593, -0.0460, -0.2649,  0.0405]],\n",
      "\n",
      "         [[-0.4177,  0.0145, -0.0976, -0.6668, -0.4332],\n",
      "          [-0.9031, -0.8343, -0.7147, -0.1461, -0.1509],\n",
      "          [-0.0626, -0.3234, -0.4521, -0.0491,  0.0692],\n",
      "          [-0.3619,  0.2125, -0.5607, -0.4331, -0.1375]]],\n",
      "\n",
      "\n",
      "        [[[-0.2324,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5514,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5786,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5040,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2156,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.1251,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2532,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2182,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.4627,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0742,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0885,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2306,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2755,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5910,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.6284,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.5848,    -inf,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5505, 0.4495, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5658, 0.4342, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5932, 0.4068, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5628, 0.4372, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4701, 0.5299, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5058, 0.4942, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4428, 0.5572, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4035, 0.5965, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3840, 0.6160, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5022, 0.4978, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4307, 0.5693, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4048, 0.5952, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4813, 0.5187, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6250, 0.3750, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6141, 0.3859, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6194, 0.3806, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1798, 0.3339, 0.1922, 0.1029, 0.1911],\n",
      "          [0.1980, 0.1590, 0.1874, 0.1574, 0.2982],\n",
      "          [0.2702, 0.1408, 0.1895, 0.1663, 0.2333],\n",
      "          [0.2272, 0.1852, 0.2493, 0.1623, 0.1759]],\n",
      "\n",
      "         [[0.2084, 0.1642, 0.1634, 0.2252, 0.2388],\n",
      "          [0.1836, 0.2158, 0.1377, 0.2826, 0.1802],\n",
      "          [0.1641, 0.1147, 0.1996, 0.2474, 0.2741],\n",
      "          [0.1808, 0.1985, 0.1212, 0.1394, 0.3601]],\n",
      "\n",
      "         [[0.2050, 0.2192, 0.1517, 0.2072, 0.2169],\n",
      "          [0.2475, 0.2375, 0.1220, 0.1927, 0.2002],\n",
      "          [0.1901, 0.1835, 0.1245, 0.2865, 0.2153],\n",
      "          [0.2602, 0.1744, 0.1954, 0.1570, 0.2130]],\n",
      "\n",
      "         [[0.1760, 0.2711, 0.2424, 0.1372, 0.1733],\n",
      "          [0.1328, 0.1422, 0.1603, 0.2830, 0.2817],\n",
      "          [0.2173, 0.1674, 0.1472, 0.2202, 0.2479],\n",
      "          [0.1730, 0.3073, 0.1419, 0.1612, 0.2166]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.0098,    -inf,    -inf,    -inf],\n",
      "          [ 0.1009,    -inf,    -inf,    -inf],\n",
      "          [ 0.2615,    -inf,    -inf,    -inf],\n",
      "          [ 0.3293,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0589,    -inf,    -inf,    -inf],\n",
      "          [-0.2063,    -inf,    -inf,    -inf],\n",
      "          [-0.2130,    -inf,    -inf,    -inf],\n",
      "          [-0.3574,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2809,    -inf,    -inf,    -inf],\n",
      "          [ 0.6114,    -inf,    -inf,    -inf],\n",
      "          [ 0.9668,    -inf,    -inf,    -inf],\n",
      "          [ 0.6800,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0576,    -inf,    -inf,    -inf],\n",
      "          [ 0.3427,    -inf,    -inf,    -inf],\n",
      "          [-0.0585,    -inf,    -inf,    -inf],\n",
      "          [ 0.0047,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8974,  0.6066,    -inf,    -inf],\n",
      "          [ 0.5865,  0.1269,    -inf,    -inf],\n",
      "          [ 0.2503,  0.2390,    -inf,    -inf],\n",
      "          [ 0.0735, -0.0184,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2189,  0.3624,    -inf,    -inf],\n",
      "          [ 0.2749, -0.0991,    -inf,    -inf],\n",
      "          [-0.0463, -0.2988,    -inf,    -inf],\n",
      "          [ 0.1188,  0.2526,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3551,  0.2189,    -inf,    -inf],\n",
      "          [-0.4753, -0.6392,    -inf,    -inf],\n",
      "          [-0.5626, -0.1013,    -inf,    -inf],\n",
      "          [-0.4783,  0.0461,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3764,  0.9205,    -inf,    -inf],\n",
      "          [-0.0408,  0.5241,    -inf,    -inf],\n",
      "          [-0.1587,  0.4391,    -inf,    -inf],\n",
      "          [-0.1228,  0.4328,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1198,    -inf,    -inf,    -inf],\n",
      "          [ 0.0540,    -inf,    -inf,    -inf],\n",
      "          [-0.0657,    -inf,    -inf,    -inf],\n",
      "          [-0.0160,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2235,    -inf,    -inf,    -inf],\n",
      "          [ 0.4241,    -inf,    -inf,    -inf],\n",
      "          [ 0.3468,    -inf,    -inf,    -inf],\n",
      "          [ 0.0030,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0430,    -inf,    -inf,    -inf],\n",
      "          [ 0.1199,    -inf,    -inf,    -inf],\n",
      "          [-0.1613,    -inf,    -inf,    -inf],\n",
      "          [-0.0962,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0024,    -inf,    -inf,    -inf],\n",
      "          [-0.5053,    -inf,    -inf,    -inf],\n",
      "          [-0.4379,    -inf,    -inf,    -inf],\n",
      "          [ 0.0121,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5722, 0.4278, 0.0000, 0.0000],\n",
      "          [0.6129, 0.3871, 0.0000, 0.0000],\n",
      "          [0.5028, 0.4972, 0.0000, 0.0000],\n",
      "          [0.5230, 0.4770, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4642, 0.5358, 0.0000, 0.0000],\n",
      "          [0.5924, 0.4076, 0.0000, 0.0000],\n",
      "          [0.5628, 0.4372, 0.0000, 0.0000],\n",
      "          [0.4666, 0.5334, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3603, 0.6397, 0.0000, 0.0000],\n",
      "          [0.5409, 0.4591, 0.0000, 0.0000],\n",
      "          [0.3867, 0.6133, 0.0000, 0.0000],\n",
      "          [0.3718, 0.6282, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3672, 0.6328, 0.0000, 0.0000],\n",
      "          [0.3624, 0.6376, 0.0000, 0.0000],\n",
      "          [0.3549, 0.6451, 0.0000, 0.0000],\n",
      "          [0.3646, 0.6354, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[-1.7193e-01, -2.3309e-01,        -inf,        -inf,        -inf],\n",
      "          [-5.5083e-01, -6.6511e-02,        -inf,        -inf,        -inf],\n",
      "          [-2.7649e-01, -1.5989e-01,        -inf,        -inf,        -inf],\n",
      "          [-3.7632e-01, -2.5821e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-7.2816e-01, -7.8541e-03,        -inf,        -inf,        -inf],\n",
      "          [-2.5816e-01,  1.2807e-01,        -inf,        -inf,        -inf],\n",
      "          [-7.2406e-02,  1.6856e-01,        -inf,        -inf,        -inf],\n",
      "          [ 1.4590e-01,  1.1384e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 1.2761e-01,  2.8284e-01,        -inf,        -inf,        -inf],\n",
      "          [-1.1647e-01,  7.2423e-04,        -inf,        -inf,        -inf],\n",
      "          [-9.7150e-03,  1.7733e-01,        -inf,        -inf,        -inf],\n",
      "          [ 8.1941e-02,  1.8999e-01,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 8.7264e-01,  7.7477e-03,        -inf,        -inf,        -inf],\n",
      "          [-7.8656e-02,  6.8374e-02,        -inf,        -inf,        -inf],\n",
      "          [-1.3489e-01,  8.9424e-02,        -inf,        -inf,        -inf],\n",
      "          [-3.2570e-01, -1.7913e-01,        -inf,        -inf,        -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3686e-01,  1.7278e-01, -9.8571e-02,  5.7513e-01,  2.4391e-01],\n",
      "          [-1.5192e-01, -1.1562e-02,  4.5128e-01,  1.4997e-01, -3.7851e-01],\n",
      "          [ 4.0175e-01,  1.4952e-01,  6.1567e-01,  6.9463e-01, -2.9636e-01],\n",
      "          [ 2.2348e-01,  9.5466e-02,  1.3470e-01,  5.9030e-01,  3.4616e-01]],\n",
      "\n",
      "         [[-4.1052e-01, -6.0534e-02, -2.4789e-01, -4.6111e-02, -3.0211e-01],\n",
      "          [-3.6598e-01,  3.4749e-02, -2.4454e-02, -2.2547e-02, -1.8330e-01],\n",
      "          [-5.9017e-01, -2.1836e-01, -2.1125e-01,  1.1517e-01, -3.7121e-01],\n",
      "          [ 1.6617e-01, -6.4703e-02, -3.4157e-02,  1.0122e-01, -5.3413e-01]],\n",
      "\n",
      "         [[ 7.5326e-02,  3.1619e-01,  3.1140e-01, -4.1690e-01, -6.1822e-01],\n",
      "          [-8.8679e-02,  2.5534e-01, -1.3540e-02, -1.2119e-01, -1.0964e-01],\n",
      "          [-1.1509e-01,  1.2691e-01,  1.6817e-01, -8.3273e-01, -2.3158e-01],\n",
      "          [-2.6296e-01, -1.4908e-02,  2.3697e-01, -4.8697e-01,  1.3749e-01]],\n",
      "\n",
      "         [[ 3.3386e-01,  5.3953e-01, -4.1319e-01, -1.6315e-01,  2.6540e-01],\n",
      "          [-9.6062e-02,  3.6644e-01,  3.0284e-01,  2.6093e-01, -2.0581e-01],\n",
      "          [ 3.4827e-01,  1.9986e-01,  3.7598e-02, -2.7172e-01, -3.3607e-01],\n",
      "          [ 1.8315e-01,  6.3504e-03, -4.4032e-01, -2.3130e-01, -3.3845e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1503e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 5.0246e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-1.6878e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.3488e-02,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[-5.4628e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-3.7801e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-7.2446e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-3.3019e-02,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 2.6511e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-7.2772e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [ 1.3340e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-6.7687e-03,        -inf,        -inf,        -inf,        -inf]],\n",
      "\n",
      "         [[ 3.5102e-02,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-6.5916e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-6.5276e-01,        -inf,        -inf,        -inf,        -inf],\n",
      "          [-5.4631e-01,        -inf,        -inf,        -inf,        -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5153, 0.4847, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3812, 0.6188, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4709, 0.5291, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4705, 0.5295, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3273, 0.6727, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4046, 0.5954, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4400, 0.5600, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5080, 0.4920, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4613, 0.5387, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4707, 0.5293, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4534, 0.5466, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4730, 0.5270, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.7037, 0.2963, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4633, 0.5367, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4442, 0.5558, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4634, 0.5366, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2686, 0.1689, 0.1287, 0.2525, 0.1813],\n",
      "          [0.1632, 0.1878, 0.2983, 0.2207, 0.1301],\n",
      "          [0.2060, 0.1601, 0.2552, 0.2762, 0.1025],\n",
      "          [0.1863, 0.1639, 0.1704, 0.2688, 0.2106]],\n",
      "\n",
      "         [[0.1626, 0.2307, 0.1913, 0.2341, 0.1812],\n",
      "          [0.1536, 0.2293, 0.2161, 0.2165, 0.1844],\n",
      "          [0.1393, 0.2020, 0.2034, 0.2820, 0.1734],\n",
      "          [0.2472, 0.1962, 0.2023, 0.2316, 0.1227]],\n",
      "\n",
      "         [[0.2151, 0.2736, 0.2723, 0.1315, 0.1075],\n",
      "          [0.1840, 0.2595, 0.1983, 0.1781, 0.1801],\n",
      "          [0.2008, 0.2558, 0.2666, 0.0980, 0.1788],\n",
      "          [0.1607, 0.2060, 0.2650, 0.1285, 0.2399]],\n",
      "\n",
      "         [[0.2356, 0.2894, 0.1116, 0.1433, 0.2200],\n",
      "          [0.1562, 0.2480, 0.2327, 0.2232, 0.1399],\n",
      "          [0.2749, 0.2370, 0.2015, 0.1479, 0.1387],\n",
      "          [0.2756, 0.2309, 0.1478, 0.1821, 0.1636]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 4])\n",
      "scaled_prod: \n",
      " tensor([[[[-0.1758,    -inf,    -inf,    -inf],\n",
      "          [ 0.1990,    -inf,    -inf,    -inf],\n",
      "          [-0.0748,    -inf,    -inf,    -inf],\n",
      "          [-0.0883,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.2265,    -inf,    -inf,    -inf],\n",
      "          [ 0.1404,    -inf,    -inf,    -inf],\n",
      "          [ 0.2271,    -inf,    -inf,    -inf],\n",
      "          [ 0.2006,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0887,    -inf,    -inf,    -inf],\n",
      "          [-0.0033,    -inf,    -inf,    -inf],\n",
      "          [-0.2130,    -inf,    -inf,    -inf],\n",
      "          [-0.0215,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.3743,    -inf,    -inf,    -inf],\n",
      "          [ 0.0216,    -inf,    -inf,    -inf],\n",
      "          [-0.1081,    -inf,    -inf,    -inf],\n",
      "          [ 0.1073,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0744,  0.3021,    -inf,    -inf],\n",
      "          [-0.0826, -0.2639,    -inf,    -inf],\n",
      "          [-0.0272, -0.3524,    -inf,    -inf],\n",
      "          [-0.1579, -0.4498,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0961, -0.4372,    -inf,    -inf],\n",
      "          [ 0.1728,  0.0225,    -inf,    -inf],\n",
      "          [-0.2445, -0.2161,    -inf,    -inf],\n",
      "          [-0.5663, -0.6124,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0519, -0.1519,    -inf,    -inf],\n",
      "          [ 0.0818,  0.2438,    -inf,    -inf],\n",
      "          [-0.0304, -0.0124,    -inf,    -inf],\n",
      "          [ 0.0870, -0.2665,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5101, -0.5657,    -inf,    -inf],\n",
      "          [-0.7679, -0.2359,    -inf,    -inf],\n",
      "          [-0.2330,  0.0335,    -inf,    -inf],\n",
      "          [-0.0468,  0.0626,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2027,    -inf,    -inf,    -inf],\n",
      "          [ 0.1168,    -inf,    -inf,    -inf],\n",
      "          [ 0.1484,    -inf,    -inf,    -inf],\n",
      "          [ 0.3254,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.5748,    -inf,    -inf,    -inf],\n",
      "          [-0.4777,    -inf,    -inf,    -inf],\n",
      "          [-0.4532,    -inf,    -inf,    -inf],\n",
      "          [-0.4913,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2694,    -inf,    -inf,    -inf],\n",
      "          [ 0.0902,    -inf,    -inf,    -inf],\n",
      "          [ 0.1706,    -inf,    -inf,    -inf],\n",
      "          [ 0.1402,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.0137,    -inf,    -inf,    -inf],\n",
      "          [-0.0823,    -inf,    -inf,    -inf],\n",
      "          [-0.1237,    -inf,    -inf,    -inf],\n",
      "          [ 0.0959,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4070, 0.5930, 0.0000, 0.0000],\n",
      "          [0.5452, 0.4548, 0.0000, 0.0000],\n",
      "          [0.5806, 0.4194, 0.0000, 0.0000],\n",
      "          [0.5724, 0.4276, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5845, 0.4155, 0.0000, 0.0000],\n",
      "          [0.5375, 0.4625, 0.0000, 0.0000],\n",
      "          [0.4929, 0.5071, 0.0000, 0.0000],\n",
      "          [0.5115, 0.4885, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5250, 0.4750, 0.0000, 0.0000],\n",
      "          [0.4596, 0.5404, 0.0000, 0.0000],\n",
      "          [0.4955, 0.5045, 0.0000, 0.0000],\n",
      "          [0.5875, 0.4125, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5139, 0.4861, 0.0000, 0.0000],\n",
      "          [0.3701, 0.6299, 0.0000, 0.0000],\n",
      "          [0.4338, 0.5662, 0.0000, 0.0000],\n",
      "          [0.4727, 0.5273, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n",
      "scaled_prod.shape: \n",
      " torch.Size([3, 4, 4, 5])\n",
      "scaled_prod: \n",
      " tensor([[[[ 0.0968,  0.0139,    -inf,    -inf,    -inf],\n",
      "          [-0.2174, -0.2924,    -inf,    -inf,    -inf],\n",
      "          [ 0.0986, -0.1083,    -inf,    -inf,    -inf],\n",
      "          [-0.1174, -0.1020,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.1086,  1.0822,    -inf,    -inf,    -inf],\n",
      "          [ 0.1062,  0.8121,    -inf,    -inf,    -inf],\n",
      "          [-0.0783,  0.6405,    -inf,    -inf,    -inf],\n",
      "          [ 0.0323,  0.6243,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.0710,  0.2181,    -inf,    -inf,    -inf],\n",
      "          [ 0.3100,  0.3284,    -inf,    -inf,    -inf],\n",
      "          [ 0.2811,  0.4238,    -inf,    -inf,    -inf],\n",
      "          [ 0.2972,  0.3464,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.2853,  0.1939,    -inf,    -inf,    -inf],\n",
      "          [-0.1470, -0.0697,    -inf,    -inf,    -inf],\n",
      "          [ 0.0046,  0.0710,    -inf,    -inf,    -inf],\n",
      "          [-0.2873, -0.0359,    -inf,    -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[-0.0065,  0.1025,  0.0553, -0.0367,  0.9098],\n",
      "          [-0.5435, -0.4175, -0.7882, -0.7409, -0.2269],\n",
      "          [-0.2694, -0.3687,  0.0232, -0.8233,  0.1419],\n",
      "          [-0.9947, -0.4612, -0.1324, -0.7467, -0.3797]],\n",
      "\n",
      "         [[ 0.0437, -0.2037, -0.3604,  0.1243,  0.2131],\n",
      "          [ 0.4649,  0.0310,  0.2418,  0.3769,  0.1276],\n",
      "          [ 0.0645,  0.1077, -0.8215, -0.0551, -0.1383],\n",
      "          [-0.2087, -0.0921, -0.0974, -0.0798, -0.1221]],\n",
      "\n",
      "         [[ 0.0803,  0.0232, -0.1387, -0.0692,  0.4649],\n",
      "          [ 0.1069,  0.1340, -0.0220, -0.0296,  0.1435],\n",
      "          [ 0.1594,  0.0414,  0.9376,  0.5501,  0.0132],\n",
      "          [ 0.5940,  0.0805,  0.7407, -0.4095,  0.3123]],\n",
      "\n",
      "         [[-0.0866,  0.1752, -0.6246, -0.3632, -0.1765],\n",
      "          [-0.4021,  0.1645,  0.1996, -0.4375, -0.2011],\n",
      "          [-0.2403, -0.0085, -0.1204, -0.8618, -0.3785],\n",
      "          [-0.5445, -0.1656, -0.3856, -1.2941, -0.4035]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1985,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2057,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2321,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2270,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.3739,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3832,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0716,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.4812,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[ 0.5433,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3768,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.3073,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.4519,    -inf,    -inf,    -inf,    -inf]],\n",
      "\n",
      "         [[-0.4923,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2927,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3508,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.4348,    -inf,    -inf,    -inf,    -inf]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "softmaxed_prod: \n",
      " tensor([[[[0.5207, 0.4793, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5188, 0.4812, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5515, 0.4485, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4961, 0.5039, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.2742, 0.7258, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3305, 0.6695, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3277, 0.6723, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3562, 0.6438, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4282, 0.5718, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4954, 0.5046, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4644, 0.5356, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4877, 0.5123, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.5228, 0.4772, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4807, 0.5193, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4834, 0.5166, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4375, 0.5625, 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1504, 0.1677, 0.1600, 0.1459, 0.3760],\n",
      "          [0.1957, 0.2219, 0.1532, 0.1606, 0.2685],\n",
      "          [0.1877, 0.1699, 0.2514, 0.1079, 0.2831],\n",
      "          [0.1219, 0.2078, 0.2887, 0.1562, 0.2254]],\n",
      "\n",
      "         [[0.2120, 0.1655, 0.1415, 0.2298, 0.2511],\n",
      "          [0.2453, 0.1589, 0.1962, 0.2246, 0.1750],\n",
      "          [0.2404, 0.2510, 0.0991, 0.2133, 0.1962],\n",
      "          [0.1828, 0.2054, 0.2044, 0.2080, 0.1994]],\n",
      "\n",
      "         [[0.1969, 0.1860, 0.1582, 0.1696, 0.2893],\n",
      "          [0.2076, 0.2133, 0.1825, 0.1811, 0.2154],\n",
      "          [0.1561, 0.1387, 0.3398, 0.2306, 0.1348],\n",
      "          [0.2579, 0.1543, 0.2986, 0.0945, 0.1946]],\n",
      "\n",
      "         [[0.2195, 0.2852, 0.1282, 0.1665, 0.2006],\n",
      "          [0.1476, 0.2601, 0.2694, 0.1425, 0.1805],\n",
      "          [0.2085, 0.2629, 0.2351, 0.1120, 0.1816],\n",
      "          [0.1902, 0.2779, 0.2230, 0.0899, 0.2190]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "output shape: torch.Size([3, 4, 100277])\n",
      "softmaxed[0, 0, :10]: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "predicted: \n",
      " tensor([[ 82681, 100257, 100257, 100257],\n",
      "        [    34,  17771,   6316,  17571],\n",
      "        [ 23380, 100257, 100257, 100257]])\n",
      "predicted decoded: \n",
      " ['Bonjour<|endoftext|><|endoftext|><|endoftext|>', \"C'est une phrase\", 'START<|endoftext|><|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "sents = [\"Hello World\", \"This is a simple sentence\", \"Me\"]\n",
    "encoded_sents = [encoding.encode(s) for s in sents]\n",
    "enc_x = pad_sequence([torch.tensor(es) for es in encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(enc_x)\n",
    "dec_sents = [\"Bonjour\", \"C'est une phrase\", \"START\"]\n",
    "dec_encoded_sents = [encoding.encode(s) for s in dec_sents]\n",
    "dec_x = pad_sequence([torch.tensor(es) for es in dec_encoded_sents], batch_first=True, padding_value=encoding.eot_token)\n",
    "print(dec_x)\n",
    "\n",
    "transformer = Transformer(vocab_size=encoding.n_vocab, n=3, d=256, h=4)\n",
    "\n",
    "enc_mask = build_padding_mask(enc_x, pad_token=100257)\n",
    "print(f\"enc_mask: \\n {enc_mask}\")\n",
    "enc_mask = reshape_mask(enc_mask)\n",
    "\n",
    "dec_mask1 = build_padding_mask(dec_x, pad_token=100257)\n",
    "dec_mask2 = build_causal_mask(dec_x)\n",
    "dec_mask = merge_masks(dec_mask1, dec_mask2)\n",
    "print(f\"dec_mask: \\n {dec_mask}\")\n",
    "dec_mask = reshape_mask(dec_mask)\n",
    "\n",
    "output = transformer(enc_x, dec_x, enc_mask=enc_mask, dec_mask=dec_mask)\n",
    "print(f\"output shape: {output.shape}\")\n",
    "softmaxed = F.softmax(output, dim=-1)\n",
    "print(f\"softmaxed[0, 0, :10]: {softmaxed[0, 0, :10]}\")\n",
    "predicted = softmaxed.argmax(dim=-1)\n",
    "print(f\"predicted: \\n {predicted}\")\n",
    "\n",
    "predicted_list = predicted.tolist()\n",
    "predicted_decoded = [encoding.decode(l) for l in predicted_list]\n",
    "print(f\"predicted decoded: \\n {predicted_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf3a2346-9e74-4eab-aab3-bd5bb0a0d78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.3]]), dim=-1)\n",
    "torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]).argmax(dim=-1)\n",
    "# torch.max(torch.tensor([[0.1,0.2,0.3],[0.1, 0.2, 0.4]]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b16e4-6684-4dbb-b21b-8c3650a10bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e70b0-a647-455b-bdfb-332d095aafc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082af878-9425-4445-b4b4-592cef79a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dba170-623a-486b-8b67-58360bde104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d1a8-69aa-4c60-b3b4-ebb2e31f1c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cda8a40-c41b-4eb3-90af-446f97e501f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f4736-acd3-4bd9-a9f5-b00240a17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1, 2, 3])\n",
    "mask = torch.ones([1, 2])\n",
    "mask[0, 1] = 0\n",
    "mask = mask.unsqueeze(1)\n",
    "print(mask == 0)\n",
    "x.masked_fill(mask == 0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ff31-01d1-4144-b1ad-b2b600609d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bb090-365d-4c4d-986d-b048e7345f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
