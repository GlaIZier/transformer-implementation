{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d29dfa4-a1e1-4dd2-b2c5-e3ed8653fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "tensor([[[0.4189, 0.2613, 0.3197],\n",
      "         [0.3650, 0.3149, 0.3201],\n",
      "         [0.3279, 0.2350, 0.4370]],\n",
      "\n",
      "        [[0.3488, 0.2995, 0.3518],\n",
      "         [0.2417, 0.3859, 0.3724],\n",
      "         [0.2332, 0.3058, 0.4610]]])\n",
      "torch.Size([2, 3, 3])\n",
      "tensor([[[0.4189, 0.2613, 0.3197],\n",
      "         [0.3650, 0.3149, 0.3201],\n",
      "         [0.3279, 0.2350, 0.4370]],\n",
      "\n",
      "        [[0.3488, 0.2995, 0.3518],\n",
      "         [0.2417, 0.3859, 0.3724],\n",
      "         [0.2332, 0.3058, 0.4610]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    # b t d\n",
    "    #prod = Q.bmm(K.permute(0, 2, 1))\n",
    "    prod = torch.einsum(\"btd, bsd -> bts\", Q, K)\n",
    "    scaled_prod = prod/torch.sqrt(torch.tensor(Q.shape[-1]))\n",
    "    softmaxed_prod = torch.nn.functional.softmax(scaled_prod, dim=-1)\n",
    "    print(softmaxed_prod.shape)\n",
    "    print(softmaxed_prod)\n",
    "    return softmaxed_prod.bmm(V)\n",
    "\n",
    "\n",
    "x = torch.rand([2, 3, 4])\n",
    "self_attention(x, x, x)\n",
    "self_attention(x, x, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c7f2692-0a9c-411f-b476-f8a36154c39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yp/nh9d_4t54_j3lb4kljncwfj40000gp/T/ipykernel_91734/2143963450.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(torch.rand([2,3]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2549, 0.3325, 0.4125],\n",
       "        [0.4198, 0.2815, 0.2987]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.rand([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24dee153-347e-4628-baef-57054c6089c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(9)\n",
    "torch.sqrt(torch.tensor(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae60133-3d0c-4ada-8789-51107800432a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand([2,3,4]).T.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
